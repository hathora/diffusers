//
// Generated by LLVM NVPTX Back-End
//

.version 8.4
.target sm_90a
.address_size 64

	// .globl	triton_
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.func  (.param .b64 func_retval0) __internal_accurate_pow
(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
;
.global .align 1 .b8 _$_str[11] = {95, 95, 67, 85, 68, 65, 95, 70, 84, 90};
.global .align 1 .b8 _$_str_$_1[12] = {95, 95, 67, 85, 68, 65, 95, 65, 82, 67, 72};
.global .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.global .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .entry triton_(
	.param .u64 triton__param_0,
	.param .u64 triton__param_1,
	.param .u64 triton__param_2,
	.param .u64 triton__param_3,
	.param .u32 triton__param_4,
	.param .u32 triton__param_5,
	.param .u32 triton__param_6
)
.maxntid 128, 1, 1
{
	.local .align 4 .b8 	__local_depot0[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<377>;
	.reg .b16 	%rs<97>;
	.reg .b32 	%r<591>;
	.reg .f32 	%f<41>;
	.reg .b64 	%rd<189>;
	.reg .f64 	%fd<847>;
	.loc	1 18 0
$L__func_begin0:
	.loc	1 18 0

	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r123, [triton__param_6];
	ld.param.u32 	%r122, [triton__param_5];
	ld.param.u64 	%rd27, [triton__param_0];
	ld.param.u64 	%rd28, [triton__param_1];
	ld.param.u32 	%r141, [triton__param_4];
$L__tmp0:
	.loc	1 19 28
	// begin inline asm
	mov.u32 %r124, %ctaid.x;
	// end inline asm
	.loc	1 19 33
	shl.b32 	%r142, %r124, 10;
	.loc	1 20 36
	mov.u32 	%r143, %tid.x;
	and.b32  	%r144, %r143, 127;
	.loc	1 20 23
	or.b32  	%r17, %r142, %r144;
	or.b32  	%r18, %r17, 128;
	or.b32  	%r19, %r17, 256;
	or.b32  	%r20, %r17, 384;
	or.b32  	%r21, %r17, 512;
	or.b32  	%r22, %r17, 640;
	or.b32  	%r23, %r17, 768;
	or.b32  	%r24, %r17, 896;
	.loc	1 21 21
	setp.lt.s32 	%p343, %r17, %r123;
	setp.lt.s32 	%p344, %r18, %r123;
	setp.lt.s32 	%p345, %r19, %r123;
	setp.lt.s32 	%p346, %r20, %r123;
	setp.lt.s32 	%p347, %r21, %r123;
	setp.lt.s32 	%p348, %r22, %r123;
	setp.lt.s32 	%p349, %r23, %r123;
	setp.lt.s32 	%p350, %r24, %r123;
	.loc	1 22 20
	mul.hi.s32 	%r145, %r17, -1840700269;
	mad.lo.s32 	%r146, %r17, 1, %r145;
	.loc	1 23 18
	shr.u32 	%r147, %r146, 31;
	shr.s32 	%r148, %r146, 5;
	add.s32 	%r33, %r148, %r147;
	mul.lo.s32 	%r149, %r33, 56;
	sub.s32 	%r25, %r17, %r149;
	.loc	1 22 20
	mul.hi.s32 	%r150, %r18, -1840700269;
	mad.lo.s32 	%r151, %r18, 1, %r150;
	.loc	1 23 18
	shr.u32 	%r152, %r151, 31;
	shr.s32 	%r153, %r151, 5;
	add.s32 	%r34, %r153, %r152;
	mul.lo.s32 	%r154, %r34, 56;
	sub.s32 	%r26, %r18, %r154;
	.loc	1 22 20
	mul.hi.s32 	%r155, %r19, -1840700269;
	mad.lo.s32 	%r156, %r19, 1, %r155;
	.loc	1 23 18
	shr.u32 	%r157, %r156, 31;
	shr.s32 	%r158, %r156, 5;
	add.s32 	%r35, %r158, %r157;
	mul.lo.s32 	%r159, %r35, 56;
	sub.s32 	%r27, %r19, %r159;
	.loc	1 22 20
	mul.hi.s32 	%r160, %r20, -1840700269;
	mad.lo.s32 	%r161, %r20, 1, %r160;
	.loc	1 23 18
	shr.u32 	%r162, %r161, 31;
	shr.s32 	%r163, %r161, 5;
	add.s32 	%r36, %r163, %r162;
	mul.lo.s32 	%r164, %r36, 56;
	sub.s32 	%r28, %r20, %r164;
	.loc	1 22 20
	mul.hi.s32 	%r165, %r21, -1840700269;
	mad.lo.s32 	%r166, %r21, 1, %r165;
	.loc	1 23 18
	shr.u32 	%r167, %r166, 31;
	shr.s32 	%r168, %r166, 5;
	add.s32 	%r37, %r168, %r167;
	mul.lo.s32 	%r169, %r37, 56;
	sub.s32 	%r29, %r21, %r169;
	.loc	1 22 20
	mul.hi.s32 	%r170, %r22, -1840700269;
	mad.lo.s32 	%r171, %r22, 1, %r170;
	.loc	1 23 18
	shr.u32 	%r172, %r171, 31;
	shr.s32 	%r173, %r171, 5;
	add.s32 	%r38, %r173, %r172;
	mul.lo.s32 	%r174, %r38, 56;
	sub.s32 	%r30, %r22, %r174;
	.loc	1 22 20
	mul.hi.s32 	%r175, %r23, -1840700269;
	mad.lo.s32 	%r176, %r23, 1, %r175;
	.loc	1 23 18
	shr.u32 	%r177, %r176, 31;
	shr.s32 	%r178, %r176, 5;
	add.s32 	%r39, %r178, %r177;
	.loc	1 22 20
	mul.hi.s32 	%r180, %r24, -1840700269;
	mad.lo.s32 	%r181, %r24, 1, %r180;
	.loc	1 23 18
	shr.u32 	%r182, %r181, 31;
	shr.s32 	%r183, %r181, 5;
	add.s32 	%r40, %r183, %r182;
	.loc	1 28 18
	setp.lt.s32 	%p73, %r40, %r141;
	setp.lt.s32 	%p74, %r39, %r141;
	setp.lt.s32 	%p75, %r38, %r141;
	setp.lt.s32 	%p76, %r37, %r141;
	setp.lt.s32 	%p77, %r36, %r141;
	setp.lt.s32 	%p78, %r35, %r141;
	setp.lt.s32 	%p79, %r34, %r141;
	setp.lt.s32 	%p80, %r33, %r141;
	.loc	1 29 37
	mul.lo.s32 	%r185, %r33, 3;
	mul.lo.s32 	%r186, %r34, 3;
	mul.lo.s32 	%r187, %r35, 3;
	mul.lo.s32 	%r188, %r36, 3;
	mul.lo.s32 	%r189, %r37, 3;
	mul.lo.s32 	%r190, %r38, 3;
	mul.lo.s32 	%r191, %r39, 3;
	mul.lo.s32 	%r192, %r40, 3;
	.loc	1 29 30
	mul.wide.s32 	%rd45, %r185, 2;
	add.s64 	%rd46, %rd27, %rd45;
	add.s64 	%rd11, %rd46, 4;
	mul.wide.s32 	%rd47, %r186, 2;
	add.s64 	%rd48, %rd27, %rd47;
	add.s64 	%rd12, %rd48, 4;
	mul.wide.s32 	%rd49, %r187, 2;
	add.s64 	%rd50, %rd27, %rd49;
	add.s64 	%rd13, %rd50, 4;
	mul.wide.s32 	%rd51, %r188, 2;
	add.s64 	%rd52, %rd27, %rd51;
	add.s64 	%rd14, %rd52, 4;
	mul.wide.s32 	%rd53, %r189, 2;
	add.s64 	%rd54, %rd27, %rd53;
	add.s64 	%rd15, %rd54, 4;
	mul.wide.s32 	%rd55, %r190, 2;
	add.s64 	%rd56, %rd27, %rd55;
	add.s64 	%rd16, %rd56, 4;
	mul.wide.s32 	%rd57, %r191, 2;
	add.s64 	%rd58, %rd27, %rd57;
	add.s64 	%rd17, %rd58, 4;
	mul.wide.s32 	%rd59, %r192, 2;
	add.s64 	%rd60, %rd27, %rd59;
	add.s64 	%rd18, %rd60, 4;
	.loc	1 29 50
	and.pred  	%p33, %p343, %p80;
	and.pred  	%p35, %p79, %p344;
	and.pred  	%p37, %p78, %p345;
	and.pred  	%p39, %p77, %p346;
	and.pred  	%p41, %p76, %p347;
	and.pred  	%p43, %p75, %p348;
	and.pred  	%p45, %p74, %p349;
	and.pred  	%p47, %p73, %p350;
	mov.u16 	%rs2, 0;
	.loc	1 29 43
	// begin inline asm
	mov.u16 %rs1, 0x0;
	@%p33 ld.global.L1::evict_last.b16 { %rs1 }, [ %rd11 + 0 ];
	@!%p33 mov.u16 %rs1, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs3, 0x0;
	@%p35 ld.global.L1::evict_last.b16 { %rs3 }, [ %rd12 + 0 ];
	@!%p35 mov.u16 %rs3, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs5, 0x0;
	@%p37 ld.global.L1::evict_last.b16 { %rs5 }, [ %rd13 + 0 ];
	@!%p37 mov.u16 %rs5, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs7, 0x0;
	@%p39 ld.global.L1::evict_last.b16 { %rs7 }, [ %rd14 + 0 ];
	@!%p39 mov.u16 %rs7, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs9, 0x0;
	@%p41 ld.global.L1::evict_last.b16 { %rs9 }, [ %rd15 + 0 ];
	@!%p41 mov.u16 %rs9, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs11, 0x0;
	@%p43 ld.global.L1::evict_last.b16 { %rs11 }, [ %rd16 + 0 ];
	@!%p43 mov.u16 %rs11, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs13, 0x0;
	@%p45 ld.global.L1::evict_last.b16 { %rs13 }, [ %rd17 + 0 ];
	@!%p45 mov.u16 %rs13, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs15, 0x0;
	@%p47 ld.global.L1::evict_last.b16 { %rs15 }, [ %rd18 + 0 ];
	@!%p47 mov.u16 %rs15, %rs2;
	// end inline asm
	.loc	1 30 19
	setp.ge.s32 	%p81, %r33, %r141;
	setp.ge.s32 	%p82, %r34, %r141;
	setp.ge.s32 	%p83, %r35, %r141;
	setp.ge.s32 	%p84, %r36, %r141;
	setp.ge.s32 	%p85, %r37, %r141;
	setp.ge.s32 	%p86, %r38, %r141;
	setp.ge.s32 	%p87, %r39, %r141;
	setp.ge.s32 	%p88, %r40, %r141;
	.loc	1 33 44
	sub.s32 	%r193, %r33, %r141;
	sub.s32 	%r194, %r34, %r141;
	sub.s32 	%r195, %r35, %r141;
	sub.s32 	%r196, %r36, %r141;
	sub.s32 	%r197, %r37, %r141;
	sub.s32 	%r198, %r38, %r141;
	sub.s32 	%r199, %r39, %r141;
	sub.s32 	%r200, %r40, %r141;
	.loc	1 33 35
	mad.lo.s32 	%r201, %r193, 3, 2;
	mad.lo.s32 	%r202, %r194, 3, 2;
	mad.lo.s32 	%r203, %r195, 3, 2;
	mad.lo.s32 	%r204, %r196, 3, 2;
	mad.lo.s32 	%r205, %r197, 3, 2;
	mad.lo.s32 	%r206, %r198, 3, 2;
	mad.lo.s32 	%r207, %r199, 3, 2;
	mad.lo.s32 	%r208, %r200, 3, 2;
	.loc	1 33 30
	mul.wide.s32 	%rd61, %r201, 2;
	add.s64 	%rd19, %rd28, %rd61;
	mul.wide.s32 	%rd62, %r202, 2;
	add.s64 	%rd20, %rd28, %rd62;
	mul.wide.s32 	%rd63, %r203, 2;
	add.s64 	%rd21, %rd28, %rd63;
	mul.wide.s32 	%rd64, %r204, 2;
	add.s64 	%rd22, %rd28, %rd64;
	mul.wide.s32 	%rd65, %r205, 2;
	add.s64 	%rd23, %rd28, %rd65;
	mul.wide.s32 	%rd66, %r206, 2;
	add.s64 	%rd24, %rd28, %rd66;
	mul.wide.s32 	%rd67, %r207, 2;
	add.s64 	%rd25, %rd28, %rd67;
	mul.wide.s32 	%rd68, %r208, 2;
	add.s64 	%rd26, %rd28, %rd68;
	.loc	1 33 65
	and.pred  	%p49, %p343, %p81;
	and.pred  	%p51, %p82, %p344;
	and.pred  	%p53, %p83, %p345;
	and.pred  	%p55, %p84, %p346;
	and.pred  	%p57, %p85, %p347;
	and.pred  	%p59, %p86, %p348;
	and.pred  	%p61, %p87, %p349;
	and.pred  	%p63, %p88, %p350;
	.loc	1 33 58
	// begin inline asm
	mov.u16 %rs25, 0x0;
	@%p49 ld.global.L1::evict_last.b16 { %rs25 }, [ %rd19 + 0 ];
	@!%p49 mov.u16 %rs25, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs27, 0x0;
	@%p51 ld.global.L1::evict_last.b16 { %rs27 }, [ %rd20 + 0 ];
	@!%p51 mov.u16 %rs27, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs29, 0x0;
	@%p53 ld.global.L1::evict_last.b16 { %rs29 }, [ %rd21 + 0 ];
	@!%p53 mov.u16 %rs29, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs31, 0x0;
	@%p55 ld.global.L1::evict_last.b16 { %rs31 }, [ %rd22 + 0 ];
	@!%p55 mov.u16 %rs31, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs33, 0x0;
	@%p57 ld.global.L1::evict_last.b16 { %rs33 }, [ %rd23 + 0 ];
	@!%p57 mov.u16 %rs33, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs35, 0x0;
	@%p59 ld.global.L1::evict_last.b16 { %rs35 }, [ %rd24 + 0 ];
	@!%p59 mov.u16 %rs35, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs37, 0x0;
	@%p61 ld.global.L1::evict_last.b16 { %rs37 }, [ %rd25 + 0 ];
	@!%p61 mov.u16 %rs37, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs39, 0x0;
	@%p63 ld.global.L1::evict_last.b16 { %rs39 }, [ %rd26 + 0 ];
	@!%p63 mov.u16 %rs39, %rs2;
	// end inline asm
	.loc	1 37 37
	cvt.rn.f64.s32 	%fd9, %r122;
	.loc	1 39 21
	cvt.u16.u32 	%rs49, %r25;
	and.b16  	%rs50, %rs49, 128;
	shr.u16 	%rs51, %rs50, 7;
	add.s16 	%rs52, %rs49, %rs51;
	cvt.s16.s8 	%rs53, %rs52;
	shr.s16 	%rs54, %rs53, 1;
	cvt.u16.u32 	%rs55, %r26;
	and.b16  	%rs56, %rs55, 128;
	shr.u16 	%rs57, %rs56, 7;
	add.s16 	%rs58, %rs55, %rs57;
	cvt.s16.s8 	%rs59, %rs58;
	shr.s16 	%rs60, %rs59, 1;
	cvt.u16.u32 	%rs61, %r27;
	and.b16  	%rs62, %rs61, 128;
	shr.u16 	%rs63, %rs62, 7;
	add.s16 	%rs64, %rs61, %rs63;
	cvt.s16.s8 	%rs65, %rs64;
	shr.s16 	%rs66, %rs65, 1;
	cvt.u16.u32 	%rs67, %r28;
	and.b16  	%rs68, %rs67, 128;
	shr.u16 	%rs69, %rs68, 7;
	add.s16 	%rs70, %rs67, %rs69;
	cvt.u16.u32 	%rs73, %r29;
	and.b16  	%rs74, %rs73, 128;
	.loc	1 39 15
	mul.wide.s16 	%r209, %rs54, 2;
	mul.wide.s16 	%r210, %rs60, 2;
	.loc	1 40 21
	cvt.rn.f64.s32 	%fd185, %r209;
	cvt.rn.f64.s32 	%fd186, %r210;
	.loc	1 42 20
	mul.f64 	%fd10, %fd185, 0d3F92492492492492;
	.loc	1 43 33
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd9;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r42}, %fd10;
	}
	bfe.u32 	%r217, %r42, 20, 11;
	add.s32 	%r218, %r217, -1012;
	mov.b64 	%rd69, %fd10;
	shl.b64 	%rd1, %rd69, %r218;
	abs.f64 	%fd18, %fd9;
	{ // callseq 0, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd18;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd10;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd193, [retval0+0];
	} // callseq 0
	setp.ne.s32 	%p89, %r122, 0;
	setp.lt.s32 	%p361, %r41, 0;
	@%p89 bra 	$L__BB0_2;
	bra.uni 	$L__BB0_1;
$L__BB0_2:
	setp.eq.s64 	%p369, %rd1, -9223372036854775808;
	setp.eq.s64 	%p91, %rd1, -9223372036854775808;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r219, %temp}, %fd193;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r220}, %fd193;
	}
	xor.b32  	%r221, %r220, -2147483648;
	mov.b64 	%fd194, {%r219, %r221};
	selp.f64 	%fd195, %fd194, %fd193, %p91;
	selp.f64 	%fd196, %fd195, %fd193, %p361;
	cvt.rzi.f64.f64 	%fd197, %fd10;
	setp.neu.f64 	%p92, %fd197, %fd10;
	selp.f64 	%fd198, 0dFFF8000000000000, %fd196, %p92;
	selp.f64 	%fd807, %fd198, %fd196, %p361;
	bra.uni 	$L__BB0_3;
$L__BB0_1:
	setp.eq.s64 	%p93, %rd1, -9223372036854775808;
	abs.f64 	%fd199, %fd10;
	setp.neu.f64 	%p94, %fd199, 0d3FE0000000000000;
	and.pred  	%p369, %p94, %p93;
	selp.b32 	%r222, %r41, 0, %p369;
	setp.lt.s32 	%p95, %r42, 0;
	or.b32  	%r223, %r222, 2146435072;
	selp.b32 	%r224, %r223, %r222, %p95;
	mov.b32 	%r225, 0;
	mov.b64 	%fd807, {%r225, %r224};
$L__BB0_3:
	.loc	1 0 33
	mul.lo.s32 	%r179, %r39, 56;
	cvt.u16.u32 	%rs79, %r30;
	shr.u16 	%rs75, %rs74, 7;
	cvt.s16.s8 	%rs71, %rs70;
	mul.wide.s16 	%r211, %rs66, 2;
	mul.f64 	%fd11, %fd186, 0d3F92492492492492;
	.loc	1 43 33
	add.f64 	%fd23, %fd10, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r226}, %fd23;
	}
	and.b32  	%r227, %r226, 2146435072;
	setp.ne.s32 	%p96, %r227, 2146435072;
	setp.nan.f64 	%p368, %fd18, %fd18;
	mov.f64 	%fd808, %fd807;
	@%p96 bra 	$L__BB0_10;
	.loc	1 0 33
	mov.f64 	%fd808, %fd23;
	.loc	1 43 33
	@%p368 bra 	$L__BB0_10;
	abs.f64 	%fd200, %fd10;
	setp.nan.f64 	%p98, %fd200, %fd200;
	mov.f64 	%fd808, %fd23;
	@%p98 bra 	$L__BB0_10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r228, %temp}, %fd10;
	}
	and.b32  	%r43, %r42, 2147483647;
	setp.eq.s32 	%p99, %r43, 2146435072;
	setp.eq.s32 	%p100, %r228, 0;
	and.pred  	%p101, %p99, %p100;
	@!%p101 bra 	$L__BB0_8;
	bra.uni 	$L__BB0_7;
$L__BB0_7:
	setp.gt.f64 	%p108, %fd18, 0d3FF0000000000000;
	selp.b32 	%r237, 2146435072, 0, %p108;
	setp.lt.s32 	%p109, %r42, 0;
	xor.b32  	%r238, %r237, 2146435072;
	selp.b32 	%r239, %r238, %r237, %p109;
	setp.eq.s32 	%p110, %r122, -1;
	selp.b32 	%r240, 1072693248, %r239, %p110;
	mov.b32 	%r241, 0;
	mov.b64 	%fd808, {%r241, %r240};
	bra.uni 	$L__BB0_10;
$L__BB0_8:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r229, %temp}, %fd9;
	}
	and.b32  	%r230, %r41, 2147483647;
	setp.eq.s32 	%p102, %r230, 2146435072;
	setp.eq.s32 	%p103, %r229, 0;
	and.pred  	%p104, %p102, %p103;
	mov.f64 	%fd808, %fd807;
	@!%p104 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_9;
$L__BB0_9:
	setp.lt.s32 	%p106, %r42, 0;
	selp.b32 	%r231, 0, 2146435072, %p106;
	setp.ne.s32 	%p107, %r43, 1071644672;
	or.b32  	%r232, %r231, -2147483648;
	selp.b32 	%r233, %r232, %r231, %p107;
	selp.b32 	%r234, %r233, %r231, %p369;
	selp.b32 	%r235, %r234, %r231, %p361;
	mov.b32 	%r236, 0;
	mov.b64 	%fd808, {%r236, %r235};
$L__BB0_10:
	.loc	1 0 33
	sub.s32 	%r31, %r23, %r179;
	and.b16  	%rs80, %rs79, 128;
	add.s16 	%rs76, %rs73, %rs75;
	shr.s16 	%rs72, %rs71, 1;
	cvt.rn.f64.s32 	%fd187, %r211;
	.loc	1 43 33
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd11;
	}
	bfe.u32 	%r242, %r44, 20, 11;
	add.s32 	%r243, %r242, -1012;
	mov.b64 	%rd70, %fd11;
	shl.b64 	%rd2, %rd70, %r243;
	{ // callseq 1, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd18;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd11;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd202, [retval0+0];
	} // callseq 1
	@%p89 bra 	$L__BB0_12;
	bra.uni 	$L__BB0_11;
$L__BB0_12:
	setp.eq.s64 	%p370, %rd2, -9223372036854775808;
	setp.eq.s64 	%p114, %rd2, -9223372036854775808;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r244, %temp}, %fd202;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r245}, %fd202;
	}
	xor.b32  	%r246, %r245, -2147483648;
	mov.b64 	%fd203, {%r244, %r246};
	selp.f64 	%fd204, %fd203, %fd202, %p114;
	selp.f64 	%fd205, %fd204, %fd202, %p361;
	cvt.rzi.f64.f64 	%fd206, %fd11;
	setp.neu.f64 	%p116, %fd206, %fd11;
	selp.f64 	%fd207, 0dFFF8000000000000, %fd205, %p116;
	selp.f64 	%fd809, %fd207, %fd205, %p361;
	bra.uni 	$L__BB0_13;
$L__BB0_11:
	setp.eq.s64 	%p117, %rd2, -9223372036854775808;
	abs.f64 	%fd208, %fd11;
	setp.neu.f64 	%p118, %fd208, 0d3FE0000000000000;
	and.pred  	%p370, %p118, %p117;
	selp.b32 	%r247, %r41, 0, %p370;
	setp.lt.s32 	%p119, %r44, 0;
	or.b32  	%r248, %r247, 2146435072;
	selp.b32 	%r249, %r248, %r247, %p119;
	mov.b32 	%r250, 0;
	mov.b64 	%fd809, {%r250, %r249};
$L__BB0_13:
	.loc	1 0 33
	mul.lo.s32 	%r184, %r40, 56;
	cvt.u16.u32 	%rs85, %r31;
	shr.u16 	%rs81, %rs80, 7;
	cvt.s16.s8 	%rs77, %rs76;
	mul.wide.s16 	%r212, %rs72, 2;
	mul.f64 	%fd12, %fd187, 0d3F92492492492492;
	.loc	1 43 33
	add.f64 	%fd32, %fd11, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r251}, %fd32;
	}
	and.b32  	%r252, %r251, 2146435072;
	setp.ne.s32 	%p120, %r252, 2146435072;
	mov.f64 	%fd810, %fd809;
	@%p120 bra 	$L__BB0_20;
	.loc	1 0 33
	mov.f64 	%fd810, %fd32;
	.loc	1 43 33
	@%p368 bra 	$L__BB0_20;
	abs.f64 	%fd209, %fd11;
	setp.nan.f64 	%p122, %fd209, %fd209;
	mov.f64 	%fd810, %fd32;
	@%p122 bra 	$L__BB0_20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r253, %temp}, %fd11;
	}
	and.b32  	%r45, %r44, 2147483647;
	setp.eq.s32 	%p123, %r45, 2146435072;
	setp.eq.s32 	%p124, %r253, 0;
	and.pred  	%p125, %p123, %p124;
	@!%p125 bra 	$L__BB0_18;
	bra.uni 	$L__BB0_17;
$L__BB0_17:
	setp.gt.f64 	%p132, %fd18, 0d3FF0000000000000;
	selp.b32 	%r262, 2146435072, 0, %p132;
	setp.lt.s32 	%p133, %r44, 0;
	xor.b32  	%r263, %r262, 2146435072;
	selp.b32 	%r264, %r263, %r262, %p133;
	setp.eq.s32 	%p134, %r122, -1;
	selp.b32 	%r265, 1072693248, %r264, %p134;
	mov.b32 	%r266, 0;
	mov.b64 	%fd810, {%r266, %r265};
	bra.uni 	$L__BB0_20;
$L__BB0_18:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r254, %temp}, %fd9;
	}
	and.b32  	%r255, %r41, 2147483647;
	setp.eq.s32 	%p126, %r255, 2146435072;
	setp.eq.s32 	%p127, %r254, 0;
	and.pred  	%p128, %p126, %p127;
	mov.f64 	%fd810, %fd809;
	@!%p128 bra 	$L__BB0_20;
	bra.uni 	$L__BB0_19;
$L__BB0_19:
	setp.lt.s32 	%p130, %r44, 0;
	selp.b32 	%r256, 0, 2146435072, %p130;
	setp.ne.s32 	%p131, %r45, 1071644672;
	or.b32  	%r257, %r256, -2147483648;
	selp.b32 	%r258, %r257, %r256, %p131;
	selp.b32 	%r259, %r258, %r256, %p370;
	selp.b32 	%r260, %r259, %r256, %p361;
	mov.b32 	%r261, 0;
	mov.b64 	%fd810, {%r261, %r260};
$L__BB0_20:
	.loc	1 0 33
	sub.s32 	%r32, %r24, %r184;
	and.b16  	%rs86, %rs85, 128;
	add.s16 	%rs82, %rs79, %rs81;
	shr.s16 	%rs78, %rs77, 1;
	cvt.rn.f64.s32 	%fd188, %r212;
	.loc	1 43 33
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd12;
	}
	bfe.u32 	%r267, %r46, 20, 11;
	add.s32 	%r268, %r267, -1012;
	mov.b64 	%rd71, %fd12;
	shl.b64 	%rd3, %rd71, %r268;
	{ // callseq 2, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd18;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd12;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd211, [retval0+0];
	} // callseq 2
	@%p89 bra 	$L__BB0_22;
	bra.uni 	$L__BB0_21;
$L__BB0_22:
	setp.eq.s64 	%p371, %rd3, -9223372036854775808;
	setp.eq.s64 	%p138, %rd3, -9223372036854775808;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r269, %temp}, %fd211;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r270}, %fd211;
	}
	xor.b32  	%r271, %r270, -2147483648;
	mov.b64 	%fd212, {%r269, %r271};
	selp.f64 	%fd213, %fd212, %fd211, %p138;
	selp.f64 	%fd214, %fd213, %fd211, %p361;
	cvt.rzi.f64.f64 	%fd215, %fd12;
	setp.neu.f64 	%p140, %fd215, %fd12;
	selp.f64 	%fd216, 0dFFF8000000000000, %fd214, %p140;
	selp.f64 	%fd811, %fd216, %fd214, %p361;
	bra.uni 	$L__BB0_23;
$L__BB0_21:
	setp.eq.s64 	%p141, %rd3, -9223372036854775808;
	abs.f64 	%fd217, %fd12;
	setp.neu.f64 	%p142, %fd217, 0d3FE0000000000000;
	and.pred  	%p371, %p142, %p141;
	selp.b32 	%r272, %r41, 0, %p371;
	setp.lt.s32 	%p143, %r46, 0;
	or.b32  	%r273, %r272, 2146435072;
	selp.b32 	%r274, %r273, %r272, %p143;
	mov.b32 	%r275, 0;
	mov.b64 	%fd811, {%r275, %r274};
$L__BB0_23:
	.loc	1 0 33
	cvt.u16.u32 	%rs91, %r32;
	shr.u16 	%rs87, %rs86, 7;
	cvt.s16.s8 	%rs83, %rs82;
	mul.wide.s16 	%r213, %rs78, 2;
	mul.f64 	%fd13, %fd188, 0d3F92492492492492;
	.loc	1 43 33
	add.f64 	%fd41, %fd12, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r276}, %fd41;
	}
	and.b32  	%r277, %r276, 2146435072;
	setp.ne.s32 	%p144, %r277, 2146435072;
	mov.f64 	%fd812, %fd811;
	@%p144 bra 	$L__BB0_30;
	.loc	1 0 33
	mov.f64 	%fd812, %fd41;
	.loc	1 43 33
	@%p368 bra 	$L__BB0_30;
	abs.f64 	%fd218, %fd12;
	setp.nan.f64 	%p146, %fd218, %fd218;
	mov.f64 	%fd812, %fd41;
	@%p146 bra 	$L__BB0_30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r278, %temp}, %fd12;
	}
	and.b32  	%r47, %r46, 2147483647;
	setp.eq.s32 	%p147, %r47, 2146435072;
	setp.eq.s32 	%p148, %r278, 0;
	and.pred  	%p149, %p147, %p148;
	@!%p149 bra 	$L__BB0_28;
	bra.uni 	$L__BB0_27;
$L__BB0_27:
	setp.gt.f64 	%p156, %fd18, 0d3FF0000000000000;
	selp.b32 	%r287, 2146435072, 0, %p156;
	setp.lt.s32 	%p157, %r46, 0;
	xor.b32  	%r288, %r287, 2146435072;
	selp.b32 	%r289, %r288, %r287, %p157;
	setp.eq.s32 	%p158, %r122, -1;
	selp.b32 	%r290, 1072693248, %r289, %p158;
	mov.b32 	%r291, 0;
	mov.b64 	%fd812, {%r291, %r290};
	bra.uni 	$L__BB0_30;
$L__BB0_28:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r279, %temp}, %fd9;
	}
	and.b32  	%r280, %r41, 2147483647;
	setp.eq.s32 	%p150, %r280, 2146435072;
	setp.eq.s32 	%p151, %r279, 0;
	and.pred  	%p152, %p150, %p151;
	mov.f64 	%fd812, %fd811;
	@!%p152 bra 	$L__BB0_30;
	bra.uni 	$L__BB0_29;
$L__BB0_29:
	setp.lt.s32 	%p154, %r46, 0;
	selp.b32 	%r281, 0, 2146435072, %p154;
	setp.ne.s32 	%p155, %r47, 1071644672;
	or.b32  	%r282, %r281, -2147483648;
	selp.b32 	%r283, %r282, %r281, %p155;
	selp.b32 	%r284, %r283, %r281, %p371;
	selp.b32 	%r285, %r284, %r281, %p361;
	mov.b32 	%r286, 0;
	mov.b64 	%fd812, {%r286, %r285};
$L__BB0_30:
	.loc	1 0 33
	and.b16  	%rs92, %rs91, 128;
	add.s16 	%rs88, %rs85, %rs87;
	shr.s16 	%rs84, %rs83, 1;
	cvt.rn.f64.s32 	%fd189, %r213;
	.loc	1 43 33
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r48}, %fd13;
	}
	bfe.u32 	%r292, %r48, 20, 11;
	add.s32 	%r293, %r292, -1012;
	mov.b64 	%rd72, %fd13;
	shl.b64 	%rd4, %rd72, %r293;
	{ // callseq 3, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd18;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd13;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd220, [retval0+0];
	} // callseq 3
	@%p89 bra 	$L__BB0_32;
	bra.uni 	$L__BB0_31;
$L__BB0_32:
	setp.eq.s64 	%p372, %rd4, -9223372036854775808;
	setp.eq.s64 	%p162, %rd4, -9223372036854775808;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r294, %temp}, %fd220;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r295}, %fd220;
	}
	xor.b32  	%r296, %r295, -2147483648;
	mov.b64 	%fd221, {%r294, %r296};
	selp.f64 	%fd222, %fd221, %fd220, %p162;
	selp.f64 	%fd223, %fd222, %fd220, %p361;
	cvt.rzi.f64.f64 	%fd224, %fd13;
	setp.neu.f64 	%p164, %fd224, %fd13;
	selp.f64 	%fd225, 0dFFF8000000000000, %fd223, %p164;
	selp.f64 	%fd813, %fd225, %fd223, %p361;
	bra.uni 	$L__BB0_33;
$L__BB0_31:
	setp.eq.s64 	%p165, %rd4, -9223372036854775808;
	abs.f64 	%fd226, %fd13;
	setp.neu.f64 	%p166, %fd226, 0d3FE0000000000000;
	and.pred  	%p372, %p166, %p165;
	selp.b32 	%r297, %r41, 0, %p372;
	setp.lt.s32 	%p167, %r48, 0;
	or.b32  	%r298, %r297, 2146435072;
	selp.b32 	%r299, %r298, %r297, %p167;
	mov.b32 	%r300, 0;
	mov.b64 	%fd813, {%r300, %r299};
$L__BB0_33:
	.loc	1 0 33
	shr.u16 	%rs93, %rs92, 7;
	cvt.s16.s8 	%rs89, %rs88;
	mul.wide.s16 	%r214, %rs84, 2;
	mul.f64 	%fd14, %fd189, 0d3F92492492492492;
	.loc	1 43 33
	add.f64 	%fd50, %fd13, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r301}, %fd50;
	}
	and.b32  	%r302, %r301, 2146435072;
	setp.ne.s32 	%p168, %r302, 2146435072;
	mov.f64 	%fd814, %fd813;
	@%p168 bra 	$L__BB0_40;
	.loc	1 0 33
	mov.f64 	%fd814, %fd50;
	.loc	1 43 33
	@%p368 bra 	$L__BB0_40;
	abs.f64 	%fd227, %fd13;
	setp.nan.f64 	%p170, %fd227, %fd227;
	mov.f64 	%fd814, %fd50;
	@%p170 bra 	$L__BB0_40;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r303, %temp}, %fd13;
	}
	and.b32  	%r49, %r48, 2147483647;
	setp.eq.s32 	%p171, %r49, 2146435072;
	setp.eq.s32 	%p172, %r303, 0;
	and.pred  	%p173, %p171, %p172;
	@!%p173 bra 	$L__BB0_38;
	bra.uni 	$L__BB0_37;
$L__BB0_37:
	setp.gt.f64 	%p180, %fd18, 0d3FF0000000000000;
	selp.b32 	%r312, 2146435072, 0, %p180;
	setp.lt.s32 	%p181, %r48, 0;
	xor.b32  	%r313, %r312, 2146435072;
	selp.b32 	%r314, %r313, %r312, %p181;
	setp.eq.s32 	%p182, %r122, -1;
	selp.b32 	%r315, 1072693248, %r314, %p182;
	mov.b32 	%r316, 0;
	mov.b64 	%fd814, {%r316, %r315};
	bra.uni 	$L__BB0_40;
$L__BB0_38:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r304, %temp}, %fd9;
	}
	and.b32  	%r305, %r41, 2147483647;
	setp.eq.s32 	%p174, %r305, 2146435072;
	setp.eq.s32 	%p175, %r304, 0;
	and.pred  	%p176, %p174, %p175;
	mov.f64 	%fd814, %fd813;
	@!%p176 bra 	$L__BB0_40;
	bra.uni 	$L__BB0_39;
$L__BB0_39:
	setp.lt.s32 	%p178, %r48, 0;
	selp.b32 	%r306, 0, 2146435072, %p178;
	setp.ne.s32 	%p179, %r49, 1071644672;
	or.b32  	%r307, %r306, -2147483648;
	selp.b32 	%r308, %r307, %r306, %p179;
	selp.b32 	%r309, %r308, %r306, %p372;
	selp.b32 	%r310, %r309, %r306, %p361;
	mov.b32 	%r311, 0;
	mov.b64 	%fd814, {%r311, %r310};
$L__BB0_40:
	.loc	1 0 33
	add.s16 	%rs94, %rs91, %rs93;
	shr.s16 	%rs90, %rs89, 1;
	cvt.rn.f64.s32 	%fd190, %r214;
	.loc	1 43 33
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd14;
	}
	bfe.u32 	%r317, %r50, 20, 11;
	add.s32 	%r318, %r317, -1012;
	mov.b64 	%rd73, %fd14;
	shl.b64 	%rd5, %rd73, %r318;
	{ // callseq 4, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd18;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd14;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd229, [retval0+0];
	} // callseq 4
	@%p89 bra 	$L__BB0_42;
	bra.uni 	$L__BB0_41;
$L__BB0_42:
	setp.eq.s64 	%p373, %rd5, -9223372036854775808;
	setp.eq.s64 	%p186, %rd5, -9223372036854775808;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r319, %temp}, %fd229;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r320}, %fd229;
	}
	xor.b32  	%r321, %r320, -2147483648;
	mov.b64 	%fd230, {%r319, %r321};
	selp.f64 	%fd231, %fd230, %fd229, %p186;
	selp.f64 	%fd232, %fd231, %fd229, %p361;
	cvt.rzi.f64.f64 	%fd233, %fd14;
	setp.neu.f64 	%p188, %fd233, %fd14;
	selp.f64 	%fd234, 0dFFF8000000000000, %fd232, %p188;
	selp.f64 	%fd815, %fd234, %fd232, %p361;
	bra.uni 	$L__BB0_43;
$L__BB0_41:
	setp.eq.s64 	%p189, %rd5, -9223372036854775808;
	abs.f64 	%fd235, %fd14;
	setp.neu.f64 	%p190, %fd235, 0d3FE0000000000000;
	and.pred  	%p373, %p190, %p189;
	selp.b32 	%r322, %r41, 0, %p373;
	setp.lt.s32 	%p191, %r50, 0;
	or.b32  	%r323, %r322, 2146435072;
	selp.b32 	%r324, %r323, %r322, %p191;
	mov.b32 	%r325, 0;
	mov.b64 	%fd815, {%r325, %r324};
$L__BB0_43:
	.loc	1 0 33
	cvt.s16.s8 	%rs95, %rs94;
	mul.wide.s16 	%r215, %rs90, 2;
	mul.f64 	%fd15, %fd190, 0d3F92492492492492;
	.loc	1 43 33
	add.f64 	%fd59, %fd14, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r326}, %fd59;
	}
	and.b32  	%r327, %r326, 2146435072;
	setp.ne.s32 	%p192, %r327, 2146435072;
	mov.f64 	%fd816, %fd815;
	@%p192 bra 	$L__BB0_50;
	.loc	1 0 33
	mov.f64 	%fd816, %fd59;
	.loc	1 43 33
	@%p368 bra 	$L__BB0_50;
	abs.f64 	%fd236, %fd14;
	setp.nan.f64 	%p194, %fd236, %fd236;
	mov.f64 	%fd816, %fd59;
	@%p194 bra 	$L__BB0_50;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r328, %temp}, %fd14;
	}
	and.b32  	%r51, %r50, 2147483647;
	setp.eq.s32 	%p195, %r51, 2146435072;
	setp.eq.s32 	%p196, %r328, 0;
	and.pred  	%p197, %p195, %p196;
	@!%p197 bra 	$L__BB0_48;
	bra.uni 	$L__BB0_47;
$L__BB0_47:
	setp.gt.f64 	%p204, %fd18, 0d3FF0000000000000;
	selp.b32 	%r337, 2146435072, 0, %p204;
	setp.lt.s32 	%p205, %r50, 0;
	xor.b32  	%r338, %r337, 2146435072;
	selp.b32 	%r339, %r338, %r337, %p205;
	setp.eq.s32 	%p206, %r122, -1;
	selp.b32 	%r340, 1072693248, %r339, %p206;
	mov.b32 	%r341, 0;
	mov.b64 	%fd816, {%r341, %r340};
	bra.uni 	$L__BB0_50;
$L__BB0_48:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r329, %temp}, %fd9;
	}
	and.b32  	%r330, %r41, 2147483647;
	setp.eq.s32 	%p198, %r330, 2146435072;
	setp.eq.s32 	%p199, %r329, 0;
	and.pred  	%p200, %p198, %p199;
	mov.f64 	%fd816, %fd815;
	@!%p200 bra 	$L__BB0_50;
	bra.uni 	$L__BB0_49;
$L__BB0_49:
	setp.lt.s32 	%p202, %r50, 0;
	selp.b32 	%r331, 0, 2146435072, %p202;
	setp.ne.s32 	%p203, %r51, 1071644672;
	or.b32  	%r332, %r331, -2147483648;
	selp.b32 	%r333, %r332, %r331, %p203;
	selp.b32 	%r334, %r333, %r331, %p373;
	selp.b32 	%r335, %r334, %r331, %p361;
	mov.b32 	%r336, 0;
	mov.b64 	%fd816, {%r336, %r335};
$L__BB0_50:
	.loc	1 0 33
	shr.s16 	%rs96, %rs95, 1;
	cvt.rn.f64.s32 	%fd191, %r215;
	.loc	1 43 33
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd15;
	}
	bfe.u32 	%r342, %r52, 20, 11;
	add.s32 	%r343, %r342, -1012;
	mov.b64 	%rd74, %fd15;
	shl.b64 	%rd6, %rd74, %r343;
	{ // callseq 5, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd18;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd15;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd238, [retval0+0];
	} // callseq 5
	@%p89 bra 	$L__BB0_52;
	bra.uni 	$L__BB0_51;
$L__BB0_52:
	setp.eq.s64 	%p374, %rd6, -9223372036854775808;
	setp.eq.s64 	%p210, %rd6, -9223372036854775808;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r344, %temp}, %fd238;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r345}, %fd238;
	}
	xor.b32  	%r346, %r345, -2147483648;
	mov.b64 	%fd239, {%r344, %r346};
	selp.f64 	%fd240, %fd239, %fd238, %p210;
	selp.f64 	%fd241, %fd240, %fd238, %p361;
	cvt.rzi.f64.f64 	%fd242, %fd15;
	setp.neu.f64 	%p212, %fd242, %fd15;
	selp.f64 	%fd243, 0dFFF8000000000000, %fd241, %p212;
	selp.f64 	%fd817, %fd243, %fd241, %p361;
	bra.uni 	$L__BB0_53;
$L__BB0_51:
	setp.eq.s64 	%p213, %rd6, -9223372036854775808;
	abs.f64 	%fd244, %fd15;
	setp.neu.f64 	%p214, %fd244, 0d3FE0000000000000;
	and.pred  	%p374, %p214, %p213;
	selp.b32 	%r347, %r41, 0, %p374;
	setp.lt.s32 	%p215, %r52, 0;
	or.b32  	%r348, %r347, 2146435072;
	selp.b32 	%r349, %r348, %r347, %p215;
	mov.b32 	%r350, 0;
	mov.b64 	%fd817, {%r350, %r349};
$L__BB0_53:
	.loc	1 0 33
	mul.wide.s16 	%r216, %rs96, 2;
	mul.f64 	%fd16, %fd191, 0d3F92492492492492;
	.loc	1 43 33
	add.f64 	%fd68, %fd15, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r351}, %fd68;
	}
	and.b32  	%r352, %r351, 2146435072;
	setp.ne.s32 	%p216, %r352, 2146435072;
	mov.f64 	%fd818, %fd817;
	@%p216 bra 	$L__BB0_60;
	.loc	1 0 33
	mov.f64 	%fd818, %fd68;
	.loc	1 43 33
	@%p368 bra 	$L__BB0_60;
	abs.f64 	%fd245, %fd15;
	setp.nan.f64 	%p218, %fd245, %fd245;
	mov.f64 	%fd818, %fd68;
	@%p218 bra 	$L__BB0_60;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r353, %temp}, %fd15;
	}
	and.b32  	%r53, %r52, 2147483647;
	setp.eq.s32 	%p219, %r53, 2146435072;
	setp.eq.s32 	%p220, %r353, 0;
	and.pred  	%p221, %p219, %p220;
	@!%p221 bra 	$L__BB0_58;
	bra.uni 	$L__BB0_57;
$L__BB0_57:
	setp.gt.f64 	%p228, %fd18, 0d3FF0000000000000;
	selp.b32 	%r362, 2146435072, 0, %p228;
	setp.lt.s32 	%p229, %r52, 0;
	xor.b32  	%r363, %r362, 2146435072;
	selp.b32 	%r364, %r363, %r362, %p229;
	setp.eq.s32 	%p230, %r122, -1;
	selp.b32 	%r365, 1072693248, %r364, %p230;
	mov.b32 	%r366, 0;
	mov.b64 	%fd818, {%r366, %r365};
	bra.uni 	$L__BB0_60;
$L__BB0_58:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r354, %temp}, %fd9;
	}
	and.b32  	%r355, %r41, 2147483647;
	setp.eq.s32 	%p222, %r355, 2146435072;
	setp.eq.s32 	%p223, %r354, 0;
	and.pred  	%p224, %p222, %p223;
	mov.f64 	%fd818, %fd817;
	@!%p224 bra 	$L__BB0_60;
	bra.uni 	$L__BB0_59;
$L__BB0_59:
	setp.lt.s32 	%p226, %r52, 0;
	selp.b32 	%r356, 0, 2146435072, %p226;
	setp.ne.s32 	%p227, %r53, 1071644672;
	or.b32  	%r357, %r356, -2147483648;
	selp.b32 	%r358, %r357, %r356, %p227;
	selp.b32 	%r359, %r358, %r356, %p374;
	selp.b32 	%r360, %r359, %r356, %p361;
	mov.b32 	%r361, 0;
	mov.b64 	%fd818, {%r361, %r360};
$L__BB0_60:
	.loc	1 0 33
	// begin inline asm
	cvt.f32.bf16 %r125, %rs1;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r126, %rs3;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r133, %rs25;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r134, %rs27;
	// end inline asm
	cvt.rn.f64.s32 	%fd192, %r216;
	.loc	1 43 33
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r54}, %fd16;
	}
	bfe.u32 	%r367, %r54, 20, 11;
	add.s32 	%r368, %r367, -1012;
	mov.b64 	%rd75, %fd16;
	shl.b64 	%rd7, %rd75, %r368;
	{ // callseq 6, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd18;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd16;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd247, [retval0+0];
	} // callseq 6
	@%p89 bra 	$L__BB0_62;
	bra.uni 	$L__BB0_61;
$L__BB0_62:
	setp.eq.s64 	%p375, %rd7, -9223372036854775808;
	setp.eq.s64 	%p234, %rd7, -9223372036854775808;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r369, %temp}, %fd247;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r370}, %fd247;
	}
	xor.b32  	%r371, %r370, -2147483648;
	mov.b64 	%fd248, {%r369, %r371};
	selp.f64 	%fd249, %fd248, %fd247, %p234;
	selp.f64 	%fd250, %fd249, %fd247, %p361;
	cvt.rzi.f64.f64 	%fd251, %fd16;
	setp.neu.f64 	%p236, %fd251, %fd16;
	selp.f64 	%fd252, 0dFFF8000000000000, %fd250, %p236;
	selp.f64 	%fd819, %fd252, %fd250, %p361;
	bra.uni 	$L__BB0_63;
$L__BB0_61:
	setp.eq.s64 	%p237, %rd7, -9223372036854775808;
	abs.f64 	%fd253, %fd16;
	setp.neu.f64 	%p238, %fd253, 0d3FE0000000000000;
	and.pred  	%p375, %p238, %p237;
	selp.b32 	%r372, %r41, 0, %p375;
	setp.lt.s32 	%p239, %r54, 0;
	or.b32  	%r373, %r372, 2146435072;
	selp.b32 	%r374, %r373, %r372, %p239;
	mov.b32 	%r375, 0;
	mov.b64 	%fd819, {%r375, %r374};
$L__BB0_63:
	.loc	1 0 33
	// begin inline asm
	cvt.f32.bf16 %r127, %rs5;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r135, %rs29;
	// end inline asm
	mov.b32 	%f1, %r125;
	mov.b32 	%f2, %r126;
	mov.b32 	%f9, %r133;
	mov.b32 	%f10, %r134;
	setp.eq.f64 	%p113, %fd10, 0d0000000000000000;
	setp.eq.f64 	%p137, %fd11, 0d0000000000000000;
	setp.eq.f64 	%p161, %fd12, 0d0000000000000000;
	setp.eq.f64 	%p185, %fd13, 0d0000000000000000;
	setp.eq.f64 	%p209, %fd14, 0d0000000000000000;
	mul.f64 	%fd17, %fd192, 0d3F92492492492492;
	.loc	1 43 33
	add.f64 	%fd77, %fd16, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r376}, %fd77;
	}
	and.b32  	%r377, %r376, 2146435072;
	setp.ne.s32 	%p240, %r377, 2146435072;
	mov.f64 	%fd820, %fd819;
	@%p240 bra 	$L__BB0_70;
	.loc	1 0 33
	mov.f64 	%fd820, %fd77;
	.loc	1 43 33
	@%p368 bra 	$L__BB0_70;
	abs.f64 	%fd254, %fd16;
	setp.nan.f64 	%p242, %fd254, %fd254;
	mov.f64 	%fd820, %fd77;
	@%p242 bra 	$L__BB0_70;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r378, %temp}, %fd16;
	}
	and.b32  	%r55, %r54, 2147483647;
	setp.eq.s32 	%p243, %r55, 2146435072;
	setp.eq.s32 	%p244, %r378, 0;
	and.pred  	%p245, %p243, %p244;
	@!%p245 bra 	$L__BB0_68;
	bra.uni 	$L__BB0_67;
$L__BB0_67:
	setp.gt.f64 	%p252, %fd18, 0d3FF0000000000000;
	selp.b32 	%r387, 2146435072, 0, %p252;
	setp.lt.s32 	%p253, %r54, 0;
	xor.b32  	%r388, %r387, 2146435072;
	selp.b32 	%r389, %r388, %r387, %p253;
	setp.eq.s32 	%p254, %r122, -1;
	selp.b32 	%r390, 1072693248, %r389, %p254;
	mov.b32 	%r391, 0;
	mov.b64 	%fd820, {%r391, %r390};
	bra.uni 	$L__BB0_70;
$L__BB0_68:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r379, %temp}, %fd9;
	}
	and.b32  	%r380, %r41, 2147483647;
	setp.eq.s32 	%p246, %r380, 2146435072;
	setp.eq.s32 	%p247, %r379, 0;
	and.pred  	%p248, %p246, %p247;
	mov.f64 	%fd820, %fd819;
	@!%p248 bra 	$L__BB0_70;
	bra.uni 	$L__BB0_69;
$L__BB0_69:
	setp.lt.s32 	%p250, %r54, 0;
	selp.b32 	%r381, 0, 2146435072, %p250;
	setp.ne.s32 	%p251, %r55, 1071644672;
	or.b32  	%r382, %r381, -2147483648;
	selp.b32 	%r383, %r382, %r381, %p251;
	selp.b32 	%r384, %r383, %r381, %p375;
	selp.b32 	%r385, %r384, %r381, %p361;
	mov.b32 	%r386, 0;
	mov.b64 	%fd820, {%r386, %r385};
$L__BB0_70:
	.loc	1 0 33
	setp.eq.f64 	%p233, %fd15, 0d0000000000000000;
	// begin inline asm
	cvt.f32.bf16 %r128, %rs7;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r136, %rs31;
	// end inline asm
	mov.b32 	%f3, %r127;
	mov.b32 	%f11, %r135;
	selp.f32 	%f17, %f1, %f9, %p80;
	selp.f32 	%f18, %f2, %f10, %p79;
	setp.eq.s32 	%p112, %r122, 1;
	selp.f64 	%fd201, 0d3FF0000000000000, %fd808, %p113;
	selp.f64 	%fd210, 0d3FF0000000000000, %fd810, %p137;
	selp.f64 	%fd219, 0d3FF0000000000000, %fd812, %p161;
	selp.f64 	%fd228, 0d3FF0000000000000, %fd814, %p185;
	selp.f64 	%fd237, 0d3FF0000000000000, %fd816, %p209;
	.loc	1 43 33
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r56}, %fd17;
	}
	bfe.u32 	%r392, %r56, 20, 11;
	add.s32 	%r393, %r392, -1012;
	mov.b64 	%rd76, %fd17;
	shl.b64 	%rd8, %rd76, %r393;
	{ // callseq 7, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd18;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd17;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd256, [retval0+0];
	} // callseq 7
	@%p89 bra 	$L__BB0_72;
	bra.uni 	$L__BB0_71;
$L__BB0_72:
	setp.eq.s64 	%p376, %rd8, -9223372036854775808;
	setp.eq.s64 	%p258, %rd8, -9223372036854775808;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r394, %temp}, %fd256;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r395}, %fd256;
	}
	xor.b32  	%r396, %r395, -2147483648;
	mov.b64 	%fd257, {%r394, %r396};
	selp.f64 	%fd258, %fd257, %fd256, %p258;
	selp.f64 	%fd259, %fd258, %fd256, %p361;
	cvt.rzi.f64.f64 	%fd260, %fd17;
	setp.neu.f64 	%p260, %fd260, %fd17;
	selp.f64 	%fd261, 0dFFF8000000000000, %fd259, %p260;
	selp.f64 	%fd821, %fd261, %fd259, %p361;
	bra.uni 	$L__BB0_73;
$L__BB0_71:
	setp.eq.s64 	%p261, %rd8, -9223372036854775808;
	abs.f64 	%fd262, %fd17;
	setp.neu.f64 	%p262, %fd262, 0d3FE0000000000000;
	and.pred  	%p376, %p262, %p261;
	selp.b32 	%r397, %r41, 0, %p376;
	setp.lt.s32 	%p263, %r56, 0;
	or.b32  	%r398, %r397, 2146435072;
	selp.b32 	%r399, %r398, %r397, %p263;
	mov.b32 	%r400, 0;
	mov.b64 	%fd821, {%r400, %r399};
$L__BB0_73:
	.loc	1 0 33
	setp.eq.f64 	%p257, %fd16, 0d0000000000000000;
	selp.f64 	%fd246, 0d3FF0000000000000, %fd818, %p233;
	// begin inline asm
	cvt.f32.bf16 %r129, %rs9;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r137, %rs33;
	// end inline asm
	mov.b32 	%f4, %r128;
	mov.b32 	%f12, %r136;
	selp.f32 	%f19, %f3, %f11, %p78;
	add.u64 	%rd29, %SP, 0;
	cvt.f64.f32 	%fd1, %f17;
	cvt.f64.f32 	%fd2, %f18;
	.loc	1 43 33
	selp.f64 	%fd27, 0d3FF0000000000000, %fd201, %p112;
	selp.f64 	%fd36, 0d3FF0000000000000, %fd210, %p112;
	selp.f64 	%fd45, 0d3FF0000000000000, %fd219, %p112;
	selp.f64 	%fd54, 0d3FF0000000000000, %fd228, %p112;
	selp.f64 	%fd63, 0d3FF0000000000000, %fd237, %p112;
	add.f64 	%fd86, %fd17, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r401}, %fd86;
	}
	and.b32  	%r402, %r401, 2146435072;
	setp.ne.s32 	%p264, %r402, 2146435072;
	mov.f64 	%fd822, %fd821;
	@%p264 bra 	$L__BB0_80;
	.loc	1 0 33
	mov.f64 	%fd822, %fd86;
	.loc	1 43 33
	@%p368 bra 	$L__BB0_80;
	abs.f64 	%fd263, %fd17;
	setp.nan.f64 	%p266, %fd263, %fd263;
	mov.f64 	%fd822, %fd86;
	@%p266 bra 	$L__BB0_80;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r403, %temp}, %fd17;
	}
	and.b32  	%r57, %r56, 2147483647;
	setp.eq.s32 	%p267, %r57, 2146435072;
	setp.eq.s32 	%p268, %r403, 0;
	and.pred  	%p269, %p267, %p268;
	@!%p269 bra 	$L__BB0_78;
	bra.uni 	$L__BB0_77;
$L__BB0_77:
	setp.gt.f64 	%p276, %fd18, 0d3FF0000000000000;
	selp.b32 	%r412, 2146435072, 0, %p276;
	setp.lt.s32 	%p277, %r56, 0;
	xor.b32  	%r413, %r412, 2146435072;
	selp.b32 	%r414, %r413, %r412, %p277;
	setp.eq.s32 	%p278, %r122, -1;
	selp.b32 	%r415, 1072693248, %r414, %p278;
	mov.b32 	%r416, 0;
	mov.b64 	%fd822, {%r416, %r415};
	bra.uni 	$L__BB0_80;
$L__BB0_78:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r404, %temp}, %fd9;
	}
	and.b32  	%r405, %r41, 2147483647;
	setp.eq.s32 	%p270, %r405, 2146435072;
	setp.eq.s32 	%p271, %r404, 0;
	and.pred  	%p272, %p270, %p271;
	mov.f64 	%fd822, %fd821;
	@!%p272 bra 	$L__BB0_80;
	bra.uni 	$L__BB0_79;
$L__BB0_79:
	setp.lt.s32 	%p274, %r56, 0;
	selp.b32 	%r406, 0, 2146435072, %p274;
	setp.ne.s32 	%p275, %r57, 1071644672;
	or.b32  	%r407, %r406, -2147483648;
	selp.b32 	%r408, %r407, %r406, %p275;
	selp.b32 	%r409, %r408, %r406, %p376;
	selp.b32 	%r410, %r409, %r406, %p361;
	mov.b32 	%r411, 0;
	mov.b64 	%fd822, {%r411, %r410};
$L__BB0_80:
	.loc	1 0 33
	selp.f64 	%fd255, 0d3FF0000000000000, %fd820, %p257;
	// begin inline asm
	cvt.f32.bf16 %r130, %rs11;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r138, %rs35;
	// end inline asm
	selp.f64 	%fd72, 0d3FF0000000000000, %fd246, %p112;
	mov.b32 	%f5, %r129;
	mov.b32 	%f13, %r137;
	selp.f32 	%f20, %f4, %f12, %p77;
	cvt.f64.f32 	%fd3, %f19;
	{ .reg .b64 %tmp;
	  cvta.to.local.u64 	%tmp, %rd29;
	  cvt.u32.u64 	%r1, %tmp; }
	.loc	1 43 33
	setp.eq.f64 	%p280, %fd17, 0d0000000000000000;
	.loc	1 45 20
	mov.b64 	%rd79, %fd27;
	mov.u64 	%rd96, 4607182418800017408;
	// begin inline asm
	div.rn.f64 %rd77, %rd96, %rd79;
	// end inline asm
	mov.b64 	%fd266, %rd77;
	mov.b64 	%rd82, %fd36;
	// begin inline asm
	div.rn.f64 %rd80, %rd96, %rd82;
	// end inline asm
	mov.b64 	%fd267, %rd80;
	mov.b64 	%rd85, %fd45;
	// begin inline asm
	div.rn.f64 %rd83, %rd96, %rd85;
	// end inline asm
	mov.b64 	%fd268, %rd83;
	mov.b64 	%rd88, %fd54;
	// begin inline asm
	div.rn.f64 %rd86, %rd96, %rd88;
	// end inline asm
	mov.b64 	%rd91, %fd63;
	.loc	1 49 20
	mul.f64 	%fd90, %fd266, %fd1;
	mul.f64 	%fd91, %fd267, %fd2;
	.loc	1 50 26
	{
	.reg .b32 %temp; 
	mov.b64 	{%r417, %temp}, %fd90;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r418}, %fd90;
	}
	and.b32  	%r419, %r418, 2147483647;
	setp.eq.s32 	%p281, %r419, 2146435072;
	setp.eq.s32 	%p282, %r417, 0;
	and.pred  	%p25, %p281, %p282;
	mov.f64 	%fd787, 0d3FF921FB54442D18;
	mov.f64 	%fd788, 0d3C91A62633145C00;
	mov.f64 	%fd789, 0d397B839A252049C0;
	abs.f64 	%fd790, %fd90;
	mul.f64 	%fd791, %fd90, 0d3FE45F306DC9C883;
	mov.f64 	%fd806, 0d0000000000000000;
	@%p25 bra 	$L__BB0_137;
	bra.uni 	$L__BB0_81;
$L__BB0_137:
	mul.rn.f64 	%fd824, %fd90, %fd806;
	mov.b32 	%r568, 1;
	bra.uni 	$L__BB0_84;
$L__BB0_81:
	cvt.rni.s32.f64 	%r567, %fd791;
	st.local.u32 	[%r1], %r567;
	cvt.rn.f64.s32 	%fd276, %r567;
	neg.f64 	%fd277, %fd276;
	fma.rn.f64 	%fd279, %fd277, %fd787, %fd90;
	fma.rn.f64 	%fd281, %fd277, %fd788, %fd279;
	fma.rn.f64 	%fd824, %fd277, %fd789, %fd281;
	setp.ltu.f64 	%p283, %fd790, 0d41E0000000000000;
	@%p283 bra 	$L__BB0_83;
	{ // callseq 8, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd90;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd824, [retval0+0];
	} // callseq 8
	ld.local.u32 	%r567, [%r1];
$L__BB0_83:
	add.s32 	%r568, %r567, 1;
$L__BB0_84:
	.loc	1 0 26
	selp.f64 	%fd264, 0d3FF0000000000000, %fd822, %p280;
	// begin inline asm
	cvt.f32.bf16 %r131, %rs13;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r139, %rs37;
	// end inline asm
	selp.f64 	%fd81, 0d3FF0000000000000, %fd255, %p112;
	mov.b32 	%f6, %r130;
	mov.b32 	%f14, %r138;
	mov.b64 	%rd94, %fd72;
	selp.f32 	%f21, %f5, %f13, %p76;
	// begin inline asm
	div.rn.f64 %rd89, %rd96, %rd91;
	// end inline asm
	cvt.f64.f32 	%fd4, %f20;
	mov.b64 	%fd269, %rd86;
	mul.f64 	%fd92, %fd268, %fd3;
	.loc	1 50 26
	and.b32  	%r421, %r568, 1;
	shl.b32 	%r422, %r421, 3;
	mul.wide.u32 	%rd102, %r422, 8;
	mov.u64 	%rd103, __cudart_sin_cos_coeffs;
	add.s64 	%rd104, %rd103, %rd102;
	ld.global.nc.f64 	%fd287, [%rd104+8];
	ld.global.nc.f64 	%fd289, [%rd104+16];
	ld.global.nc.f64 	%fd291, [%rd104+24];
	ld.global.nc.f64 	%fd293, [%rd104+32];
	ld.global.nc.f64 	%fd295, [%rd104+40];
	ld.global.nc.f64 	%fd297, [%rd104+48];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r424, %temp}, %fd91;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r425}, %fd91;
	}
	and.b32  	%r426, %r425, 2147483647;
	setp.eq.s32 	%p286, %r426, 2146435072;
	setp.eq.s32 	%p287, %r424, 0;
	and.pred  	%p26, %p286, %p287;
	abs.f64 	%fd792, %fd91;
	mul.f64 	%fd793, %fd91, 0d3FE45F306DC9C883;
	@%p26 bra 	$L__BB0_138;
	bra.uni 	$L__BB0_85;
$L__BB0_138:
	mul.rn.f64 	%fd826, %fd91, %fd806;
	mov.b32 	%r570, 1;
	bra.uni 	$L__BB0_88;
$L__BB0_85:
	cvt.rni.s32.f64 	%r569, %fd793;
	st.local.u32 	[%r1], %r569;
	cvt.rn.f64.s32 	%fd308, %r569;
	neg.f64 	%fd309, %fd308;
	fma.rn.f64 	%fd311, %fd309, %fd787, %fd91;
	fma.rn.f64 	%fd313, %fd309, %fd788, %fd311;
	fma.rn.f64 	%fd826, %fd309, %fd789, %fd313;
	setp.ltu.f64 	%p288, %fd792, 0d41E0000000000000;
	@%p288 bra 	$L__BB0_87;
	{ // callseq 9, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd91;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd826, [retval0+0];
	} // callseq 9
	ld.local.u32 	%r569, [%r1];
$L__BB0_87:
	add.s32 	%r570, %r569, 1;
$L__BB0_88:
	.loc	1 0 26
	// begin inline asm
	cvt.f32.bf16 %r132, %rs15;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r140, %rs39;
	// end inline asm
	selp.f64 	%fd265, 0d3FF0000000000000, %fd264, %p112;
	mov.b32 	%f7, %r131;
	mov.b32 	%f15, %r139;
	mov.b64 	%rd97, %fd81;
	selp.f32 	%f22, %f6, %f14, %p75;
	// begin inline asm
	div.rn.f64 %rd92, %rd96, %rd94;
	// end inline asm
	cvt.f64.f32 	%fd5, %f21;
	mov.b64 	%fd270, %rd89;
	mul.f64 	%fd93, %fd269, %fd4;
	.loc	1 50 26
	and.b32  	%r428, %r570, 1;
	shl.b32 	%r429, %r428, 3;
	mul.wide.u32 	%rd106, %r429, 8;
	add.s64 	%rd108, %rd103, %rd106;
	ld.global.nc.f64 	%fd319, [%rd108+8];
	ld.global.nc.f64 	%fd321, [%rd108+16];
	ld.global.nc.f64 	%fd323, [%rd108+24];
	ld.global.nc.f64 	%fd325, [%rd108+32];
	ld.global.nc.f64 	%fd327, [%rd108+40];
	ld.global.nc.f64 	%fd329, [%rd108+48];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r431, %temp}, %fd92;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r432}, %fd92;
	}
	and.b32  	%r433, %r432, 2147483647;
	setp.eq.s32 	%p291, %r433, 2146435072;
	setp.eq.s32 	%p292, %r431, 0;
	and.pred  	%p27, %p291, %p292;
	abs.f64 	%fd794, %fd92;
	mul.f64 	%fd795, %fd92, 0d3FE45F306DC9C883;
	@%p27 bra 	$L__BB0_139;
	bra.uni 	$L__BB0_89;
$L__BB0_139:
	mul.rn.f64 	%fd828, %fd92, %fd806;
	mov.b32 	%r572, 1;
	bra.uni 	$L__BB0_92;
$L__BB0_89:
	cvt.rni.s32.f64 	%r571, %fd795;
	st.local.u32 	[%r1], %r571;
	cvt.rn.f64.s32 	%fd340, %r571;
	neg.f64 	%fd341, %fd340;
	fma.rn.f64 	%fd343, %fd341, %fd787, %fd92;
	fma.rn.f64 	%fd345, %fd341, %fd788, %fd343;
	fma.rn.f64 	%fd828, %fd341, %fd789, %fd345;
	setp.ltu.f64 	%p293, %fd794, 0d41E0000000000000;
	@%p293 bra 	$L__BB0_91;
	{ // callseq 10, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd92;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd828, [retval0+0];
	} // callseq 10
	ld.local.u32 	%r571, [%r1];
$L__BB0_91:
	add.s32 	%r572, %r571, 1;
$L__BB0_92:
	.loc	1 0 26
	mov.b32 	%f8, %r132;
	mov.b32 	%f16, %r140;
	mov.b64 	%rd100, %fd265;
	selp.f32 	%f23, %f7, %f15, %p74;
	// begin inline asm
	div.rn.f64 %rd95, %rd96, %rd97;
	// end inline asm
	cvt.f64.f32 	%fd6, %f22;
	mov.b64 	%fd271, %rd92;
	mul.f64 	%fd94, %fd270, %fd5;
	.loc	1 50 26
	and.b32  	%r435, %r572, 1;
	shl.b32 	%r436, %r435, 3;
	mul.wide.u32 	%rd110, %r436, 8;
	add.s64 	%rd112, %rd103, %rd110;
	ld.global.nc.f64 	%fd351, [%rd112+8];
	ld.global.nc.f64 	%fd353, [%rd112+16];
	ld.global.nc.f64 	%fd355, [%rd112+24];
	ld.global.nc.f64 	%fd357, [%rd112+32];
	ld.global.nc.f64 	%fd359, [%rd112+40];
	ld.global.nc.f64 	%fd361, [%rd112+48];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r438, %temp}, %fd93;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r439}, %fd93;
	}
	and.b32  	%r440, %r439, 2147483647;
	setp.eq.s32 	%p296, %r440, 2146435072;
	setp.eq.s32 	%p297, %r438, 0;
	and.pred  	%p28, %p296, %p297;
	abs.f64 	%fd796, %fd93;
	mul.f64 	%fd797, %fd93, 0d3FE45F306DC9C883;
	@%p28 bra 	$L__BB0_140;
	bra.uni 	$L__BB0_93;
$L__BB0_140:
	mul.rn.f64 	%fd830, %fd93, %fd806;
	mov.b32 	%r574, 1;
	bra.uni 	$L__BB0_96;
$L__BB0_93:
	cvt.rni.s32.f64 	%r573, %fd797;
	st.local.u32 	[%r1], %r573;
	cvt.rn.f64.s32 	%fd372, %r573;
	neg.f64 	%fd373, %fd372;
	fma.rn.f64 	%fd375, %fd373, %fd787, %fd93;
	fma.rn.f64 	%fd377, %fd373, %fd788, %fd375;
	fma.rn.f64 	%fd830, %fd373, %fd789, %fd377;
	setp.ltu.f64 	%p298, %fd796, 0d41E0000000000000;
	@%p298 bra 	$L__BB0_95;
	{ // callseq 11, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd93;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd830, [retval0+0];
	} // callseq 11
	ld.local.u32 	%r573, [%r1];
$L__BB0_95:
	add.s32 	%r574, %r573, 1;
$L__BB0_96:
	.loc	1 0 26
	setp.eq.s32 	%p284, %r421, 0;
	setp.eq.s32 	%p289, %r428, 0;
	setp.eq.s32 	%p294, %r435, 0;
	selp.f32 	%f24, %f8, %f16, %p73;
	// begin inline asm
	div.rn.f64 %rd98, %rd96, %rd100;
	// end inline asm
	cvt.f64.f32 	%fd7, %f23;
	mov.b64 	%fd272, %rd95;
	mul.f64 	%fd95, %fd271, %fd6;
	.loc	1 50 26
	and.b32  	%r442, %r574, 1;
	shl.b32 	%r443, %r442, 3;
	mul.wide.u32 	%rd114, %r443, 8;
	add.s64 	%rd116, %rd103, %rd114;
	setp.eq.s32 	%p299, %r442, 0;
	ld.global.nc.f64 	%fd383, [%rd116+8];
	ld.global.nc.f64 	%fd385, [%rd116+16];
	ld.global.nc.f64 	%fd387, [%rd116+24];
	ld.global.nc.f64 	%fd389, [%rd116+32];
	ld.global.nc.f64 	%fd391, [%rd116+40];
	ld.global.nc.f64 	%fd393, [%rd116+48];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r445, %temp}, %fd94;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r446}, %fd94;
	}
	and.b32  	%r447, %r446, 2147483647;
	setp.eq.s32 	%p301, %r447, 2146435072;
	setp.eq.s32 	%p302, %r445, 0;
	and.pred  	%p29, %p301, %p302;
	abs.f64 	%fd798, %fd94;
	mul.f64 	%fd799, %fd94, 0d3FE45F306DC9C883;
	@%p29 bra 	$L__BB0_141;
	bra.uni 	$L__BB0_97;
$L__BB0_141:
	mul.rn.f64 	%fd832, %fd94, %fd806;
	mov.b32 	%r576, 1;
	bra.uni 	$L__BB0_100;
$L__BB0_97:
	cvt.rni.s32.f64 	%r575, %fd799;
	st.local.u32 	[%r1], %r575;
	cvt.rn.f64.s32 	%fd404, %r575;
	neg.f64 	%fd405, %fd404;
	fma.rn.f64 	%fd407, %fd405, %fd787, %fd94;
	fma.rn.f64 	%fd409, %fd405, %fd788, %fd407;
	fma.rn.f64 	%fd832, %fd405, %fd789, %fd409;
	setp.ltu.f64 	%p303, %fd798, 0d41E0000000000000;
	@%p303 bra 	$L__BB0_99;
	{ // callseq 12, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd94;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd832, [retval0+0];
	} // callseq 12
	ld.local.u32 	%r575, [%r1];
$L__BB0_99:
	add.s32 	%r576, %r575, 1;
$L__BB0_100:
	.loc	1 0 26
	mul.rn.f64 	%fd285, %fd824, %fd824;
	selp.f64 	%fd286, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p284;
	mul.rn.f64 	%fd317, %fd826, %fd826;
	selp.f64 	%fd318, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p289;
	mul.rn.f64 	%fd349, %fd828, %fd828;
	selp.f64 	%fd350, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p294;
	mul.rn.f64 	%fd381, %fd830, %fd830;
	selp.f64 	%fd382, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p299;
	cvt.f64.f32 	%fd8, %f24;
	mov.b64 	%fd273, %rd98;
	mul.f64 	%fd96, %fd272, %fd7;
	.loc	1 50 26
	and.b32  	%r449, %r576, 1;
	shl.b32 	%r450, %r449, 3;
	mul.wide.u32 	%rd118, %r450, 8;
	add.s64 	%rd120, %rd103, %rd118;
	mul.rn.f64 	%fd413, %fd832, %fd832;
	setp.eq.s32 	%p304, %r449, 0;
	selp.f64 	%fd414, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p304;
	ld.global.nc.f64 	%fd415, [%rd120+8];
	ld.global.nc.f64 	%fd417, [%rd120+16];
	ld.global.nc.f64 	%fd419, [%rd120+24];
	ld.global.nc.f64 	%fd421, [%rd120+32];
	ld.global.nc.f64 	%fd423, [%rd120+40];
	ld.global.nc.f64 	%fd425, [%rd120+48];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r452, %temp}, %fd95;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r453}, %fd95;
	}
	and.b32  	%r454, %r453, 2147483647;
	setp.eq.s32 	%p306, %r454, 2146435072;
	setp.eq.s32 	%p307, %r452, 0;
	and.pred  	%p30, %p306, %p307;
	abs.f64 	%fd800, %fd95;
	mul.f64 	%fd801, %fd95, 0d3FE45F306DC9C883;
	@%p30 bra 	$L__BB0_142;
	bra.uni 	$L__BB0_101;
$L__BB0_142:
	mul.rn.f64 	%fd834, %fd95, %fd806;
	mov.b32 	%r578, 1;
	bra.uni 	$L__BB0_104;
$L__BB0_101:
	cvt.rni.s32.f64 	%r577, %fd801;
	st.local.u32 	[%r1], %r577;
	cvt.rn.f64.s32 	%fd436, %r577;
	neg.f64 	%fd437, %fd436;
	fma.rn.f64 	%fd439, %fd437, %fd787, %fd95;
	fma.rn.f64 	%fd441, %fd437, %fd788, %fd439;
	fma.rn.f64 	%fd834, %fd437, %fd789, %fd441;
	setp.ltu.f64 	%p308, %fd800, 0d41E0000000000000;
	@%p308 bra 	$L__BB0_103;
	{ // callseq 13, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd95;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd834, [retval0+0];
	} // callseq 13
	ld.local.u32 	%r577, [%r1];
$L__BB0_103:
	add.s32 	%r578, %r577, 1;
$L__BB0_104:
	.loc	1 0 26
	fma.rn.f64 	%fd288, %fd286, %fd285, %fd287;
	fma.rn.f64 	%fd320, %fd318, %fd317, %fd319;
	fma.rn.f64 	%fd352, %fd350, %fd349, %fd351;
	fma.rn.f64 	%fd384, %fd382, %fd381, %fd383;
	fma.rn.f64 	%fd416, %fd414, %fd413, %fd415;
	mul.f64 	%fd97, %fd273, %fd8;
	.loc	1 50 26
	and.b32  	%r456, %r578, 1;
	shl.b32 	%r457, %r456, 3;
	mul.wide.u32 	%rd122, %r457, 8;
	add.s64 	%rd124, %rd103, %rd122;
	mul.rn.f64 	%fd445, %fd834, %fd834;
	setp.eq.s32 	%p309, %r456, 0;
	selp.f64 	%fd446, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p309;
	ld.global.nc.f64 	%fd447, [%rd124+8];
	fma.rn.f64 	%fd448, %fd446, %fd445, %fd447;
	ld.global.nc.f64 	%fd449, [%rd124+16];
	ld.global.nc.f64 	%fd451, [%rd124+24];
	ld.global.nc.f64 	%fd453, [%rd124+32];
	ld.global.nc.f64 	%fd455, [%rd124+40];
	ld.global.nc.f64 	%fd457, [%rd124+48];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r459, %temp}, %fd96;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r460}, %fd96;
	}
	and.b32  	%r461, %r460, 2147483647;
	setp.eq.s32 	%p311, %r461, 2146435072;
	setp.eq.s32 	%p312, %r459, 0;
	and.pred  	%p31, %p311, %p312;
	abs.f64 	%fd802, %fd96;
	mul.f64 	%fd803, %fd96, 0d3FE45F306DC9C883;
	@%p31 bra 	$L__BB0_143;
	bra.uni 	$L__BB0_105;
$L__BB0_143:
	mul.rn.f64 	%fd836, %fd96, %fd806;
	mov.b32 	%r580, 1;
	bra.uni 	$L__BB0_108;
$L__BB0_105:
	cvt.rni.s32.f64 	%r579, %fd803;
	st.local.u32 	[%r1], %r579;
	cvt.rn.f64.s32 	%fd468, %r579;
	neg.f64 	%fd469, %fd468;
	fma.rn.f64 	%fd471, %fd469, %fd787, %fd96;
	fma.rn.f64 	%fd473, %fd469, %fd788, %fd471;
	fma.rn.f64 	%fd836, %fd469, %fd789, %fd473;
	setp.ltu.f64 	%p313, %fd802, 0d41E0000000000000;
	@%p313 bra 	$L__BB0_107;
	{ // callseq 14, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd96;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd836, [retval0+0];
	} // callseq 14
	ld.local.u32 	%r579, [%r1];
$L__BB0_107:
	add.s32 	%r580, %r579, 1;
$L__BB0_108:
	.loc	1 0 26
	fma.rn.f64 	%fd290, %fd288, %fd285, %fd289;
	fma.rn.f64 	%fd322, %fd320, %fd317, %fd321;
	fma.rn.f64 	%fd354, %fd352, %fd349, %fd353;
	fma.rn.f64 	%fd386, %fd384, %fd381, %fd385;
	fma.rn.f64 	%fd418, %fd416, %fd413, %fd417;
	.loc	1 50 26
	fma.rn.f64 	%fd450, %fd448, %fd445, %fd449;
	and.b32  	%r463, %r580, 1;
	shl.b32 	%r464, %r463, 3;
	mul.wide.u32 	%rd126, %r464, 8;
	add.s64 	%rd128, %rd103, %rd126;
	mul.rn.f64 	%fd477, %fd836, %fd836;
	setp.eq.s32 	%p314, %r463, 0;
	selp.f64 	%fd478, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p314;
	ld.global.nc.f64 	%fd479, [%rd128+8];
	fma.rn.f64 	%fd480, %fd478, %fd477, %fd479;
	ld.global.nc.f64 	%fd481, [%rd128+16];
	fma.rn.f64 	%fd482, %fd480, %fd477, %fd481;
	ld.global.nc.f64 	%fd483, [%rd128+24];
	ld.global.nc.f64 	%fd485, [%rd128+32];
	ld.global.nc.f64 	%fd487, [%rd128+40];
	ld.global.nc.f64 	%fd489, [%rd128+48];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r466, %temp}, %fd97;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r467}, %fd97;
	}
	and.b32  	%r468, %r467, 2147483647;
	setp.eq.s32 	%p316, %r468, 2146435072;
	setp.eq.s32 	%p317, %r466, 0;
	and.pred  	%p32, %p316, %p317;
	abs.f64 	%fd804, %fd97;
	mul.f64 	%fd805, %fd97, 0d3FE45F306DC9C883;
	@%p32 bra 	$L__BB0_144;
	bra.uni 	$L__BB0_109;
$L__BB0_144:
	.loc	1 0 26
	mov.b32 	%r582, 1;
	.loc	1 50 26
	mul.rn.f64 	%fd838, %fd97, %fd806;
	bra.uni 	$L__BB0_112;
$L__BB0_109:
	cvt.rni.s32.f64 	%r581, %fd805;
	st.local.u32 	[%r1], %r581;
	cvt.rn.f64.s32 	%fd500, %r581;
	neg.f64 	%fd501, %fd500;
	fma.rn.f64 	%fd503, %fd501, %fd787, %fd97;
	fma.rn.f64 	%fd505, %fd501, %fd788, %fd503;
	fma.rn.f64 	%fd838, %fd501, %fd789, %fd505;
	setp.ltu.f64 	%p318, %fd804, 0d41E0000000000000;
	@%p318 bra 	$L__BB0_111;
	{ // callseq 15, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd97;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd838, [retval0+0];
	} // callseq 15
	ld.local.u32 	%r581, [%r1];
$L__BB0_111:
	add.s32 	%r582, %r581, 1;
$L__BB0_112:
	.loc	1 0 26
	fma.rn.f64 	%fd292, %fd290, %fd285, %fd291;
	fma.rn.f64 	%fd324, %fd322, %fd317, %fd323;
	fma.rn.f64 	%fd356, %fd354, %fd349, %fd355;
	fma.rn.f64 	%fd388, %fd386, %fd381, %fd387;
	fma.rn.f64 	%fd420, %fd418, %fd413, %fd419;
	.loc	1 50 26
	fma.rn.f64 	%fd452, %fd450, %fd445, %fd451;
	fma.rn.f64 	%fd484, %fd482, %fd477, %fd483;
	and.b32  	%r470, %r582, 1;
	shl.b32 	%r471, %r470, 3;
	mul.wide.u32 	%rd130, %r471, 8;
	add.s64 	%rd132, %rd103, %rd130;
	mul.rn.f64 	%fd509, %fd838, %fd838;
	setp.eq.s32 	%p319, %r470, 0;
	selp.f64 	%fd510, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p319;
	ld.global.nc.f64 	%fd511, [%rd132+8];
	fma.rn.f64 	%fd512, %fd510, %fd509, %fd511;
	ld.global.nc.f64 	%fd513, [%rd132+16];
	fma.rn.f64 	%fd514, %fd512, %fd509, %fd513;
	ld.global.nc.f64 	%fd515, [%rd132+24];
	fma.rn.f64 	%fd516, %fd514, %fd509, %fd515;
	ld.global.nc.f64 	%fd517, [%rd132+32];
	ld.global.nc.f64 	%fd519, [%rd132+40];
	ld.global.nc.f64 	%fd521, [%rd132+48];
	.loc	1 52 26
	@%p25 bra 	$L__BB0_145;
	bra.uni 	$L__BB0_113;
$L__BB0_145:
	mul.rn.f64 	%fd839, %fd90, %fd806;
	mov.b32 	%r583, 0;
	bra.uni 	$L__BB0_115;
$L__BB0_113:
	cvt.rni.s32.f64 	%r583, %fd791;
	st.local.u32 	[%r1], %r583;
	cvt.rn.f64.s32 	%fd532, %r583;
	neg.f64 	%fd533, %fd532;
	fma.rn.f64 	%fd535, %fd533, %fd787, %fd90;
	fma.rn.f64 	%fd537, %fd533, %fd788, %fd535;
	fma.rn.f64 	%fd839, %fd533, %fd789, %fd537;
	setp.ltu.f64 	%p321, %fd790, 0d41E0000000000000;
	@%p321 bra 	$L__BB0_115;
	{ // callseq 16, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd90;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd839, [retval0+0];
	} // callseq 16
	ld.local.u32 	%r583, [%r1];
$L__BB0_115:
	.loc	1 0 26
	fma.rn.f64 	%fd294, %fd292, %fd285, %fd293;
	fma.rn.f64 	%fd326, %fd324, %fd317, %fd325;
	fma.rn.f64 	%fd358, %fd356, %fd349, %fd357;
	fma.rn.f64 	%fd390, %fd388, %fd381, %fd389;
	fma.rn.f64 	%fd422, %fd420, %fd413, %fd421;
	fma.rn.f64 	%fd454, %fd452, %fd445, %fd453;
	fma.rn.f64 	%fd486, %fd484, %fd477, %fd485;
	fma.rn.f64 	%fd518, %fd516, %fd509, %fd517;
	.loc	1 52 26
	and.b32  	%r474, %r583, 1;
	shl.b32 	%r475, %r474, 3;
	mul.wide.u32 	%rd134, %r475, 8;
	add.s64 	%rd136, %rd103, %rd134;
	mul.rn.f64 	%fd541, %fd839, %fd839;
	setp.eq.s32 	%p322, %r474, 0;
	selp.f64 	%fd542, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p322;
	ld.global.nc.f64 	%fd543, [%rd136+8];
	fma.rn.f64 	%fd544, %fd542, %fd541, %fd543;
	ld.global.nc.f64 	%fd545, [%rd136+16];
	fma.rn.f64 	%fd546, %fd544, %fd541, %fd545;
	ld.global.nc.f64 	%fd547, [%rd136+24];
	fma.rn.f64 	%fd548, %fd546, %fd541, %fd547;
	ld.global.nc.f64 	%fd549, [%rd136+32];
	fma.rn.f64 	%fd550, %fd548, %fd541, %fd549;
	ld.global.nc.f64 	%fd551, [%rd136+40];
	ld.global.nc.f64 	%fd553, [%rd136+48];
	@%p26 bra 	$L__BB0_146;
	bra.uni 	$L__BB0_116;
$L__BB0_146:
	mul.rn.f64 	%fd840, %fd91, %fd806;
	mov.b32 	%r584, 0;
	bra.uni 	$L__BB0_118;
$L__BB0_116:
	cvt.rni.s32.f64 	%r584, %fd793;
	st.local.u32 	[%r1], %r584;
	cvt.rn.f64.s32 	%fd564, %r584;
	neg.f64 	%fd565, %fd564;
	fma.rn.f64 	%fd567, %fd565, %fd787, %fd91;
	fma.rn.f64 	%fd569, %fd565, %fd788, %fd567;
	fma.rn.f64 	%fd840, %fd565, %fd789, %fd569;
	setp.ltu.f64 	%p324, %fd792, 0d41E0000000000000;
	@%p324 bra 	$L__BB0_118;
	{ // callseq 17, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd91;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd840, [retval0+0];
	} // callseq 17
	ld.local.u32 	%r584, [%r1];
$L__BB0_118:
	.loc	1 0 26
	fma.rn.f64 	%fd296, %fd294, %fd285, %fd295;
	fma.rn.f64 	%fd328, %fd326, %fd317, %fd327;
	fma.rn.f64 	%fd360, %fd358, %fd349, %fd359;
	fma.rn.f64 	%fd392, %fd390, %fd381, %fd391;
	fma.rn.f64 	%fd424, %fd422, %fd413, %fd423;
	fma.rn.f64 	%fd456, %fd454, %fd445, %fd455;
	fma.rn.f64 	%fd488, %fd486, %fd477, %fd487;
	fma.rn.f64 	%fd520, %fd518, %fd509, %fd519;
	.loc	1 52 26
	fma.rn.f64 	%fd552, %fd550, %fd541, %fd551;
	and.b32  	%r478, %r584, 1;
	shl.b32 	%r479, %r478, 3;
	mul.wide.u32 	%rd138, %r479, 8;
	add.s64 	%rd140, %rd103, %rd138;
	mul.rn.f64 	%fd573, %fd840, %fd840;
	setp.eq.s32 	%p325, %r478, 0;
	selp.f64 	%fd574, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p325;
	ld.global.nc.f64 	%fd575, [%rd140+8];
	fma.rn.f64 	%fd576, %fd574, %fd573, %fd575;
	ld.global.nc.f64 	%fd577, [%rd140+16];
	fma.rn.f64 	%fd578, %fd576, %fd573, %fd577;
	ld.global.nc.f64 	%fd579, [%rd140+24];
	fma.rn.f64 	%fd580, %fd578, %fd573, %fd579;
	ld.global.nc.f64 	%fd581, [%rd140+32];
	fma.rn.f64 	%fd582, %fd580, %fd573, %fd581;
	ld.global.nc.f64 	%fd583, [%rd140+40];
	fma.rn.f64 	%fd584, %fd582, %fd573, %fd583;
	ld.global.nc.f64 	%fd585, [%rd140+48];
	@%p27 bra 	$L__BB0_147;
	bra.uni 	$L__BB0_119;
$L__BB0_147:
	mul.rn.f64 	%fd841, %fd92, %fd806;
	mov.b32 	%r585, 0;
	bra.uni 	$L__BB0_121;
$L__BB0_119:
	cvt.rni.s32.f64 	%r585, %fd795;
	st.local.u32 	[%r1], %r585;
	cvt.rn.f64.s32 	%fd596, %r585;
	neg.f64 	%fd597, %fd596;
	fma.rn.f64 	%fd599, %fd597, %fd787, %fd92;
	fma.rn.f64 	%fd601, %fd597, %fd788, %fd599;
	fma.rn.f64 	%fd841, %fd597, %fd789, %fd601;
	setp.ltu.f64 	%p327, %fd794, 0d41E0000000000000;
	@%p327 bra 	$L__BB0_121;
	{ // callseq 18, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd92;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd841, [retval0+0];
	} // callseq 18
	ld.local.u32 	%r585, [%r1];
$L__BB0_121:
	.loc	1 0 26
	fma.rn.f64 	%fd298, %fd296, %fd285, %fd297;
	mov.f64 	%fd300, 0d3FF0000000000000;
	fma.rn.f64 	%fd330, %fd328, %fd317, %fd329;
	fma.rn.f64 	%fd362, %fd360, %fd349, %fd361;
	fma.rn.f64 	%fd394, %fd392, %fd381, %fd393;
	fma.rn.f64 	%fd426, %fd424, %fd413, %fd425;
	fma.rn.f64 	%fd458, %fd456, %fd445, %fd457;
	fma.rn.f64 	%fd490, %fd488, %fd477, %fd489;
	fma.rn.f64 	%fd522, %fd520, %fd509, %fd521;
	.loc	1 52 26
	fma.rn.f64 	%fd554, %fd552, %fd541, %fd553;
	fma.rn.f64 	%fd586, %fd584, %fd573, %fd585;
	and.b32  	%r482, %r585, 1;
	shl.b32 	%r483, %r482, 3;
	mul.wide.u32 	%rd142, %r483, 8;
	add.s64 	%rd144, %rd103, %rd142;
	mul.rn.f64 	%fd605, %fd841, %fd841;
	setp.eq.s32 	%p328, %r482, 0;
	selp.f64 	%fd606, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p328;
	ld.global.nc.f64 	%fd607, [%rd144+8];
	fma.rn.f64 	%fd608, %fd606, %fd605, %fd607;
	ld.global.nc.f64 	%fd609, [%rd144+16];
	fma.rn.f64 	%fd610, %fd608, %fd605, %fd609;
	ld.global.nc.f64 	%fd611, [%rd144+24];
	fma.rn.f64 	%fd612, %fd610, %fd605, %fd611;
	ld.global.nc.f64 	%fd613, [%rd144+32];
	fma.rn.f64 	%fd614, %fd612, %fd605, %fd613;
	ld.global.nc.f64 	%fd615, [%rd144+40];
	fma.rn.f64 	%fd616, %fd614, %fd605, %fd615;
	ld.global.nc.f64 	%fd617, [%rd144+48];
	fma.rn.f64 	%fd618, %fd616, %fd605, %fd617;
	@%p28 bra 	$L__BB0_148;
	bra.uni 	$L__BB0_122;
$L__BB0_148:
	mul.rn.f64 	%fd842, %fd93, %fd806;
	mov.b32 	%r586, 0;
	bra.uni 	$L__BB0_124;
$L__BB0_122:
	cvt.rni.s32.f64 	%r586, %fd797;
	st.local.u32 	[%r1], %r586;
	cvt.rn.f64.s32 	%fd628, %r586;
	neg.f64 	%fd629, %fd628;
	fma.rn.f64 	%fd631, %fd629, %fd787, %fd93;
	fma.rn.f64 	%fd633, %fd629, %fd788, %fd631;
	fma.rn.f64 	%fd842, %fd629, %fd789, %fd633;
	setp.ltu.f64 	%p330, %fd796, 0d41E0000000000000;
	@%p330 bra 	$L__BB0_124;
	{ // callseq 19, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd93;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd842, [retval0+0];
	} // callseq 19
	ld.local.u32 	%r586, [%r1];
$L__BB0_124:
	.loc	1 0 26
	fma.rn.f64 	%fd299, %fd298, %fd824, %fd824;
	fma.rn.f64 	%fd301, %fd298, %fd285, %fd300;
	fma.rn.f64 	%fd331, %fd330, %fd826, %fd826;
	fma.rn.f64 	%fd333, %fd330, %fd317, %fd300;
	fma.rn.f64 	%fd363, %fd362, %fd828, %fd828;
	fma.rn.f64 	%fd365, %fd362, %fd349, %fd300;
	fma.rn.f64 	%fd395, %fd394, %fd830, %fd830;
	fma.rn.f64 	%fd397, %fd394, %fd381, %fd300;
	fma.rn.f64 	%fd427, %fd426, %fd832, %fd832;
	fma.rn.f64 	%fd429, %fd426, %fd413, %fd300;
	fma.rn.f64 	%fd459, %fd458, %fd834, %fd834;
	fma.rn.f64 	%fd461, %fd458, %fd445, %fd300;
	fma.rn.f64 	%fd491, %fd490, %fd836, %fd836;
	fma.rn.f64 	%fd493, %fd490, %fd477, %fd300;
	fma.rn.f64 	%fd523, %fd522, %fd838, %fd838;
	fma.rn.f64 	%fd525, %fd522, %fd509, %fd300;
	.loc	1 52 26
	fma.rn.f64 	%fd555, %fd554, %fd839, %fd839;
	fma.rn.f64 	%fd557, %fd554, %fd541, %fd300;
	fma.rn.f64 	%fd587, %fd586, %fd840, %fd840;
	fma.rn.f64 	%fd589, %fd586, %fd573, %fd300;
	fma.rn.f64 	%fd619, %fd618, %fd841, %fd841;
	fma.rn.f64 	%fd621, %fd618, %fd605, %fd300;
	and.b32  	%r486, %r586, 1;
	shl.b32 	%r487, %r486, 3;
	mul.wide.u32 	%rd146, %r487, 8;
	add.s64 	%rd148, %rd103, %rd146;
	mul.rn.f64 	%fd637, %fd842, %fd842;
	setp.eq.s32 	%p331, %r486, 0;
	selp.f64 	%fd638, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p331;
	ld.global.nc.f64 	%fd639, [%rd148+8];
	fma.rn.f64 	%fd640, %fd638, %fd637, %fd639;
	ld.global.nc.f64 	%fd641, [%rd148+16];
	fma.rn.f64 	%fd642, %fd640, %fd637, %fd641;
	ld.global.nc.f64 	%fd643, [%rd148+24];
	fma.rn.f64 	%fd644, %fd642, %fd637, %fd643;
	ld.global.nc.f64 	%fd645, [%rd148+32];
	fma.rn.f64 	%fd646, %fd644, %fd637, %fd645;
	ld.global.nc.f64 	%fd647, [%rd148+40];
	fma.rn.f64 	%fd648, %fd646, %fd637, %fd647;
	ld.global.nc.f64 	%fd649, [%rd148+48];
	fma.rn.f64 	%fd650, %fd648, %fd637, %fd649;
	fma.rn.f64 	%fd651, %fd650, %fd842, %fd842;
	fma.rn.f64 	%fd653, %fd650, %fd637, %fd300;
	@%p29 bra 	$L__BB0_149;
	bra.uni 	$L__BB0_125;
$L__BB0_149:
	mul.rn.f64 	%fd843, %fd94, %fd806;
	mov.b32 	%r587, 0;
	bra.uni 	$L__BB0_127;
$L__BB0_125:
	cvt.rni.s32.f64 	%r587, %fd799;
	st.local.u32 	[%r1], %r587;
	cvt.rn.f64.s32 	%fd660, %r587;
	neg.f64 	%fd661, %fd660;
	fma.rn.f64 	%fd663, %fd661, %fd787, %fd94;
	fma.rn.f64 	%fd665, %fd661, %fd788, %fd663;
	fma.rn.f64 	%fd843, %fd661, %fd789, %fd665;
	setp.ltu.f64 	%p333, %fd798, 0d41E0000000000000;
	@%p333 bra 	$L__BB0_127;
	{ // callseq 20, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd94;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd843, [retval0+0];
	} // callseq 20
	ld.local.u32 	%r587, [%r1];
$L__BB0_127:
	.loc	1 0 26
	selp.f64 	%fd302, %fd299, %fd301, %p284;
	and.b32  	%r423, %r568, 2;
	mov.f64 	%fd304, 0dBFF0000000000000;
	selp.f64 	%fd334, %fd331, %fd333, %p289;
	and.b32  	%r430, %r570, 2;
	selp.f64 	%fd366, %fd363, %fd365, %p294;
	and.b32  	%r437, %r572, 2;
	selp.f64 	%fd398, %fd395, %fd397, %p299;
	and.b32  	%r444, %r574, 2;
	selp.f64 	%fd430, %fd427, %fd429, %p304;
	and.b32  	%r451, %r576, 2;
	selp.f64 	%fd462, %fd459, %fd461, %p309;
	and.b32  	%r458, %r578, 2;
	selp.f64 	%fd494, %fd491, %fd493, %p314;
	and.b32  	%r465, %r580, 2;
	selp.f64 	%fd526, %fd523, %fd525, %p319;
	and.b32  	%r472, %r582, 2;
	.loc	1 52 26
	selp.f64 	%fd558, %fd555, %fd557, %p322;
	and.b32  	%r476, %r583, 2;
	selp.f64 	%fd590, %fd587, %fd589, %p325;
	and.b32  	%r480, %r584, 2;
	selp.f64 	%fd622, %fd619, %fd621, %p328;
	and.b32  	%r484, %r585, 2;
	selp.f64 	%fd654, %fd651, %fd653, %p331;
	and.b32  	%r488, %r586, 2;
	and.b32  	%r490, %r587, 1;
	shl.b32 	%r491, %r490, 3;
	mul.wide.u32 	%rd150, %r491, 8;
	add.s64 	%rd152, %rd103, %rd150;
	mul.rn.f64 	%fd669, %fd843, %fd843;
	setp.eq.s32 	%p334, %r490, 0;
	selp.f64 	%fd670, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p334;
	ld.global.nc.f64 	%fd671, [%rd152+8];
	fma.rn.f64 	%fd672, %fd670, %fd669, %fd671;
	ld.global.nc.f64 	%fd673, [%rd152+16];
	fma.rn.f64 	%fd674, %fd672, %fd669, %fd673;
	ld.global.nc.f64 	%fd675, [%rd152+24];
	fma.rn.f64 	%fd676, %fd674, %fd669, %fd675;
	ld.global.nc.f64 	%fd677, [%rd152+32];
	fma.rn.f64 	%fd678, %fd676, %fd669, %fd677;
	ld.global.nc.f64 	%fd679, [%rd152+40];
	fma.rn.f64 	%fd680, %fd678, %fd669, %fd679;
	ld.global.nc.f64 	%fd681, [%rd152+48];
	fma.rn.f64 	%fd682, %fd680, %fd669, %fd681;
	fma.rn.f64 	%fd683, %fd682, %fd843, %fd843;
	fma.rn.f64 	%fd685, %fd682, %fd669, %fd300;
	selp.f64 	%fd686, %fd683, %fd685, %p334;
	and.b32  	%r492, %r587, 2;
	@%p30 bra 	$L__BB0_150;
	bra.uni 	$L__BB0_128;
$L__BB0_150:
	mul.rn.f64 	%fd844, %fd95, %fd806;
	mov.b32 	%r588, 0;
	bra.uni 	$L__BB0_130;
$L__BB0_128:
	cvt.rni.s32.f64 	%r588, %fd801;
	st.local.u32 	[%r1], %r588;
	cvt.rn.f64.s32 	%fd692, %r588;
	neg.f64 	%fd693, %fd692;
	fma.rn.f64 	%fd695, %fd693, %fd787, %fd95;
	fma.rn.f64 	%fd697, %fd693, %fd788, %fd695;
	fma.rn.f64 	%fd844, %fd693, %fd789, %fd697;
	setp.ltu.f64 	%p336, %fd800, 0d41E0000000000000;
	@%p336 bra 	$L__BB0_130;
	{ // callseq 21, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd95;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd844, [retval0+0];
	} // callseq 21
	ld.local.u32 	%r588, [%r1];
$L__BB0_130:
	.loc	1 0 26
	setp.eq.s32 	%p285, %r423, 0;
	fma.rn.f64 	%fd305, %fd302, %fd304, %fd806;
	setp.eq.s32 	%p290, %r430, 0;
	fma.rn.f64 	%fd337, %fd334, %fd304, %fd806;
	setp.eq.s32 	%p295, %r437, 0;
	fma.rn.f64 	%fd369, %fd366, %fd304, %fd806;
	setp.eq.s32 	%p300, %r444, 0;
	fma.rn.f64 	%fd401, %fd398, %fd304, %fd806;
	setp.eq.s32 	%p305, %r451, 0;
	fma.rn.f64 	%fd433, %fd430, %fd304, %fd806;
	setp.eq.s32 	%p310, %r458, 0;
	fma.rn.f64 	%fd465, %fd462, %fd304, %fd806;
	setp.eq.s32 	%p315, %r465, 0;
	fma.rn.f64 	%fd497, %fd494, %fd304, %fd806;
	setp.eq.s32 	%p320, %r472, 0;
	fma.rn.f64 	%fd529, %fd526, %fd304, %fd806;
	.loc	1 52 26
	setp.eq.s32 	%p323, %r476, 0;
	fma.rn.f64 	%fd561, %fd558, %fd304, %fd806;
	setp.eq.s32 	%p326, %r480, 0;
	fma.rn.f64 	%fd593, %fd590, %fd304, %fd806;
	setp.eq.s32 	%p329, %r484, 0;
	fma.rn.f64 	%fd625, %fd622, %fd304, %fd806;
	setp.eq.s32 	%p332, %r488, 0;
	fma.rn.f64 	%fd657, %fd654, %fd304, %fd806;
	setp.eq.s32 	%p335, %r492, 0;
	fma.rn.f64 	%fd689, %fd686, %fd304, %fd806;
	and.b32  	%r494, %r588, 1;
	shl.b32 	%r495, %r494, 3;
	mul.wide.u32 	%rd154, %r495, 8;
	add.s64 	%rd156, %rd103, %rd154;
	mul.rn.f64 	%fd701, %fd844, %fd844;
	setp.eq.s32 	%p337, %r494, 0;
	selp.f64 	%fd702, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p337;
	ld.global.nc.f64 	%fd703, [%rd156+8];
	fma.rn.f64 	%fd704, %fd702, %fd701, %fd703;
	ld.global.nc.f64 	%fd705, [%rd156+16];
	fma.rn.f64 	%fd706, %fd704, %fd701, %fd705;
	ld.global.nc.f64 	%fd707, [%rd156+24];
	fma.rn.f64 	%fd708, %fd706, %fd701, %fd707;
	ld.global.nc.f64 	%fd709, [%rd156+32];
	fma.rn.f64 	%fd710, %fd708, %fd701, %fd709;
	ld.global.nc.f64 	%fd711, [%rd156+40];
	fma.rn.f64 	%fd712, %fd710, %fd701, %fd711;
	ld.global.nc.f64 	%fd713, [%rd156+48];
	fma.rn.f64 	%fd714, %fd712, %fd701, %fd713;
	fma.rn.f64 	%fd715, %fd714, %fd844, %fd844;
	fma.rn.f64 	%fd717, %fd714, %fd701, %fd300;
	selp.f64 	%fd718, %fd715, %fd717, %p337;
	and.b32  	%r496, %r588, 2;
	setp.eq.s32 	%p338, %r496, 0;
	fma.rn.f64 	%fd721, %fd718, %fd304, %fd806;
	@%p31 bra 	$L__BB0_151;
	bra.uni 	$L__BB0_131;
$L__BB0_151:
	mul.rn.f64 	%fd845, %fd96, %fd806;
	mov.b32 	%r589, 0;
	bra.uni 	$L__BB0_133;
$L__BB0_131:
	cvt.rni.s32.f64 	%r589, %fd803;
	st.local.u32 	[%r1], %r589;
	cvt.rn.f64.s32 	%fd724, %r589;
	neg.f64 	%fd725, %fd724;
	fma.rn.f64 	%fd727, %fd725, %fd787, %fd96;
	fma.rn.f64 	%fd729, %fd725, %fd788, %fd727;
	fma.rn.f64 	%fd845, %fd725, %fd789, %fd729;
	setp.ltu.f64 	%p339, %fd802, 0d41E0000000000000;
	@%p339 bra 	$L__BB0_133;
	{ // callseq 22, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd96;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd845, [retval0+0];
	} // callseq 22
	ld.local.u32 	%r589, [%r1];
$L__BB0_133:
	.loc	1 0 26
	ld.param.u64 	%rd10, [triton__param_3];
	ld.param.u64 	%rd9, [triton__param_2];
	selp.f64 	%fd103, %fd302, %fd305, %p285;
	selp.f64 	%fd109, %fd334, %fd337, %p290;
	selp.f64 	%fd115, %fd366, %fd369, %p295;
	selp.f64 	%fd121, %fd398, %fd401, %p300;
	selp.f64 	%fd127, %fd430, %fd433, %p305;
	selp.f64 	%fd133, %fd462, %fd465, %p310;
	selp.f64 	%fd139, %fd494, %fd497, %p315;
	selp.f64 	%fd145, %fd526, %fd529, %p320;
	.loc	1 52 26
	selp.f64 	%fd150, %fd558, %fd561, %p323;
	selp.f64 	%fd155, %fd590, %fd593, %p326;
	selp.f64 	%fd160, %fd622, %fd625, %p329;
	selp.f64 	%fd165, %fd654, %fd657, %p332;
	selp.f64 	%fd170, %fd686, %fd689, %p335;
	selp.f64 	%fd175, %fd718, %fd721, %p338;
	and.b32  	%r498, %r589, 1;
	shl.b32 	%r499, %r498, 3;
	mul.wide.u32 	%rd158, %r499, 8;
	add.s64 	%rd160, %rd103, %rd158;
	mul.rn.f64 	%fd733, %fd845, %fd845;
	setp.eq.s32 	%p340, %r498, 0;
	selp.f64 	%fd734, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p340;
	ld.global.nc.f64 	%fd735, [%rd160+8];
	fma.rn.f64 	%fd736, %fd734, %fd733, %fd735;
	ld.global.nc.f64 	%fd737, [%rd160+16];
	fma.rn.f64 	%fd738, %fd736, %fd733, %fd737;
	ld.global.nc.f64 	%fd739, [%rd160+24];
	fma.rn.f64 	%fd740, %fd738, %fd733, %fd739;
	ld.global.nc.f64 	%fd741, [%rd160+32];
	fma.rn.f64 	%fd742, %fd740, %fd733, %fd741;
	ld.global.nc.f64 	%fd743, [%rd160+40];
	fma.rn.f64 	%fd744, %fd742, %fd733, %fd743;
	ld.global.nc.f64 	%fd745, [%rd160+48];
	fma.rn.f64 	%fd746, %fd744, %fd733, %fd745;
	fma.rn.f64 	%fd747, %fd746, %fd845, %fd845;
	fma.rn.f64 	%fd749, %fd746, %fd733, %fd300;
	selp.f64 	%fd750, %fd747, %fd749, %p340;
	and.b32  	%r500, %r589, 2;
	setp.eq.s32 	%p341, %r500, 0;
	fma.rn.f64 	%fd753, %fd750, %fd304, %fd806;
	selp.f64 	%fd180, %fd750, %fd753, %p341;
	@%p32 bra 	$L__BB0_152;
	bra.uni 	$L__BB0_134;
$L__BB0_152:
	.loc	1 0 26
	mov.b32 	%r590, 0;
	.loc	1 52 26
	mul.rn.f64 	%fd846, %fd97, %fd806;
	bra.uni 	$L__BB0_136;
$L__BB0_134:
	cvt.rni.s32.f64 	%r590, %fd805;
	st.local.u32 	[%r1], %r590;
	cvt.rn.f64.s32 	%fd756, %r590;
	neg.f64 	%fd757, %fd756;
	fma.rn.f64 	%fd759, %fd757, %fd787, %fd97;
	fma.rn.f64 	%fd761, %fd757, %fd788, %fd759;
	fma.rn.f64 	%fd846, %fd757, %fd789, %fd761;
	setp.ltu.f64 	%p342, %fd804, 0d41E0000000000000;
	@%p342 bra 	$L__BB0_136;
	{ // callseq 23, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd97;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd846, [retval0+0];
	} // callseq 23
	ld.local.u32 	%r590, [%r1];
$L__BB0_136:
	and.b32  	%r518, %r590, 1;
	shl.b32 	%r519, %r518, 3;
	mul.wide.u32 	%rd178, %r519, 8;
	add.s64 	%rd180, %rd103, %rd178;
	mul.rn.f64 	%fd765, %fd846, %fd846;
	setp.eq.s32 	%p359, %r518, 0;
	selp.f64 	%fd766, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p359;
	ld.global.nc.f64 	%fd767, [%rd180+8];
	fma.rn.f64 	%fd768, %fd766, %fd765, %fd767;
	ld.global.nc.f64 	%fd769, [%rd180+16];
	fma.rn.f64 	%fd770, %fd768, %fd765, %fd769;
	ld.global.nc.f64 	%fd771, [%rd180+24];
	fma.rn.f64 	%fd772, %fd770, %fd765, %fd771;
	ld.global.nc.f64 	%fd773, [%rd180+32];
	fma.rn.f64 	%fd774, %fd772, %fd765, %fd773;
	ld.global.nc.f64 	%fd775, [%rd180+40];
	fma.rn.f64 	%fd776, %fd774, %fd765, %fd775;
	ld.global.nc.f64 	%fd777, [%rd180+48];
	fma.rn.f64 	%fd778, %fd776, %fd765, %fd777;
	fma.rn.f64 	%fd779, %fd778, %fd846, %fd846;
	fma.rn.f64 	%fd781, %fd778, %fd765, %fd300;
	selp.f64 	%fd782, %fd779, %fd781, %p359;
	and.b32  	%r520, %r590, 2;
	setp.eq.s32 	%p360, %r520, 0;
	fma.rn.f64 	%fd785, %fd782, %fd304, %fd806;
	selp.f64 	%fd786, %fd782, %fd785, %p360;
	.loc	1 51 21
	cvt.rn.f32.f64 	%f25, %fd145;
	cvt.rn.f32.f64 	%f26, %fd139;
	cvt.rn.f32.f64 	%f27, %fd133;
	cvt.rn.f32.f64 	%f28, %fd127;
	cvt.rn.f32.f64 	%f29, %fd121;
	cvt.rn.f32.f64 	%f30, %fd115;
	cvt.rn.f32.f64 	%f31, %fd109;
	cvt.rn.f32.f64 	%f32, %fd103;
	.loc	1 53 21
	cvt.rn.f32.f64 	%f33, %fd150;
	cvt.rn.f32.f64 	%f34, %fd155;
	cvt.rn.f32.f64 	%f35, %fd160;
	cvt.rn.f32.f64 	%f36, %fd165;
	cvt.rn.f32.f64 	%f37, %fd170;
	cvt.rn.f32.f64 	%f38, %fd175;
	cvt.rn.f32.f64 	%f39, %fd180;
	cvt.rn.f32.f64 	%f40, %fd786;
	.loc	1 54 35
	shl.b32 	%r521, %r33, 7;
	shl.b32 	%r522, %r34, 7;
	shl.b32 	%r523, %r35, 7;
	shl.b32 	%r524, %r36, 7;
	shl.b32 	%r525, %r37, 7;
	shl.b32 	%r526, %r38, 7;
	shl.b32 	%r527, %r39, 7;
	shl.b32 	%r528, %r40, 7;
	.loc	1 54 31
	add.s32 	%r529, %r521, %r25;
	add.s32 	%r530, %r522, %r26;
	add.s32 	%r531, %r523, %r27;
	add.s32 	%r532, %r524, %r28;
	add.s32 	%r533, %r525, %r29;
	add.s32 	%r534, %r526, %r30;
	add.s32 	%r535, %r527, %r31;
	add.s32 	%r536, %r528, %r32;
	.loc	1 54 25
	mul.wide.s32 	%rd181, %r529, 4;
	add.s64 	%rd162, %rd9, %rd181;
	mul.wide.s32 	%rd182, %r530, 4;
	add.s64 	%rd163, %rd9, %rd182;
	mul.wide.s32 	%rd183, %r531, 4;
	add.s64 	%rd164, %rd9, %rd183;
	mul.wide.s32 	%rd184, %r532, 4;
	add.s64 	%rd165, %rd9, %rd184;
	mul.wide.s32 	%rd185, %r533, 4;
	add.s64 	%rd166, %rd9, %rd185;
	mul.wide.s32 	%rd186, %r534, 4;
	add.s64 	%rd167, %rd9, %rd186;
	mul.wide.s32 	%rd187, %r535, 4;
	add.s64 	%rd168, %rd9, %rd187;
	mul.wide.s32 	%rd188, %r536, 4;
	add.s64 	%rd169, %rd9, %rd188;
	.loc	1 54 48
	mov.b32 	%r502, %f32;
	// begin inline asm
	@%p343 st.global.b32 [ %rd162 + 0 ], { %r502 };
	// end inline asm
	mov.b32 	%r503, %f31;
	// begin inline asm
	@%p344 st.global.b32 [ %rd163 + 0 ], { %r503 };
	// end inline asm
	mov.b32 	%r504, %f30;
	// begin inline asm
	@%p345 st.global.b32 [ %rd164 + 0 ], { %r504 };
	// end inline asm
	mov.b32 	%r505, %f29;
	// begin inline asm
	@%p346 st.global.b32 [ %rd165 + 0 ], { %r505 };
	// end inline asm
	mov.b32 	%r506, %f28;
	// begin inline asm
	@%p347 st.global.b32 [ %rd166 + 0 ], { %r506 };
	// end inline asm
	mov.b32 	%r507, %f27;
	// begin inline asm
	@%p348 st.global.b32 [ %rd167 + 0 ], { %r507 };
	// end inline asm
	mov.b32 	%r508, %f26;
	// begin inline asm
	@%p349 st.global.b32 [ %rd168 + 0 ], { %r508 };
	// end inline asm
	mov.b32 	%r509, %f25;
	// begin inline asm
	@%p350 st.global.b32 [ %rd169 + 0 ], { %r509 };
	// end inline asm
	.loc	1 55 25
	add.s64 	%rd170, %rd10, %rd181;
	add.s64 	%rd171, %rd10, %rd182;
	add.s64 	%rd172, %rd10, %rd183;
	add.s64 	%rd173, %rd10, %rd184;
	add.s64 	%rd174, %rd10, %rd185;
	add.s64 	%rd175, %rd10, %rd186;
	add.s64 	%rd176, %rd10, %rd187;
	add.s64 	%rd177, %rd10, %rd188;
	.loc	1 55 48
	mov.b32 	%r510, %f33;
	// begin inline asm
	@%p343 st.global.b32 [ %rd170 + 0 ], { %r510 };
	// end inline asm
	mov.b32 	%r511, %f34;
	// begin inline asm
	@%p344 st.global.b32 [ %rd171 + 0 ], { %r511 };
	// end inline asm
	mov.b32 	%r512, %f35;
	// begin inline asm
	@%p345 st.global.b32 [ %rd172 + 0 ], { %r512 };
	// end inline asm
	mov.b32 	%r513, %f36;
	// begin inline asm
	@%p346 st.global.b32 [ %rd173 + 0 ], { %r513 };
	// end inline asm
	mov.b32 	%r514, %f37;
	// begin inline asm
	@%p347 st.global.b32 [ %rd174 + 0 ], { %r514 };
	// end inline asm
	mov.b32 	%r515, %f38;
	// begin inline asm
	@%p348 st.global.b32 [ %rd175 + 0 ], { %r515 };
	// end inline asm
	mov.b32 	%r516, %f39;
	// begin inline asm
	@%p349 st.global.b32 [ %rd176 + 0 ], { %r516 };
	// end inline asm
	mov.b32 	%r517, %f40;
	// begin inline asm
	@%p350 st.global.b32 [ %rd177 + 0 ], { %r517 };
	// end inline asm
	.loc	1 55 4
	ret;
$L__tmp1:
$L__func_end0:

}
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot1[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b32 	%r<48>;
	.reg .b64 	%rd<86>;
	.reg .f64 	%fd<5>;
$L__func_begin1:

	mov.u64 	%SPL, __local_depot1;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd4;
	}
	bfe.u32 	%r4, %r22, 20, 11;
	setp.eq.s32 	%p1, %r4, 2047;
	@%p1 bra 	$L__BB1_13;
	add.u64 	%rd27, %SP, 0;
	{ .reg .b64 %tmp;
	  cvta.to.local.u64 	%tmp, %rd27;
	  cvt.u32.u64 	%r1, %tmp; }
	shr.u32 	%r3, %r22, 20;
	add.s32 	%r23, %r4, -1024;
	shr.u32 	%r5, %r23, 6;
	mov.b32 	%r24, 15;
	sub.s32 	%r43, %r24, %r5;
	mov.b32 	%r25, 19;
	sub.s32 	%r26, %r25, %r5;
	setp.lt.u32 	%p2, %r23, 128;
	selp.b32 	%r45, 18, %r26, %p2;
	setp.ge.s32 	%p3, %r43, %r45;
	@%p3 bra 	$L__BB1_14;
	mov.b64 	%rd28, %fd4;
	shl.b64 	%rd29, %rd28, 11;
	or.b64  	%rd38, %rd29, -9223372036854775808;
	neg.s32 	%r44, %r5;
	mul.wide.s32 	%rd32, %r44, 8;
	mov.u64 	%rd33, __cudart_i2opi_d;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd77, %rd34, 120;
	mov.u64 	%rd79, 0;
	mov.u32 	%r42, %r1;
$L__BB1_3:
	.pragma "nounroll";
	ld.global.nc.u64 	%rd37, [%rd77];
	// begin inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd37;    
	mov.b64         {blo,bhi}, %rd38;    
	mov.b64         {clo,chi}, %rd79;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd35, {r0,r1};      
	mov.b64         %rd79, {r2,r3};      
	}
	// end inline asm
	st.local.u64 	[%r42], %rd35;
	add.s32 	%r43, %r43, 1;
	add.s32 	%r42, %r42, 8;
	add.s64 	%rd77, %rd77, 8;
	setp.lt.s32 	%p4, %r43, %r45;
	@%p4 bra 	$L__BB1_3;
	bra.uni 	$L__BB1_4;
$L__BB1_14:
	neg.s32 	%r44, %r5;
	mov.u64 	%rd79, 0;
	mov.u32 	%r45, %r43;
$L__BB1_4:
	ld.param.u64 	%rd26, [__internal_trig_reduction_slowpathd_param_1];
	and.b32  	%r46, %r22, -2147483648;
	sub.s32 	%r27, %r45, %r44;
	shl.b32 	%r28, %r27, 3;
	add.s32 	%r29, %r1, %r28;
	st.local.u64 	[%r29+-120], %rd79;
	and.b32  	%r16, %r3, 63;
	ld.local.u64 	%rd81, [%r1+16];
	ld.local.u64 	%rd80, [%r1+24];
	setp.eq.s32 	%p5, %r16, 0;
	mov.b32 	%r41, 64;
	@%p5 bra 	$L__BB1_6;
	sub.s32 	%r31, %r41, %r16;
	shl.b64 	%rd40, %rd80, %r16;
	shr.u64 	%rd41, %rd81, %r31;
	or.b64  	%rd80, %rd40, %rd41;
	shl.b64 	%rd42, %rd81, %r16;
	ld.local.u64 	%rd43, [%r1+8];
	shr.u64 	%rd44, %rd43, %r31;
	or.b64  	%rd81, %rd44, %rd42;
$L__BB1_6:
	shr.u64 	%rd45, %rd80, 62;
	cvt.u32.u64 	%r32, %rd45;
	shr.u64 	%rd46, %rd81, 62;
	shl.b64 	%rd47, %rd80, 2;
	or.b64  	%rd84, %rd47, %rd46;
	shl.b64 	%rd83, %rd81, 2;
	bfe.u64 	%rd48, %rd80, 61, 1;
	cvt.u32.u64 	%r33, %rd48;
	add.s32 	%r34, %r33, %r32;
	setp.eq.s32 	%p6, %r46, 0;
	neg.s32 	%r35, %r34;
	selp.b32 	%r36, %r34, %r35, %p6;
	st.u32 	[%rd26], %r36;
	setp.gt.s64 	%p7, %rd84, -1;
	@%p7 bra 	$L__BB1_8;
	xor.b32  	%r46, %r46, -2147483648;
	mov.u64 	%rd51, 0;
	// begin inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd51;
	mov.b64         {a2,a3}, %rd51;
	mov.b64         {b0,b1}, %rd83;
	mov.b64         {b2,b3}, %rd84;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd83, {r0,r1};
	mov.b64         %rd84, {r2,r3};
	}
	// end inline asm
$L__BB1_8:
	clz.b64 	%r47, %rd84;
	setp.eq.s32 	%p8, %r47, 0;
	@%p8 bra 	$L__BB1_10;
	shl.b64 	%rd55, %rd84, %r47;
	sub.s32 	%r38, %r41, %r47;
	shr.u64 	%rd56, %rd83, %r38;
	or.b64  	%rd84, %rd56, %rd55;
$L__BB1_10:
	mov.u64 	%rd60, -3958705157555305931;
	// begin inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd84;   
	mov.b64         {blo,bhi}, %rd60;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd63, {r0,r1};     
	mov.b64         %rd85, {r2,r3};     
	}
	// end inline asm
	setp.lt.s64 	%p9, %rd85, 1;
	@%p9 bra 	$L__BB1_12;
	add.s32 	%r47, %r47, 1;
	// begin inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd63;
	mov.b64         {a2,a3}, %rd85;
	mov.b64         {b0,b1}, %rd63;
	mov.b64         {b2,b3}, %rd85;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd61, {r0,r1};
	mov.b64         %rd85, {r2,r3};
	}
	// end inline asm
$L__BB1_12:
	mov.b32 	%r39, 1022;
	sub.s32 	%r40, %r39, %r47;
	cvt.u64.u32 	%rd67, %r40;
	cvt.u64.u32 	%rd68, %r46;
	shl.b64 	%rd69, %rd68, 32;
	shl.b64 	%rd70, %rd67, 52;
	add.s64 	%rd71, %rd85, 1;
	shr.u64 	%rd72, %rd71, 10;
	add.s64 	%rd73, %rd72, 1;
	shr.u64 	%rd74, %rd73, 1;
	add.s64 	%rd75, %rd70, %rd74;
	or.b64  	%rd76, %rd75, %rd69;
	mov.b64 	%fd4, %rd76;
$L__BB1_13:
	st.param.f64 	[func_retval0+0], %fd4;
	ret;
$L__func_end1:

}
.func  (.param .b64 func_retval0) __internal_accurate_pow(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<52>;
	.reg .f32 	%f<3>;
	.reg .f64 	%fd<134>;
$L__func_begin2:

	ld.param.f64 	%fd12, [__internal_accurate_pow_param_0];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r49}, %fd12;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r48, %temp}, %fd12;
	}
	shr.u32 	%r50, %r49, 20;
	setp.gt.u32 	%p1, %r49, 1048575;
	@%p1 bra 	$L__BB2_2;
	mul.f64 	%fd13, %fd12, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r49}, %fd13;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r48, %temp}, %fd13;
	}
	shr.u32 	%r16, %r49, 20;
	add.s32 	%r50, %r16, -54;
$L__BB2_2:
	ld.param.f64 	%fd11, [__internal_accurate_pow_param_1];
	add.s32 	%r51, %r50, -1023;
	and.b32  	%r17, %r49, -2146435073;
	or.b32  	%r18, %r17, 1072693248;
	mov.b64 	%fd132, {%r48, %r18};
	setp.lt.u32 	%p2, %r18, 1073127583;
	@%p2 bra 	$L__BB2_4;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd132;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd132;
	}
	add.s32 	%r21, %r20, -1048576;
	mov.b64 	%fd132, {%r19, %r21};
	add.s32 	%r51, %r50, -1022;
$L__BB2_4:
	add.f64 	%fd14, %fd132, 0dBFF0000000000000;
	add.f64 	%fd15, %fd132, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd16, %fd15;
	neg.f64 	%fd17, %fd15;
	mov.f64 	%fd18, 0d3FF0000000000000;
	fma.rn.f64 	%fd19, %fd17, %fd16, %fd18;
	fma.rn.f64 	%fd20, %fd19, %fd19, %fd19;
	fma.rn.f64 	%fd21, %fd20, %fd16, %fd16;
	mul.f64 	%fd22, %fd14, %fd21;
	fma.rn.f64 	%fd23, %fd14, %fd21, %fd22;
	mul.f64 	%fd24, %fd23, %fd23;
	mov.f64 	%fd25, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd26, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd27, %fd26, %fd24, %fd25;
	mov.f64 	%fd28, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd29, %fd27, %fd24, %fd28;
	mov.f64 	%fd30, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd31, %fd29, %fd24, %fd30;
	mov.f64 	%fd32, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd33, %fd31, %fd24, %fd32;
	mov.f64 	%fd34, 0d3F6249249242B910;
	fma.rn.f64 	%fd35, %fd33, %fd24, %fd34;
	mov.f64 	%fd36, 0d3F89999999999DFB;
	fma.rn.f64 	%fd37, %fd35, %fd24, %fd36;
	sub.f64 	%fd38, %fd14, %fd23;
	add.f64 	%fd39, %fd38, %fd38;
	neg.f64 	%fd40, %fd23;
	fma.rn.f64 	%fd41, %fd40, %fd14, %fd39;
	mul.f64 	%fd42, %fd21, %fd41;
	fma.rn.f64 	%fd43, %fd24, %fd37, 0d3FB5555555555555;
	mov.f64 	%fd44, 0d3FB5555555555555;
	sub.f64 	%fd45, %fd44, %fd43;
	fma.rn.f64 	%fd46, %fd24, %fd37, %fd45;
	add.f64 	%fd47, %fd46, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd48, %fd43, %fd47;
	sub.f64 	%fd49, %fd43, %fd48;
	add.f64 	%fd50, %fd47, %fd49;
	mul.rn.f64 	%fd51, %fd23, %fd23;
	neg.f64 	%fd52, %fd51;
	fma.rn.f64 	%fd53, %fd23, %fd23, %fd52;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd42;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd42;
	}
	add.s32 	%r24, %r23, 1048576;
	mov.b64 	%fd54, {%r22, %r24};
	fma.rn.f64 	%fd55, %fd23, %fd54, %fd53;
	mul.rn.f64 	%fd56, %fd51, %fd23;
	neg.f64 	%fd57, %fd56;
	fma.rn.f64 	%fd58, %fd51, %fd23, %fd57;
	fma.rn.f64 	%fd59, %fd51, %fd42, %fd58;
	fma.rn.f64 	%fd60, %fd55, %fd23, %fd59;
	mul.rn.f64 	%fd61, %fd48, %fd56;
	neg.f64 	%fd62, %fd61;
	fma.rn.f64 	%fd63, %fd48, %fd56, %fd62;
	fma.rn.f64 	%fd64, %fd48, %fd60, %fd63;
	fma.rn.f64 	%fd65, %fd50, %fd56, %fd64;
	add.f64 	%fd66, %fd61, %fd65;
	sub.f64 	%fd67, %fd61, %fd66;
	add.f64 	%fd68, %fd65, %fd67;
	add.f64 	%fd69, %fd23, %fd66;
	sub.f64 	%fd70, %fd23, %fd69;
	add.f64 	%fd71, %fd66, %fd70;
	add.f64 	%fd72, %fd68, %fd71;
	fma.rn.f64 	%fd73, %fd21, %fd41, %fd72;
	add.f64 	%fd74, %fd69, %fd73;
	sub.f64 	%fd75, %fd69, %fd74;
	add.f64 	%fd76, %fd73, %fd75;
	xor.b32  	%r25, %r51, -2147483648;
	mov.b32 	%r26, 1127219200;
	mov.b64 	%fd77, {%r25, %r26};
	mov.b32 	%r27, -2147483648;
	mov.b64 	%fd78, {%r27, %r26};
	sub.f64 	%fd79, %fd77, %fd78;
	mov.f64 	%fd80, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd81, %fd79, %fd80, %fd74;
	neg.f64 	%fd82, %fd79;
	fma.rn.f64 	%fd83, %fd82, %fd80, %fd81;
	sub.f64 	%fd84, %fd83, %fd74;
	sub.f64 	%fd85, %fd76, %fd84;
	mov.f64 	%fd86, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd87, %fd79, %fd86, %fd85;
	add.f64 	%fd88, %fd81, %fd87;
	sub.f64 	%fd89, %fd81, %fd88;
	add.f64 	%fd90, %fd87, %fd89;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd11;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r29, %temp}, %fd11;
	}
	shl.b32 	%r30, %r28, 1;
	setp.gt.u32 	%p3, %r30, -33554433;
	and.b32  	%r31, %r28, -15728641;
	selp.b32 	%r32, %r31, %r28, %p3;
	mov.b64 	%fd91, {%r29, %r32};
	mul.rn.f64 	%fd4, %fd88, %fd91;
	neg.f64 	%fd92, %fd4;
	fma.rn.f64 	%fd93, %fd88, %fd91, %fd92;
	fma.rn.f64 	%fd5, %fd90, %fd91, %fd93;
	add.f64 	%fd6, %fd4, %fd5;
	mov.f64 	%fd94, 0d4338000000000000;
	mov.f64 	%fd95, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd96, %fd6, %fd95, %fd94;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd96;
	}
	mov.f64 	%fd97, 0dC338000000000000;
	add.rn.f64 	%fd98, %fd96, %fd97;
	mov.f64 	%fd99, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd100, %fd98, %fd99, %fd6;
	mov.f64 	%fd101, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd102, %fd98, %fd101, %fd100;
	mov.f64 	%fd103, 0d3E928AF3FCA213EA;
	mov.f64 	%fd104, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd105, %fd104, %fd102, %fd103;
	mov.f64 	%fd106, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd107, %fd105, %fd102, %fd106;
	mov.f64 	%fd108, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd109, %fd107, %fd102, %fd108;
	mov.f64 	%fd110, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd111, %fd109, %fd102, %fd110;
	mov.f64 	%fd112, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd113, %fd111, %fd102, %fd112;
	mov.f64 	%fd114, 0d3F81111111122322;
	fma.rn.f64 	%fd115, %fd113, %fd102, %fd114;
	mov.f64 	%fd116, 0d3FA55555555502A1;
	fma.rn.f64 	%fd117, %fd115, %fd102, %fd116;
	mov.f64 	%fd118, 0d3FC5555555555511;
	fma.rn.f64 	%fd119, %fd117, %fd102, %fd118;
	mov.f64 	%fd120, 0d3FE000000000000B;
	fma.rn.f64 	%fd121, %fd119, %fd102, %fd120;
	fma.rn.f64 	%fd122, %fd121, %fd102, %fd18;
	fma.rn.f64 	%fd123, %fd122, %fd102, %fd18;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd123;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd123;
	}
	shl.b32 	%r33, %r13, 20;
	add.s32 	%r34, %r15, %r33;
	mov.b64 	%fd133, {%r14, %r34};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd6;
	}
	mov.b32 	%f2, %r35;
	abs.ftz.f32 	%f1, %f2;
	setp.lt.f32 	%p4, %f1, 0f4086232B;
	@%p4 bra 	$L__BB2_7;
	setp.lt.f64 	%p5, %fd6, 0d0000000000000000;
	add.f64 	%fd124, %fd6, 0d7FF0000000000000;
	selp.f64 	%fd133, 0d0000000000000000, %fd124, %p5;
	setp.geu.f32 	%p6, %f1, 0f40874800;
	@%p6 bra 	$L__BB2_7;
	shr.u32 	%r36, %r13, 31;
	add.s32 	%r37, %r13, %r36;
	shr.s32 	%r38, %r37, 1;
	shl.b32 	%r39, %r38, 20;
	add.s32 	%r40, %r15, %r39;
	mov.b64 	%fd125, {%r14, %r40};
	sub.s32 	%r41, %r13, %r38;
	shl.b32 	%r42, %r41, 20;
	add.s32 	%r43, %r42, 1072693248;
	mov.b32 	%r44, 0;
	mov.b64 	%fd126, {%r44, %r43};
	mul.f64 	%fd133, %fd126, %fd125;
$L__BB2_7:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r45, %temp}, %fd133;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd133;
	}
	and.b32  	%r47, %r46, 2147483647;
	setp.eq.s32 	%p7, %r47, 2146435072;
	setp.eq.s32 	%p8, %r45, 0;
	sub.f64 	%fd127, %fd4, %fd6;
	add.f64 	%fd128, %fd5, %fd127;
	fma.rn.f64 	%fd129, %fd133, %fd128, %fd133;
	selp.f64 	%fd130, %fd133, %fd129, %p8;
	selp.f64 	%fd131, %fd130, %fd129, %p7;
	st.param.f64 	[func_retval0+0], %fd131;
	ret;
$L__func_end2:

}
	.file	1 "/opt/inductor_cache/rt/crtwdaopdwkm5bssimj4iwdw3wbqisq442r4env5easwkdgblfkf.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 0
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 116
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 99
.b8 114
.b8 116
.b8 119
.b8 100
.b8 97
.b8 111
.b8 112
.b8 100
.b8 119
.b8 107
.b8 109
.b8 53
.b8 98
.b8 115
.b8 115
.b8 105
.b8 109
.b8 106
.b8 52
.b8 105
.b8 119
.b8 100
.b8 119
.b8 51
.b8 119
.b8 98
.b8 113
.b8 105
.b8 115
.b8 113
.b8 52
.b8 52
.b8 50
.b8 114
.b8 52
.b8 101
.b8 110
.b8 118
.b8 53
.b8 101
.b8 97
.b8 115
.b8 119
.b8 107
.b8 100
.b8 103
.b8 98
.b8 108
.b8 102
.b8 107
.b8 102
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 47
.b8 111
.b8 112
.b8 116
.b8 47
.b8 105
.b8 110
.b8 100
.b8 117
.b8 99
.b8 116
.b8 111
.b8 114
.b8 95
.b8 99
.b8 97
.b8 99
.b8 104
.b8 101
.b8 47
.b8 114
.b8 116
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
	}
	.section	.debug_loc	{	}
