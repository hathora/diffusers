//
// Generated by LLVM NVPTX Back-End
//

.version 8.4
.target sm_90a
.address_size 64

	// .globl	triton_
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.func  (.param .b64 func_retval0) __internal_accurate_pow
(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
;
.global .align 1 .b8 _$_str[11] = {95, 95, 67, 85, 68, 65, 95, 70, 84, 90};
.global .align 1 .b8 _$_str_$_1[12] = {95, 95, 67, 85, 68, 65, 95, 65, 82, 67, 72};
.global .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.global .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .entry triton_(
	.param .u64 triton__param_0,
	.param .u64 triton__param_1,
	.param .u64 triton__param_2,
	.param .u64 triton__param_3,
	.param .u32 triton__param_4,
	.param .u32 triton__param_5,
	.param .u32 triton__param_6,
	.param .u32 triton__param_7
)
.maxntid 128, 1, 1
{
	.local .align 4 .b8 	__local_depot0[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<365>;
	.reg .b16 	%rs<81>;
	.reg .b32 	%r<570>;
	.reg .f32 	%f<41>;
	.reg .b64 	%rd<171>;
	.reg .f64 	%fd<847>;
	.loc	1 18 0
$L__func_begin0:
	.loc	1 18 0

	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r117, [triton__param_7];
	ld.param.u32 	%r116, [triton__param_6];
	ld.param.u64 	%rd27, [triton__param_0];
	ld.param.u64 	%rd28, [triton__param_1];
	ld.param.u32 	%r135, [triton__param_4];
$L__tmp0:
	.loc	1 19 28
	// begin inline asm
	mov.u32 %r118, %ctaid.x;
	// end inline asm
	.loc	1 19 33
	shl.b32 	%r136, %r118, 10;
	.loc	1 20 36
	mov.u32 	%r137, %tid.x;
	shl.b32 	%r138, %r137, 2;
	and.b32  	%r139, %r138, 508;
	.loc	1 20 23
	or.b32  	%r17, %r136, %r139;
	or.b32  	%r18, %r17, 1;
	or.b32  	%r19, %r17, 2;
	or.b32  	%r20, %r17, 3;
	or.b32  	%r21, %r17, 512;
	or.b32  	%r22, %r17, 513;
	or.b32  	%r23, %r17, 514;
	or.b32  	%r24, %r17, 515;
	.loc	1 21 21
	setp.lt.s32 	%p331, %r17, %r117;
	setp.lt.s32 	%p332, %r18, %r117;
	setp.lt.s32 	%p333, %r19, %r117;
	setp.lt.s32 	%p334, %r20, %r117;
	setp.lt.s32 	%p335, %r21, %r117;
	setp.lt.s32 	%p336, %r22, %r117;
	setp.lt.s32 	%p337, %r23, %r117;
	setp.lt.s32 	%p338, %r24, %r117;
	.loc	1 22 20
	mul.hi.s32 	%r141, %r17, -1840700269;
	mad.lo.s32 	%r142, %r17, 1, %r141;
	shr.u32 	%r143, %r142, 31;
	shr.s32 	%r144, %r142, 5;
	add.s32 	%r25, %r144, %r143;
	mul.hi.s32 	%r146, %r21, -1840700269;
	mad.lo.s32 	%r147, %r21, 1, %r146;
	shr.u32 	%r148, %r147, 31;
	shr.s32 	%r149, %r147, 5;
	add.s32 	%r26, %r149, %r148;
	mul.lo.s32 	%r150, %r25, 56;
	sub.s32 	%r27, %r17, %r150;
	.loc	1 23 18
	mul.hi.s32 	%r151, %r18, -1840700269;
	mad.lo.s32 	%r152, %r18, 1, %r151;
	shr.u32 	%r153, %r152, 31;
	shr.s32 	%r154, %r152, 5;
	add.s32 	%r155, %r154, %r153;
	mul.lo.s32 	%r156, %r155, 56;
	sub.s32 	%r28, %r18, %r156;
	mul.hi.s32 	%r157, %r19, -1840700269;
	mad.lo.s32 	%r158, %r19, 1, %r157;
	shr.u32 	%r159, %r158, 31;
	shr.s32 	%r160, %r158, 5;
	add.s32 	%r161, %r160, %r159;
	mul.lo.s32 	%r162, %r161, 56;
	sub.s32 	%r29, %r19, %r162;
	mul.hi.s32 	%r163, %r20, -1840700269;
	mad.lo.s32 	%r164, %r20, 1, %r163;
	shr.u32 	%r165, %r164, 31;
	shr.s32 	%r166, %r164, 5;
	add.s32 	%r167, %r166, %r165;
	mul.lo.s32 	%r168, %r167, 56;
	sub.s32 	%r30, %r20, %r168;
	mul.hi.s32 	%r170, %r22, -1840700269;
	mad.lo.s32 	%r171, %r22, 1, %r170;
	shr.u32 	%r172, %r171, 31;
	shr.s32 	%r173, %r171, 5;
	add.s32 	%r174, %r173, %r172;
	mul.lo.s32 	%r175, %r174, 56;
	sub.s32 	%r32, %r22, %r175;
	mul.hi.s32 	%r182, %r24, -1840700269;
	mad.lo.s32 	%r183, %r24, 1, %r182;
	.loc	1 28 18
	setp.lt.s32 	%p73, %r25, %r135;
	setp.lt.s32 	%p74, %r26, %r135;
	.loc	1 29 37
	mul.lo.s32 	%r188, %r25, 3;
	mul.lo.s32 	%r189, %r26, 3;
	.loc	1 29 30
	mul.wide.s32 	%rd45, %r188, 2;
	add.s64 	%rd46, %rd27, %rd45;
	add.s64 	%rd11, %rd46, 2;
	mul.wide.s32 	%rd47, %r189, 2;
	add.s64 	%rd48, %rd27, %rd47;
	add.s64 	%rd15, %rd48, 2;
	.loc	1 29 50
	and.pred  	%p33, %p331, %p73;
	and.pred  	%p35, %p332, %p73;
	and.pred  	%p37, %p333, %p73;
	and.pred  	%p39, %p334, %p73;
	and.pred  	%p41, %p335, %p74;
	and.pred  	%p43, %p336, %p74;
	and.pred  	%p45, %p337, %p74;
	and.pred  	%p47, %p338, %p74;
	mov.u16 	%rs2, 0;
	.loc	1 29 43
	// begin inline asm
	mov.u16 %rs1, 0x0;
	@%p33 ld.global.L1::evict_last.b16 { %rs1 }, [ %rd11 + 0 ];
	@!%p33 mov.u16 %rs1, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs3, 0x0;
	@%p35 ld.global.L1::evict_last.b16 { %rs3 }, [ %rd11 + 0 ];
	@!%p35 mov.u16 %rs3, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs5, 0x0;
	@%p37 ld.global.L1::evict_last.b16 { %rs5 }, [ %rd11 + 0 ];
	@!%p37 mov.u16 %rs5, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs7, 0x0;
	@%p39 ld.global.L1::evict_last.b16 { %rs7 }, [ %rd11 + 0 ];
	@!%p39 mov.u16 %rs7, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs9, 0x0;
	@%p41 ld.global.L1::evict_last.b16 { %rs9 }, [ %rd15 + 0 ];
	@!%p41 mov.u16 %rs9, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs11, 0x0;
	@%p43 ld.global.L1::evict_last.b16 { %rs11 }, [ %rd15 + 0 ];
	@!%p43 mov.u16 %rs11, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs13, 0x0;
	@%p45 ld.global.L1::evict_last.b16 { %rs13 }, [ %rd15 + 0 ];
	@!%p45 mov.u16 %rs13, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs15, 0x0;
	@%p47 ld.global.L1::evict_last.b16 { %rs15 }, [ %rd15 + 0 ];
	@!%p47 mov.u16 %rs15, %rs2;
	// end inline asm
	.loc	1 30 19
	setp.ge.s32 	%p75, %r25, %r135;
	setp.ge.s32 	%p76, %r26, %r135;
	.loc	1 33 44
	sub.s32 	%r190, %r25, %r135;
	sub.s32 	%r191, %r26, %r135;
	.loc	1 33 35
	mad.lo.s32 	%r192, %r190, 3, 1;
	mad.lo.s32 	%r193, %r191, 3, 1;
	.loc	1 33 30
	mul.wide.s32 	%rd49, %r192, 2;
	add.s64 	%rd19, %rd28, %rd49;
	mul.wide.s32 	%rd50, %r193, 2;
	add.s64 	%rd23, %rd28, %rd50;
	.loc	1 33 65
	and.pred  	%p49, %p331, %p75;
	and.pred  	%p51, %p332, %p75;
	and.pred  	%p53, %p333, %p75;
	and.pred  	%p55, %p334, %p75;
	and.pred  	%p57, %p335, %p76;
	and.pred  	%p59, %p336, %p76;
	and.pred  	%p61, %p337, %p76;
	and.pred  	%p63, %p338, %p76;
	.loc	1 33 58
	// begin inline asm
	mov.u16 %rs25, 0x0;
	@%p49 ld.global.L1::evict_last.b16 { %rs25 }, [ %rd19 + 0 ];
	@!%p49 mov.u16 %rs25, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs27, 0x0;
	@%p51 ld.global.L1::evict_last.b16 { %rs27 }, [ %rd19 + 0 ];
	@!%p51 mov.u16 %rs27, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs29, 0x0;
	@%p53 ld.global.L1::evict_last.b16 { %rs29 }, [ %rd19 + 0 ];
	@!%p53 mov.u16 %rs29, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs31, 0x0;
	@%p55 ld.global.L1::evict_last.b16 { %rs31 }, [ %rd19 + 0 ];
	@!%p55 mov.u16 %rs31, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs33, 0x0;
	@%p57 ld.global.L1::evict_last.b16 { %rs33 }, [ %rd23 + 0 ];
	@!%p57 mov.u16 %rs33, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs35, 0x0;
	@%p59 ld.global.L1::evict_last.b16 { %rs35 }, [ %rd23 + 0 ];
	@!%p59 mov.u16 %rs35, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs37, 0x0;
	@%p61 ld.global.L1::evict_last.b16 { %rs37 }, [ %rd23 + 0 ];
	@!%p61 mov.u16 %rs37, %rs2;
	// end inline asm
	// begin inline asm
	mov.u16 %rs39, 0x0;
	@%p63 ld.global.L1::evict_last.b16 { %rs39 }, [ %rd23 + 0 ];
	@!%p63 mov.u16 %rs39, %rs2;
	// end inline asm
	.loc	1 37 37
	cvt.rn.f64.s32 	%fd9, %r116;
	.loc	1 39 21
	cvt.s8.s32 	%rs49, %r27;
	shr.s16 	%rs50, %rs49, 1;
	cvt.u16.u32 	%rs51, %r28;
	and.b16  	%rs52, %rs51, 128;
	shr.u16 	%rs53, %rs52, 7;
	add.s16 	%rs54, %rs51, %rs53;
	cvt.s16.s8 	%rs55, %rs54;
	shr.s16 	%rs56, %rs55, 1;
	cvt.s8.s32 	%rs57, %r29;
	shr.s16 	%rs58, %rs57, 1;
	cvt.u16.u32 	%rs59, %r30;
	and.b16  	%rs60, %rs59, 128;
	shr.u16 	%rs61, %rs60, 7;
	add.s16 	%rs62, %rs59, %rs61;
	.loc	1 39 15
	mul.wide.s16 	%r194, %rs50, 2;
	mul.wide.s16 	%r195, %rs56, 2;
	.loc	1 40 21
	cvt.rn.f64.s32 	%fd185, %r194;
	cvt.rn.f64.s32 	%fd186, %r195;
	.loc	1 42 20
	mul.f64 	%fd10, %fd185, 0d3F92492492492492;
	.loc	1 43 33
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd9;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r36}, %fd10;
	}
	bfe.u32 	%r202, %r36, 20, 11;
	add.s32 	%r203, %r202, -1012;
	mov.b64 	%rd51, %fd10;
	shl.b64 	%rd1, %rd51, %r203;
	abs.f64 	%fd18, %fd9;
	{ // callseq 0, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd18;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd10;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd193, [retval0+0];
	} // callseq 0
	setp.ne.s32 	%p77, %r116, 0;
	setp.lt.s32 	%p349, %r35, 0;
	@%p77 bra 	$L__BB0_2;
	bra.uni 	$L__BB0_1;
$L__BB0_2:
	setp.eq.s64 	%p357, %rd1, -9223372036854775808;
	setp.eq.s64 	%p79, %rd1, -9223372036854775808;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r204, %temp}, %fd193;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r205}, %fd193;
	}
	xor.b32  	%r206, %r205, -2147483648;
	mov.b64 	%fd194, {%r204, %r206};
	selp.f64 	%fd195, %fd194, %fd193, %p79;
	selp.f64 	%fd196, %fd195, %fd193, %p349;
	cvt.rzi.f64.f64 	%fd197, %fd10;
	setp.neu.f64 	%p80, %fd197, %fd10;
	selp.f64 	%fd198, 0dFFF8000000000000, %fd196, %p80;
	selp.f64 	%fd807, %fd198, %fd196, %p349;
	bra.uni 	$L__BB0_3;
$L__BB0_1:
	setp.eq.s64 	%p81, %rd1, -9223372036854775808;
	abs.f64 	%fd199, %fd10;
	setp.neu.f64 	%p82, %fd199, 0d3FE0000000000000;
	and.pred  	%p357, %p82, %p81;
	selp.b32 	%r207, %r35, 0, %p357;
	setp.lt.s32 	%p83, %r36, 0;
	or.b32  	%r208, %r207, 2146435072;
	selp.b32 	%r209, %r208, %r207, %p83;
	mov.b32 	%r210, 0;
	mov.b64 	%fd807, {%r210, %r209};
$L__BB0_3:
	.loc	1 0 33
	shr.u32 	%r184, %r183, 31;
	shr.s32 	%r185, %r183, 5;
	mul.hi.s32 	%r176, %r23, -1840700269;
	cvt.u16.u32 	%rs67, %r32;
	mul.lo.s32 	%r169, %r26, 56;
	cvt.s16.s8 	%rs63, %rs62;
	mul.wide.s16 	%r196, %rs58, 2;
	mul.f64 	%fd11, %fd186, 0d3F92492492492492;
	.loc	1 43 33
	add.f64 	%fd23, %fd10, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r211}, %fd23;
	}
	and.b32  	%r212, %r211, 2146435072;
	setp.ne.s32 	%p84, %r212, 2146435072;
	setp.nan.f64 	%p356, %fd18, %fd18;
	mov.f64 	%fd808, %fd807;
	@%p84 bra 	$L__BB0_10;
	.loc	1 0 33
	mov.f64 	%fd808, %fd23;
	.loc	1 43 33
	@%p356 bra 	$L__BB0_10;
	abs.f64 	%fd200, %fd10;
	setp.nan.f64 	%p86, %fd200, %fd200;
	mov.f64 	%fd808, %fd23;
	@%p86 bra 	$L__BB0_10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r213, %temp}, %fd10;
	}
	and.b32  	%r37, %r36, 2147483647;
	setp.eq.s32 	%p87, %r37, 2146435072;
	setp.eq.s32 	%p88, %r213, 0;
	and.pred  	%p89, %p87, %p88;
	@!%p89 bra 	$L__BB0_8;
	bra.uni 	$L__BB0_7;
$L__BB0_7:
	setp.gt.f64 	%p96, %fd18, 0d3FF0000000000000;
	selp.b32 	%r222, 2146435072, 0, %p96;
	setp.lt.s32 	%p97, %r36, 0;
	xor.b32  	%r223, %r222, 2146435072;
	selp.b32 	%r224, %r223, %r222, %p97;
	setp.eq.s32 	%p98, %r116, -1;
	selp.b32 	%r225, 1072693248, %r224, %p98;
	mov.b32 	%r226, 0;
	mov.b64 	%fd808, {%r226, %r225};
	bra.uni 	$L__BB0_10;
$L__BB0_8:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r214, %temp}, %fd9;
	}
	and.b32  	%r215, %r35, 2147483647;
	setp.eq.s32 	%p90, %r215, 2146435072;
	setp.eq.s32 	%p91, %r214, 0;
	and.pred  	%p92, %p90, %p91;
	mov.f64 	%fd808, %fd807;
	@!%p92 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_9;
$L__BB0_9:
	setp.lt.s32 	%p94, %r36, 0;
	selp.b32 	%r216, 0, 2146435072, %p94;
	setp.ne.s32 	%p95, %r37, 1071644672;
	or.b32  	%r217, %r216, -2147483648;
	selp.b32 	%r218, %r217, %r216, %p95;
	selp.b32 	%r219, %r218, %r216, %p357;
	selp.b32 	%r220, %r219, %r216, %p349;
	mov.b32 	%r221, 0;
	mov.b64 	%fd808, {%r221, %r220};
$L__BB0_10:
	.loc	1 0 33
	add.s32 	%r186, %r185, %r184;
	mad.lo.s32 	%r177, %r23, 1, %r176;
	and.b16  	%rs68, %rs67, 128;
	sub.s32 	%r31, %r21, %r169;
	shr.s16 	%rs64, %rs63, 1;
	cvt.rn.f64.s32 	%fd187, %r196;
	.loc	1 43 33
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r38}, %fd11;
	}
	bfe.u32 	%r227, %r38, 20, 11;
	add.s32 	%r228, %r227, -1012;
	mov.b64 	%rd52, %fd11;
	shl.b64 	%rd2, %rd52, %r228;
	{ // callseq 1, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd18;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd11;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd202, [retval0+0];
	} // callseq 1
	@%p77 bra 	$L__BB0_12;
	bra.uni 	$L__BB0_11;
$L__BB0_12:
	setp.eq.s64 	%p358, %rd2, -9223372036854775808;
	setp.eq.s64 	%p102, %rd2, -9223372036854775808;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r229, %temp}, %fd202;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r230}, %fd202;
	}
	xor.b32  	%r231, %r230, -2147483648;
	mov.b64 	%fd203, {%r229, %r231};
	selp.f64 	%fd204, %fd203, %fd202, %p102;
	selp.f64 	%fd205, %fd204, %fd202, %p349;
	cvt.rzi.f64.f64 	%fd206, %fd11;
	setp.neu.f64 	%p104, %fd206, %fd11;
	selp.f64 	%fd207, 0dFFF8000000000000, %fd205, %p104;
	selp.f64 	%fd809, %fd207, %fd205, %p349;
	bra.uni 	$L__BB0_13;
$L__BB0_11:
	setp.eq.s64 	%p105, %rd2, -9223372036854775808;
	abs.f64 	%fd208, %fd11;
	setp.neu.f64 	%p106, %fd208, 0d3FE0000000000000;
	and.pred  	%p358, %p106, %p105;
	selp.b32 	%r232, %r35, 0, %p358;
	setp.lt.s32 	%p107, %r38, 0;
	or.b32  	%r233, %r232, 2146435072;
	selp.b32 	%r234, %r233, %r232, %p107;
	mov.b32 	%r235, 0;
	mov.b64 	%fd809, {%r235, %r234};
$L__BB0_13:
	.loc	1 0 33
	mul.lo.s32 	%r187, %r186, 56;
	shr.u32 	%r178, %r177, 31;
	shr.s32 	%r179, %r177, 5;
	shr.u16 	%rs69, %rs68, 7;
	cvt.s8.s32 	%rs65, %r31;
	mul.wide.s16 	%r197, %rs64, 2;
	mul.f64 	%fd12, %fd187, 0d3F92492492492492;
	.loc	1 43 33
	add.f64 	%fd32, %fd11, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r236}, %fd32;
	}
	and.b32  	%r237, %r236, 2146435072;
	setp.ne.s32 	%p108, %r237, 2146435072;
	mov.f64 	%fd810, %fd809;
	@%p108 bra 	$L__BB0_20;
	.loc	1 0 33
	mov.f64 	%fd810, %fd32;
	.loc	1 43 33
	@%p356 bra 	$L__BB0_20;
	abs.f64 	%fd209, %fd11;
	setp.nan.f64 	%p110, %fd209, %fd209;
	mov.f64 	%fd810, %fd32;
	@%p110 bra 	$L__BB0_20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r238, %temp}, %fd11;
	}
	and.b32  	%r39, %r38, 2147483647;
	setp.eq.s32 	%p111, %r39, 2146435072;
	setp.eq.s32 	%p112, %r238, 0;
	and.pred  	%p113, %p111, %p112;
	@!%p113 bra 	$L__BB0_18;
	bra.uni 	$L__BB0_17;
$L__BB0_17:
	setp.gt.f64 	%p120, %fd18, 0d3FF0000000000000;
	selp.b32 	%r247, 2146435072, 0, %p120;
	setp.lt.s32 	%p121, %r38, 0;
	xor.b32  	%r248, %r247, 2146435072;
	selp.b32 	%r249, %r248, %r247, %p121;
	setp.eq.s32 	%p122, %r116, -1;
	selp.b32 	%r250, 1072693248, %r249, %p122;
	mov.b32 	%r251, 0;
	mov.b64 	%fd810, {%r251, %r250};
	bra.uni 	$L__BB0_20;
$L__BB0_18:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r239, %temp}, %fd9;
	}
	and.b32  	%r240, %r35, 2147483647;
	setp.eq.s32 	%p114, %r240, 2146435072;
	setp.eq.s32 	%p115, %r239, 0;
	and.pred  	%p116, %p114, %p115;
	mov.f64 	%fd810, %fd809;
	@!%p116 bra 	$L__BB0_20;
	bra.uni 	$L__BB0_19;
$L__BB0_19:
	setp.lt.s32 	%p118, %r38, 0;
	selp.b32 	%r241, 0, 2146435072, %p118;
	setp.ne.s32 	%p119, %r39, 1071644672;
	or.b32  	%r242, %r241, -2147483648;
	selp.b32 	%r243, %r242, %r241, %p119;
	selp.b32 	%r244, %r243, %r241, %p358;
	selp.b32 	%r245, %r244, %r241, %p349;
	mov.b32 	%r246, 0;
	mov.b64 	%fd810, {%r246, %r245};
$L__BB0_20:
	.loc	1 0 33
	sub.s32 	%r34, %r24, %r187;
	add.s32 	%r180, %r179, %r178;
	add.s16 	%rs70, %rs67, %rs69;
	shr.s16 	%rs66, %rs65, 1;
	cvt.rn.f64.s32 	%fd188, %r197;
	.loc	1 43 33
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r40}, %fd12;
	}
	bfe.u32 	%r252, %r40, 20, 11;
	add.s32 	%r253, %r252, -1012;
	mov.b64 	%rd53, %fd12;
	shl.b64 	%rd3, %rd53, %r253;
	{ // callseq 2, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd18;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd12;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd211, [retval0+0];
	} // callseq 2
	@%p77 bra 	$L__BB0_22;
	bra.uni 	$L__BB0_21;
$L__BB0_22:
	setp.eq.s64 	%p359, %rd3, -9223372036854775808;
	setp.eq.s64 	%p126, %rd3, -9223372036854775808;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r254, %temp}, %fd211;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r255}, %fd211;
	}
	xor.b32  	%r256, %r255, -2147483648;
	mov.b64 	%fd212, {%r254, %r256};
	selp.f64 	%fd213, %fd212, %fd211, %p126;
	selp.f64 	%fd214, %fd213, %fd211, %p349;
	cvt.rzi.f64.f64 	%fd215, %fd12;
	setp.neu.f64 	%p128, %fd215, %fd12;
	selp.f64 	%fd216, 0dFFF8000000000000, %fd214, %p128;
	selp.f64 	%fd811, %fd216, %fd214, %p349;
	bra.uni 	$L__BB0_23;
$L__BB0_21:
	setp.eq.s64 	%p129, %rd3, -9223372036854775808;
	abs.f64 	%fd217, %fd12;
	setp.neu.f64 	%p130, %fd217, 0d3FE0000000000000;
	and.pred  	%p359, %p130, %p129;
	selp.b32 	%r257, %r35, 0, %p359;
	setp.lt.s32 	%p131, %r40, 0;
	or.b32  	%r258, %r257, 2146435072;
	selp.b32 	%r259, %r258, %r257, %p131;
	mov.b32 	%r260, 0;
	mov.b64 	%fd811, {%r260, %r259};
$L__BB0_23:
	.loc	1 0 33
	cvt.u16.u32 	%rs75, %r34;
	mul.lo.s32 	%r181, %r180, 56;
	cvt.s16.s8 	%rs71, %rs70;
	mul.wide.s16 	%r198, %rs66, 2;
	mul.f64 	%fd13, %fd188, 0d3F92492492492492;
	.loc	1 43 33
	add.f64 	%fd41, %fd12, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r261}, %fd41;
	}
	and.b32  	%r262, %r261, 2146435072;
	setp.ne.s32 	%p132, %r262, 2146435072;
	mov.f64 	%fd812, %fd811;
	@%p132 bra 	$L__BB0_30;
	.loc	1 0 33
	mov.f64 	%fd812, %fd41;
	.loc	1 43 33
	@%p356 bra 	$L__BB0_30;
	abs.f64 	%fd218, %fd12;
	setp.nan.f64 	%p134, %fd218, %fd218;
	mov.f64 	%fd812, %fd41;
	@%p134 bra 	$L__BB0_30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r263, %temp}, %fd12;
	}
	and.b32  	%r41, %r40, 2147483647;
	setp.eq.s32 	%p135, %r41, 2146435072;
	setp.eq.s32 	%p136, %r263, 0;
	and.pred  	%p137, %p135, %p136;
	@!%p137 bra 	$L__BB0_28;
	bra.uni 	$L__BB0_27;
$L__BB0_27:
	setp.gt.f64 	%p144, %fd18, 0d3FF0000000000000;
	selp.b32 	%r272, 2146435072, 0, %p144;
	setp.lt.s32 	%p145, %r40, 0;
	xor.b32  	%r273, %r272, 2146435072;
	selp.b32 	%r274, %r273, %r272, %p145;
	setp.eq.s32 	%p146, %r116, -1;
	selp.b32 	%r275, 1072693248, %r274, %p146;
	mov.b32 	%r276, 0;
	mov.b64 	%fd812, {%r276, %r275};
	bra.uni 	$L__BB0_30;
$L__BB0_28:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r264, %temp}, %fd9;
	}
	and.b32  	%r265, %r35, 2147483647;
	setp.eq.s32 	%p138, %r265, 2146435072;
	setp.eq.s32 	%p139, %r264, 0;
	and.pred  	%p140, %p138, %p139;
	mov.f64 	%fd812, %fd811;
	@!%p140 bra 	$L__BB0_30;
	bra.uni 	$L__BB0_29;
$L__BB0_29:
	setp.lt.s32 	%p142, %r40, 0;
	selp.b32 	%r266, 0, 2146435072, %p142;
	setp.ne.s32 	%p143, %r41, 1071644672;
	or.b32  	%r267, %r266, -2147483648;
	selp.b32 	%r268, %r267, %r266, %p143;
	selp.b32 	%r269, %r268, %r266, %p359;
	selp.b32 	%r270, %r269, %r266, %p349;
	mov.b32 	%r271, 0;
	mov.b64 	%fd812, {%r271, %r270};
$L__BB0_30:
	.loc	1 0 33
	and.b16  	%rs76, %rs75, 128;
	sub.s32 	%r33, %r23, %r181;
	shr.s16 	%rs72, %rs71, 1;
	cvt.rn.f64.s32 	%fd189, %r198;
	.loc	1 43 33
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r42}, %fd13;
	}
	bfe.u32 	%r277, %r42, 20, 11;
	add.s32 	%r278, %r277, -1012;
	mov.b64 	%rd54, %fd13;
	shl.b64 	%rd4, %rd54, %r278;
	{ // callseq 3, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd18;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd13;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd220, [retval0+0];
	} // callseq 3
	@%p77 bra 	$L__BB0_32;
	bra.uni 	$L__BB0_31;
$L__BB0_32:
	setp.eq.s64 	%p360, %rd4, -9223372036854775808;
	setp.eq.s64 	%p150, %rd4, -9223372036854775808;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r279, %temp}, %fd220;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r280}, %fd220;
	}
	xor.b32  	%r281, %r280, -2147483648;
	mov.b64 	%fd221, {%r279, %r281};
	selp.f64 	%fd222, %fd221, %fd220, %p150;
	selp.f64 	%fd223, %fd222, %fd220, %p349;
	cvt.rzi.f64.f64 	%fd224, %fd13;
	setp.neu.f64 	%p152, %fd224, %fd13;
	selp.f64 	%fd225, 0dFFF8000000000000, %fd223, %p152;
	selp.f64 	%fd813, %fd225, %fd223, %p349;
	bra.uni 	$L__BB0_33;
$L__BB0_31:
	setp.eq.s64 	%p153, %rd4, -9223372036854775808;
	abs.f64 	%fd226, %fd13;
	setp.neu.f64 	%p154, %fd226, 0d3FE0000000000000;
	and.pred  	%p360, %p154, %p153;
	selp.b32 	%r282, %r35, 0, %p360;
	setp.lt.s32 	%p155, %r42, 0;
	or.b32  	%r283, %r282, 2146435072;
	selp.b32 	%r284, %r283, %r282, %p155;
	mov.b32 	%r285, 0;
	mov.b64 	%fd813, {%r285, %r284};
$L__BB0_33:
	.loc	1 0 33
	shr.u16 	%rs77, %rs76, 7;
	cvt.s8.s32 	%rs73, %r33;
	mul.wide.s16 	%r199, %rs72, 2;
	mul.f64 	%fd14, %fd189, 0d3F92492492492492;
	.loc	1 43 33
	add.f64 	%fd50, %fd13, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r286}, %fd50;
	}
	and.b32  	%r287, %r286, 2146435072;
	setp.ne.s32 	%p156, %r287, 2146435072;
	mov.f64 	%fd814, %fd813;
	@%p156 bra 	$L__BB0_40;
	.loc	1 0 33
	mov.f64 	%fd814, %fd50;
	.loc	1 43 33
	@%p356 bra 	$L__BB0_40;
	abs.f64 	%fd227, %fd13;
	setp.nan.f64 	%p158, %fd227, %fd227;
	mov.f64 	%fd814, %fd50;
	@%p158 bra 	$L__BB0_40;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r288, %temp}, %fd13;
	}
	and.b32  	%r43, %r42, 2147483647;
	setp.eq.s32 	%p159, %r43, 2146435072;
	setp.eq.s32 	%p160, %r288, 0;
	and.pred  	%p161, %p159, %p160;
	@!%p161 bra 	$L__BB0_38;
	bra.uni 	$L__BB0_37;
$L__BB0_37:
	setp.gt.f64 	%p168, %fd18, 0d3FF0000000000000;
	selp.b32 	%r297, 2146435072, 0, %p168;
	setp.lt.s32 	%p169, %r42, 0;
	xor.b32  	%r298, %r297, 2146435072;
	selp.b32 	%r299, %r298, %r297, %p169;
	setp.eq.s32 	%p170, %r116, -1;
	selp.b32 	%r300, 1072693248, %r299, %p170;
	mov.b32 	%r301, 0;
	mov.b64 	%fd814, {%r301, %r300};
	bra.uni 	$L__BB0_40;
$L__BB0_38:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r289, %temp}, %fd9;
	}
	and.b32  	%r290, %r35, 2147483647;
	setp.eq.s32 	%p162, %r290, 2146435072;
	setp.eq.s32 	%p163, %r289, 0;
	and.pred  	%p164, %p162, %p163;
	mov.f64 	%fd814, %fd813;
	@!%p164 bra 	$L__BB0_40;
	bra.uni 	$L__BB0_39;
$L__BB0_39:
	setp.lt.s32 	%p166, %r42, 0;
	selp.b32 	%r291, 0, 2146435072, %p166;
	setp.ne.s32 	%p167, %r43, 1071644672;
	or.b32  	%r292, %r291, -2147483648;
	selp.b32 	%r293, %r292, %r291, %p167;
	selp.b32 	%r294, %r293, %r291, %p360;
	selp.b32 	%r295, %r294, %r291, %p349;
	mov.b32 	%r296, 0;
	mov.b64 	%fd814, {%r296, %r295};
$L__BB0_40:
	.loc	1 0 33
	add.s16 	%rs78, %rs75, %rs77;
	shr.s16 	%rs74, %rs73, 1;
	cvt.rn.f64.s32 	%fd190, %r199;
	.loc	1 43 33
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd14;
	}
	bfe.u32 	%r302, %r44, 20, 11;
	add.s32 	%r303, %r302, -1012;
	mov.b64 	%rd55, %fd14;
	shl.b64 	%rd5, %rd55, %r303;
	{ // callseq 4, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd18;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd14;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd229, [retval0+0];
	} // callseq 4
	@%p77 bra 	$L__BB0_42;
	bra.uni 	$L__BB0_41;
$L__BB0_42:
	setp.eq.s64 	%p361, %rd5, -9223372036854775808;
	setp.eq.s64 	%p174, %rd5, -9223372036854775808;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r304, %temp}, %fd229;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r305}, %fd229;
	}
	xor.b32  	%r306, %r305, -2147483648;
	mov.b64 	%fd230, {%r304, %r306};
	selp.f64 	%fd231, %fd230, %fd229, %p174;
	selp.f64 	%fd232, %fd231, %fd229, %p349;
	cvt.rzi.f64.f64 	%fd233, %fd14;
	setp.neu.f64 	%p176, %fd233, %fd14;
	selp.f64 	%fd234, 0dFFF8000000000000, %fd232, %p176;
	selp.f64 	%fd815, %fd234, %fd232, %p349;
	bra.uni 	$L__BB0_43;
$L__BB0_41:
	setp.eq.s64 	%p177, %rd5, -9223372036854775808;
	abs.f64 	%fd235, %fd14;
	setp.neu.f64 	%p178, %fd235, 0d3FE0000000000000;
	and.pred  	%p361, %p178, %p177;
	selp.b32 	%r307, %r35, 0, %p361;
	setp.lt.s32 	%p179, %r44, 0;
	or.b32  	%r308, %r307, 2146435072;
	selp.b32 	%r309, %r308, %r307, %p179;
	mov.b32 	%r310, 0;
	mov.b64 	%fd815, {%r310, %r309};
$L__BB0_43:
	.loc	1 0 33
	cvt.s16.s8 	%rs79, %rs78;
	mul.wide.s16 	%r200, %rs74, 2;
	mul.f64 	%fd15, %fd190, 0d3F92492492492492;
	.loc	1 43 33
	add.f64 	%fd59, %fd14, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r311}, %fd59;
	}
	and.b32  	%r312, %r311, 2146435072;
	setp.ne.s32 	%p180, %r312, 2146435072;
	mov.f64 	%fd816, %fd815;
	@%p180 bra 	$L__BB0_50;
	.loc	1 0 33
	mov.f64 	%fd816, %fd59;
	.loc	1 43 33
	@%p356 bra 	$L__BB0_50;
	abs.f64 	%fd236, %fd14;
	setp.nan.f64 	%p182, %fd236, %fd236;
	mov.f64 	%fd816, %fd59;
	@%p182 bra 	$L__BB0_50;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r313, %temp}, %fd14;
	}
	and.b32  	%r45, %r44, 2147483647;
	setp.eq.s32 	%p183, %r45, 2146435072;
	setp.eq.s32 	%p184, %r313, 0;
	and.pred  	%p185, %p183, %p184;
	@!%p185 bra 	$L__BB0_48;
	bra.uni 	$L__BB0_47;
$L__BB0_47:
	setp.gt.f64 	%p192, %fd18, 0d3FF0000000000000;
	selp.b32 	%r322, 2146435072, 0, %p192;
	setp.lt.s32 	%p193, %r44, 0;
	xor.b32  	%r323, %r322, 2146435072;
	selp.b32 	%r324, %r323, %r322, %p193;
	setp.eq.s32 	%p194, %r116, -1;
	selp.b32 	%r325, 1072693248, %r324, %p194;
	mov.b32 	%r326, 0;
	mov.b64 	%fd816, {%r326, %r325};
	bra.uni 	$L__BB0_50;
$L__BB0_48:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r314, %temp}, %fd9;
	}
	and.b32  	%r315, %r35, 2147483647;
	setp.eq.s32 	%p186, %r315, 2146435072;
	setp.eq.s32 	%p187, %r314, 0;
	and.pred  	%p188, %p186, %p187;
	mov.f64 	%fd816, %fd815;
	@!%p188 bra 	$L__BB0_50;
	bra.uni 	$L__BB0_49;
$L__BB0_49:
	setp.lt.s32 	%p190, %r44, 0;
	selp.b32 	%r316, 0, 2146435072, %p190;
	setp.ne.s32 	%p191, %r45, 1071644672;
	or.b32  	%r317, %r316, -2147483648;
	selp.b32 	%r318, %r317, %r316, %p191;
	selp.b32 	%r319, %r318, %r316, %p361;
	selp.b32 	%r320, %r319, %r316, %p349;
	mov.b32 	%r321, 0;
	mov.b64 	%fd816, {%r321, %r320};
$L__BB0_50:
	.loc	1 0 33
	shr.s16 	%rs80, %rs79, 1;
	cvt.rn.f64.s32 	%fd191, %r200;
	.loc	1 43 33
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd15;
	}
	bfe.u32 	%r327, %r46, 20, 11;
	add.s32 	%r328, %r327, -1012;
	mov.b64 	%rd56, %fd15;
	shl.b64 	%rd6, %rd56, %r328;
	{ // callseq 5, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd18;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd15;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd238, [retval0+0];
	} // callseq 5
	@%p77 bra 	$L__BB0_52;
	bra.uni 	$L__BB0_51;
$L__BB0_52:
	setp.eq.s64 	%p362, %rd6, -9223372036854775808;
	setp.eq.s64 	%p198, %rd6, -9223372036854775808;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r329, %temp}, %fd238;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r330}, %fd238;
	}
	xor.b32  	%r331, %r330, -2147483648;
	mov.b64 	%fd239, {%r329, %r331};
	selp.f64 	%fd240, %fd239, %fd238, %p198;
	selp.f64 	%fd241, %fd240, %fd238, %p349;
	cvt.rzi.f64.f64 	%fd242, %fd15;
	setp.neu.f64 	%p200, %fd242, %fd15;
	selp.f64 	%fd243, 0dFFF8000000000000, %fd241, %p200;
	selp.f64 	%fd817, %fd243, %fd241, %p349;
	bra.uni 	$L__BB0_53;
$L__BB0_51:
	setp.eq.s64 	%p201, %rd6, -9223372036854775808;
	abs.f64 	%fd244, %fd15;
	setp.neu.f64 	%p202, %fd244, 0d3FE0000000000000;
	and.pred  	%p362, %p202, %p201;
	selp.b32 	%r332, %r35, 0, %p362;
	setp.lt.s32 	%p203, %r46, 0;
	or.b32  	%r333, %r332, 2146435072;
	selp.b32 	%r334, %r333, %r332, %p203;
	mov.b32 	%r335, 0;
	mov.b64 	%fd817, {%r335, %r334};
$L__BB0_53:
	.loc	1 0 33
	mul.wide.s16 	%r201, %rs80, 2;
	mul.f64 	%fd16, %fd191, 0d3F92492492492492;
	.loc	1 43 33
	add.f64 	%fd68, %fd15, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r336}, %fd68;
	}
	and.b32  	%r337, %r336, 2146435072;
	setp.ne.s32 	%p204, %r337, 2146435072;
	mov.f64 	%fd818, %fd817;
	@%p204 bra 	$L__BB0_60;
	.loc	1 0 33
	mov.f64 	%fd818, %fd68;
	.loc	1 43 33
	@%p356 bra 	$L__BB0_60;
	abs.f64 	%fd245, %fd15;
	setp.nan.f64 	%p206, %fd245, %fd245;
	mov.f64 	%fd818, %fd68;
	@%p206 bra 	$L__BB0_60;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r338, %temp}, %fd15;
	}
	and.b32  	%r47, %r46, 2147483647;
	setp.eq.s32 	%p207, %r47, 2146435072;
	setp.eq.s32 	%p208, %r338, 0;
	and.pred  	%p209, %p207, %p208;
	@!%p209 bra 	$L__BB0_58;
	bra.uni 	$L__BB0_57;
$L__BB0_57:
	setp.gt.f64 	%p216, %fd18, 0d3FF0000000000000;
	selp.b32 	%r347, 2146435072, 0, %p216;
	setp.lt.s32 	%p217, %r46, 0;
	xor.b32  	%r348, %r347, 2146435072;
	selp.b32 	%r349, %r348, %r347, %p217;
	setp.eq.s32 	%p218, %r116, -1;
	selp.b32 	%r350, 1072693248, %r349, %p218;
	mov.b32 	%r351, 0;
	mov.b64 	%fd818, {%r351, %r350};
	bra.uni 	$L__BB0_60;
$L__BB0_58:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r339, %temp}, %fd9;
	}
	and.b32  	%r340, %r35, 2147483647;
	setp.eq.s32 	%p210, %r340, 2146435072;
	setp.eq.s32 	%p211, %r339, 0;
	and.pred  	%p212, %p210, %p211;
	mov.f64 	%fd818, %fd817;
	@!%p212 bra 	$L__BB0_60;
	bra.uni 	$L__BB0_59;
$L__BB0_59:
	setp.lt.s32 	%p214, %r46, 0;
	selp.b32 	%r341, 0, 2146435072, %p214;
	setp.ne.s32 	%p215, %r47, 1071644672;
	or.b32  	%r342, %r341, -2147483648;
	selp.b32 	%r343, %r342, %r341, %p215;
	selp.b32 	%r344, %r343, %r341, %p362;
	selp.b32 	%r345, %r344, %r341, %p349;
	mov.b32 	%r346, 0;
	mov.b64 	%fd818, {%r346, %r345};
$L__BB0_60:
	.loc	1 0 33
	// begin inline asm
	cvt.f32.bf16 %r119, %rs1;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r120, %rs3;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r127, %rs25;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r128, %rs27;
	// end inline asm
	cvt.rn.f64.s32 	%fd192, %r201;
	.loc	1 43 33
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r48}, %fd16;
	}
	bfe.u32 	%r352, %r48, 20, 11;
	add.s32 	%r353, %r352, -1012;
	mov.b64 	%rd57, %fd16;
	shl.b64 	%rd7, %rd57, %r353;
	{ // callseq 6, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd18;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd16;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd247, [retval0+0];
	} // callseq 6
	@%p77 bra 	$L__BB0_62;
	bra.uni 	$L__BB0_61;
$L__BB0_62:
	setp.eq.s64 	%p363, %rd7, -9223372036854775808;
	setp.eq.s64 	%p222, %rd7, -9223372036854775808;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r354, %temp}, %fd247;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r355}, %fd247;
	}
	xor.b32  	%r356, %r355, -2147483648;
	mov.b64 	%fd248, {%r354, %r356};
	selp.f64 	%fd249, %fd248, %fd247, %p222;
	selp.f64 	%fd250, %fd249, %fd247, %p349;
	cvt.rzi.f64.f64 	%fd251, %fd16;
	setp.neu.f64 	%p224, %fd251, %fd16;
	selp.f64 	%fd252, 0dFFF8000000000000, %fd250, %p224;
	selp.f64 	%fd819, %fd252, %fd250, %p349;
	bra.uni 	$L__BB0_63;
$L__BB0_61:
	setp.eq.s64 	%p225, %rd7, -9223372036854775808;
	abs.f64 	%fd253, %fd16;
	setp.neu.f64 	%p226, %fd253, 0d3FE0000000000000;
	and.pred  	%p363, %p226, %p225;
	selp.b32 	%r357, %r35, 0, %p363;
	setp.lt.s32 	%p227, %r48, 0;
	or.b32  	%r358, %r357, 2146435072;
	selp.b32 	%r359, %r358, %r357, %p227;
	mov.b32 	%r360, 0;
	mov.b64 	%fd819, {%r360, %r359};
$L__BB0_63:
	.loc	1 0 33
	// begin inline asm
	cvt.f32.bf16 %r121, %rs5;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r129, %rs29;
	// end inline asm
	mov.b32 	%f1, %r119;
	mov.b32 	%f2, %r120;
	mov.b32 	%f9, %r127;
	mov.b32 	%f10, %r128;
	setp.eq.f64 	%p101, %fd10, 0d0000000000000000;
	setp.eq.f64 	%p125, %fd11, 0d0000000000000000;
	setp.eq.f64 	%p149, %fd12, 0d0000000000000000;
	setp.eq.f64 	%p173, %fd13, 0d0000000000000000;
	setp.eq.f64 	%p197, %fd14, 0d0000000000000000;
	mul.f64 	%fd17, %fd192, 0d3F92492492492492;
	.loc	1 43 33
	add.f64 	%fd77, %fd16, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r361}, %fd77;
	}
	and.b32  	%r362, %r361, 2146435072;
	setp.ne.s32 	%p228, %r362, 2146435072;
	mov.f64 	%fd820, %fd819;
	@%p228 bra 	$L__BB0_70;
	.loc	1 0 33
	mov.f64 	%fd820, %fd77;
	.loc	1 43 33
	@%p356 bra 	$L__BB0_70;
	abs.f64 	%fd254, %fd16;
	setp.nan.f64 	%p230, %fd254, %fd254;
	mov.f64 	%fd820, %fd77;
	@%p230 bra 	$L__BB0_70;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r363, %temp}, %fd16;
	}
	and.b32  	%r49, %r48, 2147483647;
	setp.eq.s32 	%p231, %r49, 2146435072;
	setp.eq.s32 	%p232, %r363, 0;
	and.pred  	%p233, %p231, %p232;
	@!%p233 bra 	$L__BB0_68;
	bra.uni 	$L__BB0_67;
$L__BB0_67:
	setp.gt.f64 	%p240, %fd18, 0d3FF0000000000000;
	selp.b32 	%r372, 2146435072, 0, %p240;
	setp.lt.s32 	%p241, %r48, 0;
	xor.b32  	%r373, %r372, 2146435072;
	selp.b32 	%r374, %r373, %r372, %p241;
	setp.eq.s32 	%p242, %r116, -1;
	selp.b32 	%r375, 1072693248, %r374, %p242;
	mov.b32 	%r376, 0;
	mov.b64 	%fd820, {%r376, %r375};
	bra.uni 	$L__BB0_70;
$L__BB0_68:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r364, %temp}, %fd9;
	}
	and.b32  	%r365, %r35, 2147483647;
	setp.eq.s32 	%p234, %r365, 2146435072;
	setp.eq.s32 	%p235, %r364, 0;
	and.pred  	%p236, %p234, %p235;
	mov.f64 	%fd820, %fd819;
	@!%p236 bra 	$L__BB0_70;
	bra.uni 	$L__BB0_69;
$L__BB0_69:
	setp.lt.s32 	%p238, %r48, 0;
	selp.b32 	%r366, 0, 2146435072, %p238;
	setp.ne.s32 	%p239, %r49, 1071644672;
	or.b32  	%r367, %r366, -2147483648;
	selp.b32 	%r368, %r367, %r366, %p239;
	selp.b32 	%r369, %r368, %r366, %p363;
	selp.b32 	%r370, %r369, %r366, %p349;
	mov.b32 	%r371, 0;
	mov.b64 	%fd820, {%r371, %r370};
$L__BB0_70:
	.loc	1 0 33
	setp.eq.f64 	%p221, %fd15, 0d0000000000000000;
	// begin inline asm
	cvt.f32.bf16 %r122, %rs7;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r130, %rs31;
	// end inline asm
	mov.b32 	%f3, %r121;
	mov.b32 	%f11, %r129;
	selp.f32 	%f17, %f1, %f9, %p73;
	selp.f32 	%f18, %f2, %f10, %p73;
	setp.eq.s32 	%p100, %r116, 1;
	selp.f64 	%fd201, 0d3FF0000000000000, %fd808, %p101;
	selp.f64 	%fd210, 0d3FF0000000000000, %fd810, %p125;
	selp.f64 	%fd219, 0d3FF0000000000000, %fd812, %p149;
	selp.f64 	%fd228, 0d3FF0000000000000, %fd814, %p173;
	selp.f64 	%fd237, 0d3FF0000000000000, %fd816, %p197;
	.loc	1 43 33
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd17;
	}
	bfe.u32 	%r377, %r50, 20, 11;
	add.s32 	%r378, %r377, -1012;
	mov.b64 	%rd58, %fd17;
	shl.b64 	%rd8, %rd58, %r378;
	{ // callseq 7, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd18;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd17;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd256, [retval0+0];
	} // callseq 7
	@%p77 bra 	$L__BB0_72;
	bra.uni 	$L__BB0_71;
$L__BB0_72:
	setp.eq.s64 	%p364, %rd8, -9223372036854775808;
	setp.eq.s64 	%p246, %rd8, -9223372036854775808;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r379, %temp}, %fd256;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r380}, %fd256;
	}
	xor.b32  	%r381, %r380, -2147483648;
	mov.b64 	%fd257, {%r379, %r381};
	selp.f64 	%fd258, %fd257, %fd256, %p246;
	selp.f64 	%fd259, %fd258, %fd256, %p349;
	cvt.rzi.f64.f64 	%fd260, %fd17;
	setp.neu.f64 	%p248, %fd260, %fd17;
	selp.f64 	%fd261, 0dFFF8000000000000, %fd259, %p248;
	selp.f64 	%fd821, %fd261, %fd259, %p349;
	bra.uni 	$L__BB0_73;
$L__BB0_71:
	setp.eq.s64 	%p249, %rd8, -9223372036854775808;
	abs.f64 	%fd262, %fd17;
	setp.neu.f64 	%p250, %fd262, 0d3FE0000000000000;
	and.pred  	%p364, %p250, %p249;
	selp.b32 	%r382, %r35, 0, %p364;
	setp.lt.s32 	%p251, %r50, 0;
	or.b32  	%r383, %r382, 2146435072;
	selp.b32 	%r384, %r383, %r382, %p251;
	mov.b32 	%r385, 0;
	mov.b64 	%fd821, {%r385, %r384};
$L__BB0_73:
	.loc	1 0 33
	setp.eq.f64 	%p245, %fd16, 0d0000000000000000;
	selp.f64 	%fd246, 0d3FF0000000000000, %fd818, %p221;
	// begin inline asm
	cvt.f32.bf16 %r123, %rs9;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r131, %rs33;
	// end inline asm
	mov.b32 	%f4, %r122;
	mov.b32 	%f12, %r130;
	selp.f32 	%f19, %f3, %f11, %p73;
	add.u64 	%rd29, %SP, 0;
	cvt.f64.f32 	%fd1, %f17;
	cvt.f64.f32 	%fd2, %f18;
	.loc	1 43 33
	selp.f64 	%fd27, 0d3FF0000000000000, %fd201, %p100;
	selp.f64 	%fd36, 0d3FF0000000000000, %fd210, %p100;
	selp.f64 	%fd45, 0d3FF0000000000000, %fd219, %p100;
	selp.f64 	%fd54, 0d3FF0000000000000, %fd228, %p100;
	selp.f64 	%fd63, 0d3FF0000000000000, %fd237, %p100;
	add.f64 	%fd86, %fd17, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r386}, %fd86;
	}
	and.b32  	%r387, %r386, 2146435072;
	setp.ne.s32 	%p252, %r387, 2146435072;
	mov.f64 	%fd822, %fd821;
	@%p252 bra 	$L__BB0_80;
	.loc	1 0 33
	mov.f64 	%fd822, %fd86;
	.loc	1 43 33
	@%p356 bra 	$L__BB0_80;
	abs.f64 	%fd263, %fd17;
	setp.nan.f64 	%p254, %fd263, %fd263;
	mov.f64 	%fd822, %fd86;
	@%p254 bra 	$L__BB0_80;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r388, %temp}, %fd17;
	}
	and.b32  	%r51, %r50, 2147483647;
	setp.eq.s32 	%p255, %r51, 2146435072;
	setp.eq.s32 	%p256, %r388, 0;
	and.pred  	%p257, %p255, %p256;
	@!%p257 bra 	$L__BB0_78;
	bra.uni 	$L__BB0_77;
$L__BB0_77:
	setp.gt.f64 	%p264, %fd18, 0d3FF0000000000000;
	selp.b32 	%r397, 2146435072, 0, %p264;
	setp.lt.s32 	%p265, %r50, 0;
	xor.b32  	%r398, %r397, 2146435072;
	selp.b32 	%r399, %r398, %r397, %p265;
	setp.eq.s32 	%p266, %r116, -1;
	selp.b32 	%r400, 1072693248, %r399, %p266;
	mov.b32 	%r401, 0;
	mov.b64 	%fd822, {%r401, %r400};
	bra.uni 	$L__BB0_80;
$L__BB0_78:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r389, %temp}, %fd9;
	}
	and.b32  	%r390, %r35, 2147483647;
	setp.eq.s32 	%p258, %r390, 2146435072;
	setp.eq.s32 	%p259, %r389, 0;
	and.pred  	%p260, %p258, %p259;
	mov.f64 	%fd822, %fd821;
	@!%p260 bra 	$L__BB0_80;
	bra.uni 	$L__BB0_79;
$L__BB0_79:
	setp.lt.s32 	%p262, %r50, 0;
	selp.b32 	%r391, 0, 2146435072, %p262;
	setp.ne.s32 	%p263, %r51, 1071644672;
	or.b32  	%r392, %r391, -2147483648;
	selp.b32 	%r393, %r392, %r391, %p263;
	selp.b32 	%r394, %r393, %r391, %p364;
	selp.b32 	%r395, %r394, %r391, %p349;
	mov.b32 	%r396, 0;
	mov.b64 	%fd822, {%r396, %r395};
$L__BB0_80:
	.loc	1 0 33
	selp.f64 	%fd255, 0d3FF0000000000000, %fd820, %p245;
	// begin inline asm
	cvt.f32.bf16 %r124, %rs11;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r132, %rs35;
	// end inline asm
	selp.f64 	%fd72, 0d3FF0000000000000, %fd246, %p100;
	mov.b32 	%f5, %r123;
	mov.b32 	%f13, %r131;
	selp.f32 	%f20, %f4, %f12, %p73;
	cvt.f64.f32 	%fd3, %f19;
	{ .reg .b64 %tmp;
	  cvta.to.local.u64 	%tmp, %rd29;
	  cvt.u32.u64 	%r1, %tmp; }
	.loc	1 43 33
	setp.eq.f64 	%p268, %fd17, 0d0000000000000000;
	.loc	1 45 20
	mov.b64 	%rd61, %fd27;
	mov.u64 	%rd78, 4607182418800017408;
	// begin inline asm
	div.rn.f64 %rd59, %rd78, %rd61;
	// end inline asm
	mov.b64 	%fd266, %rd59;
	mov.b64 	%rd64, %fd36;
	// begin inline asm
	div.rn.f64 %rd62, %rd78, %rd64;
	// end inline asm
	mov.b64 	%fd267, %rd62;
	mov.b64 	%rd67, %fd45;
	// begin inline asm
	div.rn.f64 %rd65, %rd78, %rd67;
	// end inline asm
	mov.b64 	%fd268, %rd65;
	mov.b64 	%rd70, %fd54;
	// begin inline asm
	div.rn.f64 %rd68, %rd78, %rd70;
	// end inline asm
	mov.b64 	%rd73, %fd63;
	.loc	1 49 20
	mul.f64 	%fd90, %fd266, %fd1;
	mul.f64 	%fd91, %fd267, %fd2;
	.loc	1 50 26
	{
	.reg .b32 %temp; 
	mov.b64 	{%r402, %temp}, %fd90;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r403}, %fd90;
	}
	and.b32  	%r404, %r403, 2147483647;
	setp.eq.s32 	%p269, %r404, 2146435072;
	setp.eq.s32 	%p270, %r402, 0;
	and.pred  	%p25, %p269, %p270;
	mov.f64 	%fd787, 0d3FF921FB54442D18;
	mov.f64 	%fd788, 0d3C91A62633145C00;
	mov.f64 	%fd789, 0d397B839A252049C0;
	abs.f64 	%fd790, %fd90;
	mul.f64 	%fd791, %fd90, 0d3FE45F306DC9C883;
	mov.f64 	%fd806, 0d0000000000000000;
	@%p25 bra 	$L__BB0_137;
	bra.uni 	$L__BB0_81;
$L__BB0_137:
	mul.rn.f64 	%fd824, %fd90, %fd806;
	mov.b32 	%r547, 1;
	bra.uni 	$L__BB0_84;
$L__BB0_81:
	cvt.rni.s32.f64 	%r546, %fd791;
	st.local.u32 	[%r1], %r546;
	cvt.rn.f64.s32 	%fd276, %r546;
	neg.f64 	%fd277, %fd276;
	fma.rn.f64 	%fd279, %fd277, %fd787, %fd90;
	fma.rn.f64 	%fd281, %fd277, %fd788, %fd279;
	fma.rn.f64 	%fd824, %fd277, %fd789, %fd281;
	setp.ltu.f64 	%p271, %fd790, 0d41E0000000000000;
	@%p271 bra 	$L__BB0_83;
	{ // callseq 8, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd90;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd824, [retval0+0];
	} // callseq 8
	ld.local.u32 	%r546, [%r1];
$L__BB0_83:
	add.s32 	%r547, %r546, 1;
$L__BB0_84:
	.loc	1 0 26
	selp.f64 	%fd264, 0d3FF0000000000000, %fd822, %p268;
	// begin inline asm
	cvt.f32.bf16 %r125, %rs13;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r133, %rs37;
	// end inline asm
	selp.f64 	%fd81, 0d3FF0000000000000, %fd255, %p100;
	mov.b32 	%f6, %r124;
	mov.b32 	%f14, %r132;
	mov.b64 	%rd76, %fd72;
	selp.f32 	%f21, %f5, %f13, %p74;
	// begin inline asm
	div.rn.f64 %rd71, %rd78, %rd73;
	// end inline asm
	cvt.f64.f32 	%fd4, %f20;
	mov.b64 	%fd269, %rd68;
	mul.f64 	%fd92, %fd268, %fd3;
	.loc	1 50 26
	and.b32  	%r406, %r547, 1;
	shl.b32 	%r407, %r406, 3;
	mul.wide.u32 	%rd84, %r407, 8;
	mov.u64 	%rd85, __cudart_sin_cos_coeffs;
	add.s64 	%rd86, %rd85, %rd84;
	ld.global.nc.f64 	%fd287, [%rd86+8];
	ld.global.nc.f64 	%fd289, [%rd86+16];
	ld.global.nc.f64 	%fd291, [%rd86+24];
	ld.global.nc.f64 	%fd293, [%rd86+32];
	ld.global.nc.f64 	%fd295, [%rd86+40];
	ld.global.nc.f64 	%fd297, [%rd86+48];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r409, %temp}, %fd91;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r410}, %fd91;
	}
	and.b32  	%r411, %r410, 2147483647;
	setp.eq.s32 	%p274, %r411, 2146435072;
	setp.eq.s32 	%p275, %r409, 0;
	and.pred  	%p26, %p274, %p275;
	abs.f64 	%fd792, %fd91;
	mul.f64 	%fd793, %fd91, 0d3FE45F306DC9C883;
	@%p26 bra 	$L__BB0_138;
	bra.uni 	$L__BB0_85;
$L__BB0_138:
	mul.rn.f64 	%fd826, %fd91, %fd806;
	mov.b32 	%r549, 1;
	bra.uni 	$L__BB0_88;
$L__BB0_85:
	cvt.rni.s32.f64 	%r548, %fd793;
	st.local.u32 	[%r1], %r548;
	cvt.rn.f64.s32 	%fd308, %r548;
	neg.f64 	%fd309, %fd308;
	fma.rn.f64 	%fd311, %fd309, %fd787, %fd91;
	fma.rn.f64 	%fd313, %fd309, %fd788, %fd311;
	fma.rn.f64 	%fd826, %fd309, %fd789, %fd313;
	setp.ltu.f64 	%p276, %fd792, 0d41E0000000000000;
	@%p276 bra 	$L__BB0_87;
	{ // callseq 9, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd91;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd826, [retval0+0];
	} // callseq 9
	ld.local.u32 	%r548, [%r1];
$L__BB0_87:
	add.s32 	%r549, %r548, 1;
$L__BB0_88:
	.loc	1 0 26
	// begin inline asm
	cvt.f32.bf16 %r126, %rs15;
	// end inline asm
	// begin inline asm
	cvt.f32.bf16 %r134, %rs39;
	// end inline asm
	selp.f64 	%fd265, 0d3FF0000000000000, %fd264, %p100;
	mov.b32 	%f7, %r125;
	mov.b32 	%f15, %r133;
	mov.b64 	%rd79, %fd81;
	selp.f32 	%f22, %f6, %f14, %p74;
	// begin inline asm
	div.rn.f64 %rd74, %rd78, %rd76;
	// end inline asm
	cvt.f64.f32 	%fd5, %f21;
	mov.b64 	%fd270, %rd71;
	mul.f64 	%fd93, %fd269, %fd4;
	.loc	1 50 26
	and.b32  	%r413, %r549, 1;
	shl.b32 	%r414, %r413, 3;
	mul.wide.u32 	%rd88, %r414, 8;
	add.s64 	%rd90, %rd85, %rd88;
	ld.global.nc.f64 	%fd319, [%rd90+8];
	ld.global.nc.f64 	%fd321, [%rd90+16];
	ld.global.nc.f64 	%fd323, [%rd90+24];
	ld.global.nc.f64 	%fd325, [%rd90+32];
	ld.global.nc.f64 	%fd327, [%rd90+40];
	ld.global.nc.f64 	%fd329, [%rd90+48];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r416, %temp}, %fd92;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r417}, %fd92;
	}
	and.b32  	%r418, %r417, 2147483647;
	setp.eq.s32 	%p279, %r418, 2146435072;
	setp.eq.s32 	%p280, %r416, 0;
	and.pred  	%p27, %p279, %p280;
	abs.f64 	%fd794, %fd92;
	mul.f64 	%fd795, %fd92, 0d3FE45F306DC9C883;
	@%p27 bra 	$L__BB0_139;
	bra.uni 	$L__BB0_89;
$L__BB0_139:
	mul.rn.f64 	%fd828, %fd92, %fd806;
	mov.b32 	%r551, 1;
	bra.uni 	$L__BB0_92;
$L__BB0_89:
	cvt.rni.s32.f64 	%r550, %fd795;
	st.local.u32 	[%r1], %r550;
	cvt.rn.f64.s32 	%fd340, %r550;
	neg.f64 	%fd341, %fd340;
	fma.rn.f64 	%fd343, %fd341, %fd787, %fd92;
	fma.rn.f64 	%fd345, %fd341, %fd788, %fd343;
	fma.rn.f64 	%fd828, %fd341, %fd789, %fd345;
	setp.ltu.f64 	%p281, %fd794, 0d41E0000000000000;
	@%p281 bra 	$L__BB0_91;
	{ // callseq 10, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd92;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd828, [retval0+0];
	} // callseq 10
	ld.local.u32 	%r550, [%r1];
$L__BB0_91:
	add.s32 	%r551, %r550, 1;
$L__BB0_92:
	.loc	1 0 26
	mov.b32 	%f8, %r126;
	mov.b32 	%f16, %r134;
	mov.b64 	%rd82, %fd265;
	selp.f32 	%f23, %f7, %f15, %p74;
	// begin inline asm
	div.rn.f64 %rd77, %rd78, %rd79;
	// end inline asm
	cvt.f64.f32 	%fd6, %f22;
	mov.b64 	%fd271, %rd74;
	mul.f64 	%fd94, %fd270, %fd5;
	.loc	1 50 26
	and.b32  	%r420, %r551, 1;
	shl.b32 	%r421, %r420, 3;
	mul.wide.u32 	%rd92, %r421, 8;
	add.s64 	%rd94, %rd85, %rd92;
	ld.global.nc.f64 	%fd351, [%rd94+8];
	ld.global.nc.f64 	%fd353, [%rd94+16];
	ld.global.nc.f64 	%fd355, [%rd94+24];
	ld.global.nc.f64 	%fd357, [%rd94+32];
	ld.global.nc.f64 	%fd359, [%rd94+40];
	ld.global.nc.f64 	%fd361, [%rd94+48];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r423, %temp}, %fd93;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r424}, %fd93;
	}
	and.b32  	%r425, %r424, 2147483647;
	setp.eq.s32 	%p284, %r425, 2146435072;
	setp.eq.s32 	%p285, %r423, 0;
	and.pred  	%p28, %p284, %p285;
	abs.f64 	%fd796, %fd93;
	mul.f64 	%fd797, %fd93, 0d3FE45F306DC9C883;
	@%p28 bra 	$L__BB0_140;
	bra.uni 	$L__BB0_93;
$L__BB0_140:
	mul.rn.f64 	%fd830, %fd93, %fd806;
	mov.b32 	%r553, 1;
	bra.uni 	$L__BB0_96;
$L__BB0_93:
	cvt.rni.s32.f64 	%r552, %fd797;
	st.local.u32 	[%r1], %r552;
	cvt.rn.f64.s32 	%fd372, %r552;
	neg.f64 	%fd373, %fd372;
	fma.rn.f64 	%fd375, %fd373, %fd787, %fd93;
	fma.rn.f64 	%fd377, %fd373, %fd788, %fd375;
	fma.rn.f64 	%fd830, %fd373, %fd789, %fd377;
	setp.ltu.f64 	%p286, %fd796, 0d41E0000000000000;
	@%p286 bra 	$L__BB0_95;
	{ // callseq 11, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd93;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd830, [retval0+0];
	} // callseq 11
	ld.local.u32 	%r552, [%r1];
$L__BB0_95:
	add.s32 	%r553, %r552, 1;
$L__BB0_96:
	.loc	1 0 26
	setp.eq.s32 	%p272, %r406, 0;
	setp.eq.s32 	%p277, %r413, 0;
	setp.eq.s32 	%p282, %r420, 0;
	selp.f32 	%f24, %f8, %f16, %p74;
	// begin inline asm
	div.rn.f64 %rd80, %rd78, %rd82;
	// end inline asm
	cvt.f64.f32 	%fd7, %f23;
	mov.b64 	%fd272, %rd77;
	mul.f64 	%fd95, %fd271, %fd6;
	.loc	1 50 26
	and.b32  	%r427, %r553, 1;
	shl.b32 	%r428, %r427, 3;
	mul.wide.u32 	%rd96, %r428, 8;
	add.s64 	%rd98, %rd85, %rd96;
	setp.eq.s32 	%p287, %r427, 0;
	ld.global.nc.f64 	%fd383, [%rd98+8];
	ld.global.nc.f64 	%fd385, [%rd98+16];
	ld.global.nc.f64 	%fd387, [%rd98+24];
	ld.global.nc.f64 	%fd389, [%rd98+32];
	ld.global.nc.f64 	%fd391, [%rd98+40];
	ld.global.nc.f64 	%fd393, [%rd98+48];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r430, %temp}, %fd94;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r431}, %fd94;
	}
	and.b32  	%r432, %r431, 2147483647;
	setp.eq.s32 	%p289, %r432, 2146435072;
	setp.eq.s32 	%p290, %r430, 0;
	and.pred  	%p29, %p289, %p290;
	abs.f64 	%fd798, %fd94;
	mul.f64 	%fd799, %fd94, 0d3FE45F306DC9C883;
	@%p29 bra 	$L__BB0_141;
	bra.uni 	$L__BB0_97;
$L__BB0_141:
	mul.rn.f64 	%fd832, %fd94, %fd806;
	mov.b32 	%r555, 1;
	bra.uni 	$L__BB0_100;
$L__BB0_97:
	cvt.rni.s32.f64 	%r554, %fd799;
	st.local.u32 	[%r1], %r554;
	cvt.rn.f64.s32 	%fd404, %r554;
	neg.f64 	%fd405, %fd404;
	fma.rn.f64 	%fd407, %fd405, %fd787, %fd94;
	fma.rn.f64 	%fd409, %fd405, %fd788, %fd407;
	fma.rn.f64 	%fd832, %fd405, %fd789, %fd409;
	setp.ltu.f64 	%p291, %fd798, 0d41E0000000000000;
	@%p291 bra 	$L__BB0_99;
	{ // callseq 12, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd94;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd832, [retval0+0];
	} // callseq 12
	ld.local.u32 	%r554, [%r1];
$L__BB0_99:
	add.s32 	%r555, %r554, 1;
$L__BB0_100:
	.loc	1 0 26
	mul.rn.f64 	%fd285, %fd824, %fd824;
	selp.f64 	%fd286, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p272;
	mul.rn.f64 	%fd317, %fd826, %fd826;
	selp.f64 	%fd318, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p277;
	mul.rn.f64 	%fd349, %fd828, %fd828;
	selp.f64 	%fd350, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p282;
	mul.rn.f64 	%fd381, %fd830, %fd830;
	selp.f64 	%fd382, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p287;
	cvt.f64.f32 	%fd8, %f24;
	mov.b64 	%fd273, %rd80;
	mul.f64 	%fd96, %fd272, %fd7;
	.loc	1 50 26
	and.b32  	%r434, %r555, 1;
	shl.b32 	%r435, %r434, 3;
	mul.wide.u32 	%rd100, %r435, 8;
	add.s64 	%rd102, %rd85, %rd100;
	mul.rn.f64 	%fd413, %fd832, %fd832;
	setp.eq.s32 	%p292, %r434, 0;
	selp.f64 	%fd414, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p292;
	ld.global.nc.f64 	%fd415, [%rd102+8];
	ld.global.nc.f64 	%fd417, [%rd102+16];
	ld.global.nc.f64 	%fd419, [%rd102+24];
	ld.global.nc.f64 	%fd421, [%rd102+32];
	ld.global.nc.f64 	%fd423, [%rd102+40];
	ld.global.nc.f64 	%fd425, [%rd102+48];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r437, %temp}, %fd95;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r438}, %fd95;
	}
	and.b32  	%r439, %r438, 2147483647;
	setp.eq.s32 	%p294, %r439, 2146435072;
	setp.eq.s32 	%p295, %r437, 0;
	and.pred  	%p30, %p294, %p295;
	abs.f64 	%fd800, %fd95;
	mul.f64 	%fd801, %fd95, 0d3FE45F306DC9C883;
	@%p30 bra 	$L__BB0_142;
	bra.uni 	$L__BB0_101;
$L__BB0_142:
	mul.rn.f64 	%fd834, %fd95, %fd806;
	mov.b32 	%r557, 1;
	bra.uni 	$L__BB0_104;
$L__BB0_101:
	cvt.rni.s32.f64 	%r556, %fd801;
	st.local.u32 	[%r1], %r556;
	cvt.rn.f64.s32 	%fd436, %r556;
	neg.f64 	%fd437, %fd436;
	fma.rn.f64 	%fd439, %fd437, %fd787, %fd95;
	fma.rn.f64 	%fd441, %fd437, %fd788, %fd439;
	fma.rn.f64 	%fd834, %fd437, %fd789, %fd441;
	setp.ltu.f64 	%p296, %fd800, 0d41E0000000000000;
	@%p296 bra 	$L__BB0_103;
	{ // callseq 13, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd95;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd834, [retval0+0];
	} // callseq 13
	ld.local.u32 	%r556, [%r1];
$L__BB0_103:
	add.s32 	%r557, %r556, 1;
$L__BB0_104:
	.loc	1 0 26
	fma.rn.f64 	%fd288, %fd286, %fd285, %fd287;
	fma.rn.f64 	%fd320, %fd318, %fd317, %fd319;
	fma.rn.f64 	%fd352, %fd350, %fd349, %fd351;
	fma.rn.f64 	%fd384, %fd382, %fd381, %fd383;
	fma.rn.f64 	%fd416, %fd414, %fd413, %fd415;
	mul.f64 	%fd97, %fd273, %fd8;
	.loc	1 50 26
	and.b32  	%r441, %r557, 1;
	shl.b32 	%r442, %r441, 3;
	mul.wide.u32 	%rd104, %r442, 8;
	add.s64 	%rd106, %rd85, %rd104;
	mul.rn.f64 	%fd445, %fd834, %fd834;
	setp.eq.s32 	%p297, %r441, 0;
	selp.f64 	%fd446, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p297;
	ld.global.nc.f64 	%fd447, [%rd106+8];
	fma.rn.f64 	%fd448, %fd446, %fd445, %fd447;
	ld.global.nc.f64 	%fd449, [%rd106+16];
	ld.global.nc.f64 	%fd451, [%rd106+24];
	ld.global.nc.f64 	%fd453, [%rd106+32];
	ld.global.nc.f64 	%fd455, [%rd106+40];
	ld.global.nc.f64 	%fd457, [%rd106+48];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r444, %temp}, %fd96;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r445}, %fd96;
	}
	and.b32  	%r446, %r445, 2147483647;
	setp.eq.s32 	%p299, %r446, 2146435072;
	setp.eq.s32 	%p300, %r444, 0;
	and.pred  	%p31, %p299, %p300;
	abs.f64 	%fd802, %fd96;
	mul.f64 	%fd803, %fd96, 0d3FE45F306DC9C883;
	@%p31 bra 	$L__BB0_143;
	bra.uni 	$L__BB0_105;
$L__BB0_143:
	mul.rn.f64 	%fd836, %fd96, %fd806;
	mov.b32 	%r559, 1;
	bra.uni 	$L__BB0_108;
$L__BB0_105:
	cvt.rni.s32.f64 	%r558, %fd803;
	st.local.u32 	[%r1], %r558;
	cvt.rn.f64.s32 	%fd468, %r558;
	neg.f64 	%fd469, %fd468;
	fma.rn.f64 	%fd471, %fd469, %fd787, %fd96;
	fma.rn.f64 	%fd473, %fd469, %fd788, %fd471;
	fma.rn.f64 	%fd836, %fd469, %fd789, %fd473;
	setp.ltu.f64 	%p301, %fd802, 0d41E0000000000000;
	@%p301 bra 	$L__BB0_107;
	{ // callseq 14, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd96;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd836, [retval0+0];
	} // callseq 14
	ld.local.u32 	%r558, [%r1];
$L__BB0_107:
	add.s32 	%r559, %r558, 1;
$L__BB0_108:
	.loc	1 0 26
	fma.rn.f64 	%fd290, %fd288, %fd285, %fd289;
	fma.rn.f64 	%fd322, %fd320, %fd317, %fd321;
	fma.rn.f64 	%fd354, %fd352, %fd349, %fd353;
	fma.rn.f64 	%fd386, %fd384, %fd381, %fd385;
	fma.rn.f64 	%fd418, %fd416, %fd413, %fd417;
	.loc	1 50 26
	fma.rn.f64 	%fd450, %fd448, %fd445, %fd449;
	and.b32  	%r448, %r559, 1;
	shl.b32 	%r449, %r448, 3;
	mul.wide.u32 	%rd108, %r449, 8;
	add.s64 	%rd110, %rd85, %rd108;
	mul.rn.f64 	%fd477, %fd836, %fd836;
	setp.eq.s32 	%p302, %r448, 0;
	selp.f64 	%fd478, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p302;
	ld.global.nc.f64 	%fd479, [%rd110+8];
	fma.rn.f64 	%fd480, %fd478, %fd477, %fd479;
	ld.global.nc.f64 	%fd481, [%rd110+16];
	fma.rn.f64 	%fd482, %fd480, %fd477, %fd481;
	ld.global.nc.f64 	%fd483, [%rd110+24];
	ld.global.nc.f64 	%fd485, [%rd110+32];
	ld.global.nc.f64 	%fd487, [%rd110+40];
	ld.global.nc.f64 	%fd489, [%rd110+48];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r451, %temp}, %fd97;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r452}, %fd97;
	}
	and.b32  	%r453, %r452, 2147483647;
	setp.eq.s32 	%p304, %r453, 2146435072;
	setp.eq.s32 	%p305, %r451, 0;
	and.pred  	%p32, %p304, %p305;
	abs.f64 	%fd804, %fd97;
	mul.f64 	%fd805, %fd97, 0d3FE45F306DC9C883;
	@%p32 bra 	$L__BB0_144;
	bra.uni 	$L__BB0_109;
$L__BB0_144:
	.loc	1 0 26
	mov.b32 	%r561, 1;
	.loc	1 50 26
	mul.rn.f64 	%fd838, %fd97, %fd806;
	bra.uni 	$L__BB0_112;
$L__BB0_109:
	cvt.rni.s32.f64 	%r560, %fd805;
	st.local.u32 	[%r1], %r560;
	cvt.rn.f64.s32 	%fd500, %r560;
	neg.f64 	%fd501, %fd500;
	fma.rn.f64 	%fd503, %fd501, %fd787, %fd97;
	fma.rn.f64 	%fd505, %fd501, %fd788, %fd503;
	fma.rn.f64 	%fd838, %fd501, %fd789, %fd505;
	setp.ltu.f64 	%p306, %fd804, 0d41E0000000000000;
	@%p306 bra 	$L__BB0_111;
	{ // callseq 15, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd97;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd838, [retval0+0];
	} // callseq 15
	ld.local.u32 	%r560, [%r1];
$L__BB0_111:
	add.s32 	%r561, %r560, 1;
$L__BB0_112:
	.loc	1 0 26
	fma.rn.f64 	%fd292, %fd290, %fd285, %fd291;
	fma.rn.f64 	%fd324, %fd322, %fd317, %fd323;
	fma.rn.f64 	%fd356, %fd354, %fd349, %fd355;
	fma.rn.f64 	%fd388, %fd386, %fd381, %fd387;
	fma.rn.f64 	%fd420, %fd418, %fd413, %fd419;
	.loc	1 50 26
	fma.rn.f64 	%fd452, %fd450, %fd445, %fd451;
	fma.rn.f64 	%fd484, %fd482, %fd477, %fd483;
	and.b32  	%r455, %r561, 1;
	shl.b32 	%r456, %r455, 3;
	mul.wide.u32 	%rd112, %r456, 8;
	add.s64 	%rd114, %rd85, %rd112;
	mul.rn.f64 	%fd509, %fd838, %fd838;
	setp.eq.s32 	%p307, %r455, 0;
	selp.f64 	%fd510, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p307;
	ld.global.nc.f64 	%fd511, [%rd114+8];
	fma.rn.f64 	%fd512, %fd510, %fd509, %fd511;
	ld.global.nc.f64 	%fd513, [%rd114+16];
	fma.rn.f64 	%fd514, %fd512, %fd509, %fd513;
	ld.global.nc.f64 	%fd515, [%rd114+24];
	fma.rn.f64 	%fd516, %fd514, %fd509, %fd515;
	ld.global.nc.f64 	%fd517, [%rd114+32];
	ld.global.nc.f64 	%fd519, [%rd114+40];
	ld.global.nc.f64 	%fd521, [%rd114+48];
	.loc	1 52 26
	@%p25 bra 	$L__BB0_145;
	bra.uni 	$L__BB0_113;
$L__BB0_145:
	mul.rn.f64 	%fd839, %fd90, %fd806;
	mov.b32 	%r562, 0;
	bra.uni 	$L__BB0_115;
$L__BB0_113:
	cvt.rni.s32.f64 	%r562, %fd791;
	st.local.u32 	[%r1], %r562;
	cvt.rn.f64.s32 	%fd532, %r562;
	neg.f64 	%fd533, %fd532;
	fma.rn.f64 	%fd535, %fd533, %fd787, %fd90;
	fma.rn.f64 	%fd537, %fd533, %fd788, %fd535;
	fma.rn.f64 	%fd839, %fd533, %fd789, %fd537;
	setp.ltu.f64 	%p309, %fd790, 0d41E0000000000000;
	@%p309 bra 	$L__BB0_115;
	{ // callseq 16, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd90;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd839, [retval0+0];
	} // callseq 16
	ld.local.u32 	%r562, [%r1];
$L__BB0_115:
	.loc	1 0 26
	fma.rn.f64 	%fd294, %fd292, %fd285, %fd293;
	fma.rn.f64 	%fd326, %fd324, %fd317, %fd325;
	fma.rn.f64 	%fd358, %fd356, %fd349, %fd357;
	fma.rn.f64 	%fd390, %fd388, %fd381, %fd389;
	fma.rn.f64 	%fd422, %fd420, %fd413, %fd421;
	fma.rn.f64 	%fd454, %fd452, %fd445, %fd453;
	fma.rn.f64 	%fd486, %fd484, %fd477, %fd485;
	fma.rn.f64 	%fd518, %fd516, %fd509, %fd517;
	.loc	1 52 26
	and.b32  	%r459, %r562, 1;
	shl.b32 	%r460, %r459, 3;
	mul.wide.u32 	%rd116, %r460, 8;
	add.s64 	%rd118, %rd85, %rd116;
	mul.rn.f64 	%fd541, %fd839, %fd839;
	setp.eq.s32 	%p310, %r459, 0;
	selp.f64 	%fd542, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p310;
	ld.global.nc.f64 	%fd543, [%rd118+8];
	fma.rn.f64 	%fd544, %fd542, %fd541, %fd543;
	ld.global.nc.f64 	%fd545, [%rd118+16];
	fma.rn.f64 	%fd546, %fd544, %fd541, %fd545;
	ld.global.nc.f64 	%fd547, [%rd118+24];
	fma.rn.f64 	%fd548, %fd546, %fd541, %fd547;
	ld.global.nc.f64 	%fd549, [%rd118+32];
	fma.rn.f64 	%fd550, %fd548, %fd541, %fd549;
	ld.global.nc.f64 	%fd551, [%rd118+40];
	ld.global.nc.f64 	%fd553, [%rd118+48];
	@%p26 bra 	$L__BB0_146;
	bra.uni 	$L__BB0_116;
$L__BB0_146:
	mul.rn.f64 	%fd840, %fd91, %fd806;
	mov.b32 	%r563, 0;
	bra.uni 	$L__BB0_118;
$L__BB0_116:
	cvt.rni.s32.f64 	%r563, %fd793;
	st.local.u32 	[%r1], %r563;
	cvt.rn.f64.s32 	%fd564, %r563;
	neg.f64 	%fd565, %fd564;
	fma.rn.f64 	%fd567, %fd565, %fd787, %fd91;
	fma.rn.f64 	%fd569, %fd565, %fd788, %fd567;
	fma.rn.f64 	%fd840, %fd565, %fd789, %fd569;
	setp.ltu.f64 	%p312, %fd792, 0d41E0000000000000;
	@%p312 bra 	$L__BB0_118;
	{ // callseq 17, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd91;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd840, [retval0+0];
	} // callseq 17
	ld.local.u32 	%r563, [%r1];
$L__BB0_118:
	.loc	1 0 26
	fma.rn.f64 	%fd296, %fd294, %fd285, %fd295;
	fma.rn.f64 	%fd328, %fd326, %fd317, %fd327;
	fma.rn.f64 	%fd360, %fd358, %fd349, %fd359;
	fma.rn.f64 	%fd392, %fd390, %fd381, %fd391;
	fma.rn.f64 	%fd424, %fd422, %fd413, %fd423;
	fma.rn.f64 	%fd456, %fd454, %fd445, %fd455;
	fma.rn.f64 	%fd488, %fd486, %fd477, %fd487;
	fma.rn.f64 	%fd520, %fd518, %fd509, %fd519;
	.loc	1 52 26
	fma.rn.f64 	%fd552, %fd550, %fd541, %fd551;
	and.b32  	%r463, %r563, 1;
	shl.b32 	%r464, %r463, 3;
	mul.wide.u32 	%rd120, %r464, 8;
	add.s64 	%rd122, %rd85, %rd120;
	mul.rn.f64 	%fd573, %fd840, %fd840;
	setp.eq.s32 	%p313, %r463, 0;
	selp.f64 	%fd574, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p313;
	ld.global.nc.f64 	%fd575, [%rd122+8];
	fma.rn.f64 	%fd576, %fd574, %fd573, %fd575;
	ld.global.nc.f64 	%fd577, [%rd122+16];
	fma.rn.f64 	%fd578, %fd576, %fd573, %fd577;
	ld.global.nc.f64 	%fd579, [%rd122+24];
	fma.rn.f64 	%fd580, %fd578, %fd573, %fd579;
	ld.global.nc.f64 	%fd581, [%rd122+32];
	fma.rn.f64 	%fd582, %fd580, %fd573, %fd581;
	ld.global.nc.f64 	%fd583, [%rd122+40];
	fma.rn.f64 	%fd584, %fd582, %fd573, %fd583;
	ld.global.nc.f64 	%fd585, [%rd122+48];
	@%p27 bra 	$L__BB0_147;
	bra.uni 	$L__BB0_119;
$L__BB0_147:
	mul.rn.f64 	%fd841, %fd92, %fd806;
	mov.b32 	%r564, 0;
	bra.uni 	$L__BB0_121;
$L__BB0_119:
	cvt.rni.s32.f64 	%r564, %fd795;
	st.local.u32 	[%r1], %r564;
	cvt.rn.f64.s32 	%fd596, %r564;
	neg.f64 	%fd597, %fd596;
	fma.rn.f64 	%fd599, %fd597, %fd787, %fd92;
	fma.rn.f64 	%fd601, %fd597, %fd788, %fd599;
	fma.rn.f64 	%fd841, %fd597, %fd789, %fd601;
	setp.ltu.f64 	%p315, %fd794, 0d41E0000000000000;
	@%p315 bra 	$L__BB0_121;
	{ // callseq 18, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd92;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd841, [retval0+0];
	} // callseq 18
	ld.local.u32 	%r564, [%r1];
$L__BB0_121:
	.loc	1 0 26
	fma.rn.f64 	%fd298, %fd296, %fd285, %fd297;
	mov.f64 	%fd300, 0d3FF0000000000000;
	fma.rn.f64 	%fd330, %fd328, %fd317, %fd329;
	fma.rn.f64 	%fd362, %fd360, %fd349, %fd361;
	fma.rn.f64 	%fd394, %fd392, %fd381, %fd393;
	fma.rn.f64 	%fd426, %fd424, %fd413, %fd425;
	fma.rn.f64 	%fd458, %fd456, %fd445, %fd457;
	fma.rn.f64 	%fd490, %fd488, %fd477, %fd489;
	fma.rn.f64 	%fd522, %fd520, %fd509, %fd521;
	.loc	1 52 26
	fma.rn.f64 	%fd554, %fd552, %fd541, %fd553;
	fma.rn.f64 	%fd586, %fd584, %fd573, %fd585;
	and.b32  	%r467, %r564, 1;
	shl.b32 	%r468, %r467, 3;
	mul.wide.u32 	%rd124, %r468, 8;
	add.s64 	%rd126, %rd85, %rd124;
	mul.rn.f64 	%fd605, %fd841, %fd841;
	setp.eq.s32 	%p316, %r467, 0;
	selp.f64 	%fd606, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p316;
	ld.global.nc.f64 	%fd607, [%rd126+8];
	fma.rn.f64 	%fd608, %fd606, %fd605, %fd607;
	ld.global.nc.f64 	%fd609, [%rd126+16];
	fma.rn.f64 	%fd610, %fd608, %fd605, %fd609;
	ld.global.nc.f64 	%fd611, [%rd126+24];
	fma.rn.f64 	%fd612, %fd610, %fd605, %fd611;
	ld.global.nc.f64 	%fd613, [%rd126+32];
	fma.rn.f64 	%fd614, %fd612, %fd605, %fd613;
	ld.global.nc.f64 	%fd615, [%rd126+40];
	fma.rn.f64 	%fd616, %fd614, %fd605, %fd615;
	ld.global.nc.f64 	%fd617, [%rd126+48];
	fma.rn.f64 	%fd618, %fd616, %fd605, %fd617;
	@%p28 bra 	$L__BB0_148;
	bra.uni 	$L__BB0_122;
$L__BB0_148:
	mul.rn.f64 	%fd842, %fd93, %fd806;
	mov.b32 	%r565, 0;
	bra.uni 	$L__BB0_124;
$L__BB0_122:
	cvt.rni.s32.f64 	%r565, %fd797;
	st.local.u32 	[%r1], %r565;
	cvt.rn.f64.s32 	%fd628, %r565;
	neg.f64 	%fd629, %fd628;
	fma.rn.f64 	%fd631, %fd629, %fd787, %fd93;
	fma.rn.f64 	%fd633, %fd629, %fd788, %fd631;
	fma.rn.f64 	%fd842, %fd629, %fd789, %fd633;
	setp.ltu.f64 	%p318, %fd796, 0d41E0000000000000;
	@%p318 bra 	$L__BB0_124;
	{ // callseq 19, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd93;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd842, [retval0+0];
	} // callseq 19
	ld.local.u32 	%r565, [%r1];
$L__BB0_124:
	.loc	1 0 26
	fma.rn.f64 	%fd299, %fd298, %fd824, %fd824;
	fma.rn.f64 	%fd301, %fd298, %fd285, %fd300;
	fma.rn.f64 	%fd331, %fd330, %fd826, %fd826;
	fma.rn.f64 	%fd333, %fd330, %fd317, %fd300;
	fma.rn.f64 	%fd363, %fd362, %fd828, %fd828;
	fma.rn.f64 	%fd365, %fd362, %fd349, %fd300;
	fma.rn.f64 	%fd395, %fd394, %fd830, %fd830;
	fma.rn.f64 	%fd397, %fd394, %fd381, %fd300;
	fma.rn.f64 	%fd427, %fd426, %fd832, %fd832;
	fma.rn.f64 	%fd429, %fd426, %fd413, %fd300;
	fma.rn.f64 	%fd459, %fd458, %fd834, %fd834;
	fma.rn.f64 	%fd461, %fd458, %fd445, %fd300;
	fma.rn.f64 	%fd491, %fd490, %fd836, %fd836;
	fma.rn.f64 	%fd493, %fd490, %fd477, %fd300;
	fma.rn.f64 	%fd523, %fd522, %fd838, %fd838;
	fma.rn.f64 	%fd525, %fd522, %fd509, %fd300;
	.loc	1 52 26
	fma.rn.f64 	%fd555, %fd554, %fd839, %fd839;
	fma.rn.f64 	%fd557, %fd554, %fd541, %fd300;
	fma.rn.f64 	%fd587, %fd586, %fd840, %fd840;
	fma.rn.f64 	%fd589, %fd586, %fd573, %fd300;
	fma.rn.f64 	%fd619, %fd618, %fd841, %fd841;
	fma.rn.f64 	%fd621, %fd618, %fd605, %fd300;
	and.b32  	%r471, %r565, 1;
	shl.b32 	%r472, %r471, 3;
	mul.wide.u32 	%rd128, %r472, 8;
	add.s64 	%rd130, %rd85, %rd128;
	mul.rn.f64 	%fd637, %fd842, %fd842;
	setp.eq.s32 	%p319, %r471, 0;
	selp.f64 	%fd638, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p319;
	ld.global.nc.f64 	%fd639, [%rd130+8];
	fma.rn.f64 	%fd640, %fd638, %fd637, %fd639;
	ld.global.nc.f64 	%fd641, [%rd130+16];
	fma.rn.f64 	%fd642, %fd640, %fd637, %fd641;
	ld.global.nc.f64 	%fd643, [%rd130+24];
	fma.rn.f64 	%fd644, %fd642, %fd637, %fd643;
	ld.global.nc.f64 	%fd645, [%rd130+32];
	fma.rn.f64 	%fd646, %fd644, %fd637, %fd645;
	ld.global.nc.f64 	%fd647, [%rd130+40];
	fma.rn.f64 	%fd648, %fd646, %fd637, %fd647;
	ld.global.nc.f64 	%fd649, [%rd130+48];
	fma.rn.f64 	%fd650, %fd648, %fd637, %fd649;
	fma.rn.f64 	%fd651, %fd650, %fd842, %fd842;
	fma.rn.f64 	%fd653, %fd650, %fd637, %fd300;
	@%p29 bra 	$L__BB0_149;
	bra.uni 	$L__BB0_125;
$L__BB0_149:
	mul.rn.f64 	%fd843, %fd94, %fd806;
	mov.b32 	%r566, 0;
	bra.uni 	$L__BB0_127;
$L__BB0_125:
	cvt.rni.s32.f64 	%r566, %fd799;
	st.local.u32 	[%r1], %r566;
	cvt.rn.f64.s32 	%fd660, %r566;
	neg.f64 	%fd661, %fd660;
	fma.rn.f64 	%fd663, %fd661, %fd787, %fd94;
	fma.rn.f64 	%fd665, %fd661, %fd788, %fd663;
	fma.rn.f64 	%fd843, %fd661, %fd789, %fd665;
	setp.ltu.f64 	%p321, %fd798, 0d41E0000000000000;
	@%p321 bra 	$L__BB0_127;
	{ // callseq 20, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd94;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd843, [retval0+0];
	} // callseq 20
	ld.local.u32 	%r566, [%r1];
$L__BB0_127:
	.loc	1 0 26
	selp.f64 	%fd302, %fd299, %fd301, %p272;
	and.b32  	%r408, %r547, 2;
	mov.f64 	%fd304, 0dBFF0000000000000;
	selp.f64 	%fd334, %fd331, %fd333, %p277;
	and.b32  	%r415, %r549, 2;
	selp.f64 	%fd366, %fd363, %fd365, %p282;
	and.b32  	%r422, %r551, 2;
	selp.f64 	%fd398, %fd395, %fd397, %p287;
	and.b32  	%r429, %r553, 2;
	selp.f64 	%fd430, %fd427, %fd429, %p292;
	and.b32  	%r436, %r555, 2;
	selp.f64 	%fd462, %fd459, %fd461, %p297;
	and.b32  	%r443, %r557, 2;
	selp.f64 	%fd494, %fd491, %fd493, %p302;
	and.b32  	%r450, %r559, 2;
	selp.f64 	%fd526, %fd523, %fd525, %p307;
	and.b32  	%r457, %r561, 2;
	.loc	1 52 26
	selp.f64 	%fd558, %fd555, %fd557, %p310;
	and.b32  	%r461, %r562, 2;
	selp.f64 	%fd590, %fd587, %fd589, %p313;
	and.b32  	%r465, %r563, 2;
	selp.f64 	%fd622, %fd619, %fd621, %p316;
	and.b32  	%r469, %r564, 2;
	selp.f64 	%fd654, %fd651, %fd653, %p319;
	and.b32  	%r473, %r565, 2;
	and.b32  	%r475, %r566, 1;
	shl.b32 	%r476, %r475, 3;
	mul.wide.u32 	%rd132, %r476, 8;
	add.s64 	%rd134, %rd85, %rd132;
	mul.rn.f64 	%fd669, %fd843, %fd843;
	setp.eq.s32 	%p322, %r475, 0;
	selp.f64 	%fd670, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p322;
	ld.global.nc.f64 	%fd671, [%rd134+8];
	fma.rn.f64 	%fd672, %fd670, %fd669, %fd671;
	ld.global.nc.f64 	%fd673, [%rd134+16];
	fma.rn.f64 	%fd674, %fd672, %fd669, %fd673;
	ld.global.nc.f64 	%fd675, [%rd134+24];
	fma.rn.f64 	%fd676, %fd674, %fd669, %fd675;
	ld.global.nc.f64 	%fd677, [%rd134+32];
	fma.rn.f64 	%fd678, %fd676, %fd669, %fd677;
	ld.global.nc.f64 	%fd679, [%rd134+40];
	fma.rn.f64 	%fd680, %fd678, %fd669, %fd679;
	ld.global.nc.f64 	%fd681, [%rd134+48];
	fma.rn.f64 	%fd682, %fd680, %fd669, %fd681;
	fma.rn.f64 	%fd683, %fd682, %fd843, %fd843;
	fma.rn.f64 	%fd685, %fd682, %fd669, %fd300;
	selp.f64 	%fd686, %fd683, %fd685, %p322;
	and.b32  	%r477, %r566, 2;
	@%p30 bra 	$L__BB0_150;
	bra.uni 	$L__BB0_128;
$L__BB0_150:
	mul.rn.f64 	%fd844, %fd95, %fd806;
	mov.b32 	%r567, 0;
	bra.uni 	$L__BB0_130;
$L__BB0_128:
	cvt.rni.s32.f64 	%r567, %fd801;
	st.local.u32 	[%r1], %r567;
	cvt.rn.f64.s32 	%fd692, %r567;
	neg.f64 	%fd693, %fd692;
	fma.rn.f64 	%fd695, %fd693, %fd787, %fd95;
	fma.rn.f64 	%fd697, %fd693, %fd788, %fd695;
	fma.rn.f64 	%fd844, %fd693, %fd789, %fd697;
	setp.ltu.f64 	%p324, %fd800, 0d41E0000000000000;
	@%p324 bra 	$L__BB0_130;
	{ // callseq 21, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd95;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd844, [retval0+0];
	} // callseq 21
	ld.local.u32 	%r567, [%r1];
$L__BB0_130:
	.loc	1 0 26
	setp.eq.s32 	%p273, %r408, 0;
	fma.rn.f64 	%fd305, %fd302, %fd304, %fd806;
	setp.eq.s32 	%p278, %r415, 0;
	fma.rn.f64 	%fd337, %fd334, %fd304, %fd806;
	setp.eq.s32 	%p283, %r422, 0;
	fma.rn.f64 	%fd369, %fd366, %fd304, %fd806;
	setp.eq.s32 	%p288, %r429, 0;
	fma.rn.f64 	%fd401, %fd398, %fd304, %fd806;
	setp.eq.s32 	%p293, %r436, 0;
	fma.rn.f64 	%fd433, %fd430, %fd304, %fd806;
	setp.eq.s32 	%p298, %r443, 0;
	fma.rn.f64 	%fd465, %fd462, %fd304, %fd806;
	setp.eq.s32 	%p303, %r450, 0;
	fma.rn.f64 	%fd497, %fd494, %fd304, %fd806;
	setp.eq.s32 	%p308, %r457, 0;
	fma.rn.f64 	%fd529, %fd526, %fd304, %fd806;
	.loc	1 52 26
	setp.eq.s32 	%p311, %r461, 0;
	fma.rn.f64 	%fd561, %fd558, %fd304, %fd806;
	setp.eq.s32 	%p314, %r465, 0;
	fma.rn.f64 	%fd593, %fd590, %fd304, %fd806;
	setp.eq.s32 	%p317, %r469, 0;
	fma.rn.f64 	%fd625, %fd622, %fd304, %fd806;
	setp.eq.s32 	%p320, %r473, 0;
	fma.rn.f64 	%fd657, %fd654, %fd304, %fd806;
	setp.eq.s32 	%p323, %r477, 0;
	fma.rn.f64 	%fd689, %fd686, %fd304, %fd806;
	and.b32  	%r479, %r567, 1;
	shl.b32 	%r480, %r479, 3;
	mul.wide.u32 	%rd136, %r480, 8;
	add.s64 	%rd138, %rd85, %rd136;
	mul.rn.f64 	%fd701, %fd844, %fd844;
	setp.eq.s32 	%p325, %r479, 0;
	selp.f64 	%fd702, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p325;
	ld.global.nc.f64 	%fd703, [%rd138+8];
	fma.rn.f64 	%fd704, %fd702, %fd701, %fd703;
	ld.global.nc.f64 	%fd705, [%rd138+16];
	fma.rn.f64 	%fd706, %fd704, %fd701, %fd705;
	ld.global.nc.f64 	%fd707, [%rd138+24];
	fma.rn.f64 	%fd708, %fd706, %fd701, %fd707;
	ld.global.nc.f64 	%fd709, [%rd138+32];
	fma.rn.f64 	%fd710, %fd708, %fd701, %fd709;
	ld.global.nc.f64 	%fd711, [%rd138+40];
	fma.rn.f64 	%fd712, %fd710, %fd701, %fd711;
	ld.global.nc.f64 	%fd713, [%rd138+48];
	fma.rn.f64 	%fd714, %fd712, %fd701, %fd713;
	fma.rn.f64 	%fd715, %fd714, %fd844, %fd844;
	fma.rn.f64 	%fd717, %fd714, %fd701, %fd300;
	selp.f64 	%fd718, %fd715, %fd717, %p325;
	and.b32  	%r481, %r567, 2;
	setp.eq.s32 	%p326, %r481, 0;
	fma.rn.f64 	%fd721, %fd718, %fd304, %fd806;
	@%p31 bra 	$L__BB0_151;
	bra.uni 	$L__BB0_131;
$L__BB0_151:
	mul.rn.f64 	%fd845, %fd96, %fd806;
	mov.b32 	%r568, 0;
	bra.uni 	$L__BB0_133;
$L__BB0_131:
	cvt.rni.s32.f64 	%r568, %fd803;
	st.local.u32 	[%r1], %r568;
	cvt.rn.f64.s32 	%fd724, %r568;
	neg.f64 	%fd725, %fd724;
	fma.rn.f64 	%fd727, %fd725, %fd787, %fd96;
	fma.rn.f64 	%fd729, %fd725, %fd788, %fd727;
	fma.rn.f64 	%fd845, %fd725, %fd789, %fd729;
	setp.ltu.f64 	%p327, %fd802, 0d41E0000000000000;
	@%p327 bra 	$L__BB0_133;
	{ // callseq 22, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd96;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd845, [retval0+0];
	} // callseq 22
	ld.local.u32 	%r568, [%r1];
$L__BB0_133:
	.loc	1 0 26
	ld.param.u64 	%rd10, [triton__param_3];
	ld.param.u64 	%rd9, [triton__param_2];
	selp.f64 	%fd103, %fd302, %fd305, %p273;
	selp.f64 	%fd109, %fd334, %fd337, %p278;
	selp.f64 	%fd115, %fd366, %fd369, %p283;
	selp.f64 	%fd121, %fd398, %fd401, %p288;
	selp.f64 	%fd127, %fd430, %fd433, %p293;
	selp.f64 	%fd133, %fd462, %fd465, %p298;
	selp.f64 	%fd139, %fd494, %fd497, %p303;
	selp.f64 	%fd145, %fd526, %fd529, %p308;
	.loc	1 52 26
	selp.f64 	%fd150, %fd558, %fd561, %p311;
	selp.f64 	%fd155, %fd590, %fd593, %p314;
	selp.f64 	%fd160, %fd622, %fd625, %p317;
	selp.f64 	%fd165, %fd654, %fd657, %p320;
	selp.f64 	%fd170, %fd686, %fd689, %p323;
	selp.f64 	%fd175, %fd718, %fd721, %p326;
	and.b32  	%r483, %r568, 1;
	shl.b32 	%r484, %r483, 3;
	mul.wide.u32 	%rd140, %r484, 8;
	add.s64 	%rd142, %rd85, %rd140;
	mul.rn.f64 	%fd733, %fd845, %fd845;
	setp.eq.s32 	%p328, %r483, 0;
	selp.f64 	%fd734, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p328;
	ld.global.nc.f64 	%fd735, [%rd142+8];
	fma.rn.f64 	%fd736, %fd734, %fd733, %fd735;
	ld.global.nc.f64 	%fd737, [%rd142+16];
	fma.rn.f64 	%fd738, %fd736, %fd733, %fd737;
	ld.global.nc.f64 	%fd739, [%rd142+24];
	fma.rn.f64 	%fd740, %fd738, %fd733, %fd739;
	ld.global.nc.f64 	%fd741, [%rd142+32];
	fma.rn.f64 	%fd742, %fd740, %fd733, %fd741;
	ld.global.nc.f64 	%fd743, [%rd142+40];
	fma.rn.f64 	%fd744, %fd742, %fd733, %fd743;
	ld.global.nc.f64 	%fd745, [%rd142+48];
	fma.rn.f64 	%fd746, %fd744, %fd733, %fd745;
	fma.rn.f64 	%fd747, %fd746, %fd845, %fd845;
	fma.rn.f64 	%fd749, %fd746, %fd733, %fd300;
	selp.f64 	%fd750, %fd747, %fd749, %p328;
	and.b32  	%r485, %r568, 2;
	setp.eq.s32 	%p329, %r485, 0;
	fma.rn.f64 	%fd753, %fd750, %fd304, %fd806;
	selp.f64 	%fd180, %fd750, %fd753, %p329;
	@%p32 bra 	$L__BB0_152;
	bra.uni 	$L__BB0_134;
$L__BB0_152:
	.loc	1 0 26
	mov.b32 	%r569, 0;
	.loc	1 52 26
	mul.rn.f64 	%fd846, %fd97, %fd806;
	bra.uni 	$L__BB0_136;
$L__BB0_134:
	cvt.rni.s32.f64 	%r569, %fd805;
	st.local.u32 	[%r1], %r569;
	cvt.rn.f64.s32 	%fd756, %r569;
	neg.f64 	%fd757, %fd756;
	fma.rn.f64 	%fd759, %fd757, %fd787, %fd97;
	fma.rn.f64 	%fd761, %fd757, %fd788, %fd759;
	fma.rn.f64 	%fd846, %fd757, %fd789, %fd761;
	setp.ltu.f64 	%p330, %fd804, 0d41E0000000000000;
	@%p330 bra 	$L__BB0_136;
	{ // callseq 23, 0
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd97;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd29;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd846, [retval0+0];
	} // callseq 23
	ld.local.u32 	%r569, [%r1];
$L__BB0_136:
	and.b32  	%r503, %r569, 1;
	shl.b32 	%r504, %r503, 3;
	mul.wide.u32 	%rd160, %r504, 8;
	add.s64 	%rd162, %rd85, %rd160;
	mul.rn.f64 	%fd765, %fd846, %fd846;
	setp.eq.s32 	%p347, %r503, 0;
	selp.f64 	%fd766, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p347;
	ld.global.nc.f64 	%fd767, [%rd162+8];
	fma.rn.f64 	%fd768, %fd766, %fd765, %fd767;
	ld.global.nc.f64 	%fd769, [%rd162+16];
	fma.rn.f64 	%fd770, %fd768, %fd765, %fd769;
	ld.global.nc.f64 	%fd771, [%rd162+24];
	fma.rn.f64 	%fd772, %fd770, %fd765, %fd771;
	ld.global.nc.f64 	%fd773, [%rd162+32];
	fma.rn.f64 	%fd774, %fd772, %fd765, %fd773;
	ld.global.nc.f64 	%fd775, [%rd162+40];
	fma.rn.f64 	%fd776, %fd774, %fd765, %fd775;
	ld.global.nc.f64 	%fd777, [%rd162+48];
	fma.rn.f64 	%fd778, %fd776, %fd765, %fd777;
	fma.rn.f64 	%fd779, %fd778, %fd846, %fd846;
	fma.rn.f64 	%fd781, %fd778, %fd765, %fd300;
	selp.f64 	%fd782, %fd779, %fd781, %p347;
	and.b32  	%r505, %r569, 2;
	setp.eq.s32 	%p348, %r505, 0;
	fma.rn.f64 	%fd785, %fd782, %fd304, %fd806;
	selp.f64 	%fd786, %fd782, %fd785, %p348;
	.loc	1 51 21
	cvt.rn.f32.f64 	%f25, %fd145;
	cvt.rn.f32.f64 	%f26, %fd139;
	cvt.rn.f32.f64 	%f27, %fd133;
	cvt.rn.f32.f64 	%f28, %fd127;
	cvt.rn.f32.f64 	%f29, %fd121;
	cvt.rn.f32.f64 	%f30, %fd115;
	cvt.rn.f32.f64 	%f31, %fd109;
	cvt.rn.f32.f64 	%f32, %fd103;
	.loc	1 53 21
	cvt.rn.f32.f64 	%f33, %fd150;
	cvt.rn.f32.f64 	%f34, %fd155;
	cvt.rn.f32.f64 	%f35, %fd160;
	cvt.rn.f32.f64 	%f36, %fd165;
	cvt.rn.f32.f64 	%f37, %fd170;
	cvt.rn.f32.f64 	%f38, %fd175;
	cvt.rn.f32.f64 	%f39, %fd180;
	cvt.rn.f32.f64 	%f40, %fd786;
	.loc	1 54 35
	shl.b32 	%r506, %r25, 7;
	shl.b32 	%r507, %r26, 7;
	.loc	1 54 31
	add.s32 	%r508, %r506, %r27;
	add.s32 	%r509, %r506, %r28;
	add.s32 	%r510, %r506, %r29;
	add.s32 	%r511, %r506, %r30;
	add.s32 	%r512, %r507, %r31;
	add.s32 	%r513, %r507, %r32;
	add.s32 	%r514, %r507, %r33;
	add.s32 	%r515, %r507, %r34;
	.loc	1 54 25
	mul.wide.s32 	%rd163, %r508, 4;
	add.s64 	%rd144, %rd9, %rd163;
	mul.wide.s32 	%rd164, %r509, 4;
	add.s64 	%rd145, %rd9, %rd164;
	mul.wide.s32 	%rd165, %r510, 4;
	add.s64 	%rd146, %rd9, %rd165;
	mul.wide.s32 	%rd166, %r511, 4;
	add.s64 	%rd147, %rd9, %rd166;
	mul.wide.s32 	%rd167, %r512, 4;
	add.s64 	%rd148, %rd9, %rd167;
	mul.wide.s32 	%rd168, %r513, 4;
	add.s64 	%rd149, %rd9, %rd168;
	mul.wide.s32 	%rd169, %r514, 4;
	add.s64 	%rd150, %rd9, %rd169;
	mul.wide.s32 	%rd170, %r515, 4;
	add.s64 	%rd151, %rd9, %rd170;
	.loc	1 54 48
	mov.b32 	%r487, %f32;
	// begin inline asm
	@%p331 st.global.b32 [ %rd144 + 0 ], { %r487 };
	// end inline asm
	mov.b32 	%r488, %f31;
	// begin inline asm
	@%p332 st.global.b32 [ %rd145 + 0 ], { %r488 };
	// end inline asm
	mov.b32 	%r489, %f30;
	// begin inline asm
	@%p333 st.global.b32 [ %rd146 + 0 ], { %r489 };
	// end inline asm
	mov.b32 	%r490, %f29;
	// begin inline asm
	@%p334 st.global.b32 [ %rd147 + 0 ], { %r490 };
	// end inline asm
	mov.b32 	%r491, %f28;
	// begin inline asm
	@%p335 st.global.b32 [ %rd148 + 0 ], { %r491 };
	// end inline asm
	mov.b32 	%r492, %f27;
	// begin inline asm
	@%p336 st.global.b32 [ %rd149 + 0 ], { %r492 };
	// end inline asm
	mov.b32 	%r493, %f26;
	// begin inline asm
	@%p337 st.global.b32 [ %rd150 + 0 ], { %r493 };
	// end inline asm
	mov.b32 	%r494, %f25;
	// begin inline asm
	@%p338 st.global.b32 [ %rd151 + 0 ], { %r494 };
	// end inline asm
	.loc	1 55 25
	add.s64 	%rd152, %rd10, %rd163;
	add.s64 	%rd153, %rd10, %rd164;
	add.s64 	%rd154, %rd10, %rd165;
	add.s64 	%rd155, %rd10, %rd166;
	add.s64 	%rd156, %rd10, %rd167;
	add.s64 	%rd157, %rd10, %rd168;
	add.s64 	%rd158, %rd10, %rd169;
	add.s64 	%rd159, %rd10, %rd170;
	.loc	1 55 48
	mov.b32 	%r495, %f33;
	// begin inline asm
	@%p331 st.global.b32 [ %rd152 + 0 ], { %r495 };
	// end inline asm
	mov.b32 	%r496, %f34;
	// begin inline asm
	@%p332 st.global.b32 [ %rd153 + 0 ], { %r496 };
	// end inline asm
	mov.b32 	%r497, %f35;
	// begin inline asm
	@%p333 st.global.b32 [ %rd154 + 0 ], { %r497 };
	// end inline asm
	mov.b32 	%r498, %f36;
	// begin inline asm
	@%p334 st.global.b32 [ %rd155 + 0 ], { %r498 };
	// end inline asm
	mov.b32 	%r499, %f37;
	// begin inline asm
	@%p335 st.global.b32 [ %rd156 + 0 ], { %r499 };
	// end inline asm
	mov.b32 	%r500, %f38;
	// begin inline asm
	@%p336 st.global.b32 [ %rd157 + 0 ], { %r500 };
	// end inline asm
	mov.b32 	%r501, %f39;
	// begin inline asm
	@%p337 st.global.b32 [ %rd158 + 0 ], { %r501 };
	// end inline asm
	mov.b32 	%r502, %f40;
	// begin inline asm
	@%p338 st.global.b32 [ %rd159 + 0 ], { %r502 };
	// end inline asm
	.loc	1 55 4
	ret;
$L__tmp1:
$L__func_end0:

}
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot1[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b32 	%r<48>;
	.reg .b64 	%rd<86>;
	.reg .f64 	%fd<5>;
$L__func_begin1:

	mov.u64 	%SPL, __local_depot1;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd4;
	}
	bfe.u32 	%r4, %r22, 20, 11;
	setp.eq.s32 	%p1, %r4, 2047;
	@%p1 bra 	$L__BB1_13;
	add.u64 	%rd27, %SP, 0;
	{ .reg .b64 %tmp;
	  cvta.to.local.u64 	%tmp, %rd27;
	  cvt.u32.u64 	%r1, %tmp; }
	shr.u32 	%r3, %r22, 20;
	add.s32 	%r23, %r4, -1024;
	shr.u32 	%r5, %r23, 6;
	mov.b32 	%r24, 15;
	sub.s32 	%r43, %r24, %r5;
	mov.b32 	%r25, 19;
	sub.s32 	%r26, %r25, %r5;
	setp.lt.u32 	%p2, %r23, 128;
	selp.b32 	%r45, 18, %r26, %p2;
	setp.ge.s32 	%p3, %r43, %r45;
	@%p3 bra 	$L__BB1_14;
	mov.b64 	%rd28, %fd4;
	shl.b64 	%rd29, %rd28, 11;
	or.b64  	%rd38, %rd29, -9223372036854775808;
	neg.s32 	%r44, %r5;
	mul.wide.s32 	%rd32, %r44, 8;
	mov.u64 	%rd33, __cudart_i2opi_d;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd77, %rd34, 120;
	mov.u64 	%rd79, 0;
	mov.u32 	%r42, %r1;
$L__BB1_3:
	.pragma "nounroll";
	ld.global.nc.u64 	%rd37, [%rd77];
	// begin inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd37;    
	mov.b64         {blo,bhi}, %rd38;    
	mov.b64         {clo,chi}, %rd79;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd35, {r0,r1};      
	mov.b64         %rd79, {r2,r3};      
	}
	// end inline asm
	st.local.u64 	[%r42], %rd35;
	add.s32 	%r43, %r43, 1;
	add.s32 	%r42, %r42, 8;
	add.s64 	%rd77, %rd77, 8;
	setp.lt.s32 	%p4, %r43, %r45;
	@%p4 bra 	$L__BB1_3;
	bra.uni 	$L__BB1_4;
$L__BB1_14:
	neg.s32 	%r44, %r5;
	mov.u64 	%rd79, 0;
	mov.u32 	%r45, %r43;
$L__BB1_4:
	ld.param.u64 	%rd26, [__internal_trig_reduction_slowpathd_param_1];
	and.b32  	%r46, %r22, -2147483648;
	sub.s32 	%r27, %r45, %r44;
	shl.b32 	%r28, %r27, 3;
	add.s32 	%r29, %r1, %r28;
	st.local.u64 	[%r29+-120], %rd79;
	and.b32  	%r16, %r3, 63;
	ld.local.u64 	%rd81, [%r1+16];
	ld.local.u64 	%rd80, [%r1+24];
	setp.eq.s32 	%p5, %r16, 0;
	mov.b32 	%r41, 64;
	@%p5 bra 	$L__BB1_6;
	sub.s32 	%r31, %r41, %r16;
	shl.b64 	%rd40, %rd80, %r16;
	shr.u64 	%rd41, %rd81, %r31;
	or.b64  	%rd80, %rd40, %rd41;
	shl.b64 	%rd42, %rd81, %r16;
	ld.local.u64 	%rd43, [%r1+8];
	shr.u64 	%rd44, %rd43, %r31;
	or.b64  	%rd81, %rd44, %rd42;
$L__BB1_6:
	shr.u64 	%rd45, %rd80, 62;
	cvt.u32.u64 	%r32, %rd45;
	shr.u64 	%rd46, %rd81, 62;
	shl.b64 	%rd47, %rd80, 2;
	or.b64  	%rd84, %rd47, %rd46;
	shl.b64 	%rd83, %rd81, 2;
	bfe.u64 	%rd48, %rd80, 61, 1;
	cvt.u32.u64 	%r33, %rd48;
	add.s32 	%r34, %r33, %r32;
	setp.eq.s32 	%p6, %r46, 0;
	neg.s32 	%r35, %r34;
	selp.b32 	%r36, %r34, %r35, %p6;
	st.u32 	[%rd26], %r36;
	setp.gt.s64 	%p7, %rd84, -1;
	@%p7 bra 	$L__BB1_8;
	xor.b32  	%r46, %r46, -2147483648;
	mov.u64 	%rd51, 0;
	// begin inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd51;
	mov.b64         {a2,a3}, %rd51;
	mov.b64         {b0,b1}, %rd83;
	mov.b64         {b2,b3}, %rd84;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd83, {r0,r1};
	mov.b64         %rd84, {r2,r3};
	}
	// end inline asm
$L__BB1_8:
	clz.b64 	%r47, %rd84;
	setp.eq.s32 	%p8, %r47, 0;
	@%p8 bra 	$L__BB1_10;
	shl.b64 	%rd55, %rd84, %r47;
	sub.s32 	%r38, %r41, %r47;
	shr.u64 	%rd56, %rd83, %r38;
	or.b64  	%rd84, %rd56, %rd55;
$L__BB1_10:
	mov.u64 	%rd60, -3958705157555305931;
	// begin inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd84;   
	mov.b64         {blo,bhi}, %rd60;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd63, {r0,r1};     
	mov.b64         %rd85, {r2,r3};     
	}
	// end inline asm
	setp.lt.s64 	%p9, %rd85, 1;
	@%p9 bra 	$L__BB1_12;
	add.s32 	%r47, %r47, 1;
	// begin inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd63;
	mov.b64         {a2,a3}, %rd85;
	mov.b64         {b0,b1}, %rd63;
	mov.b64         {b2,b3}, %rd85;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd61, {r0,r1};
	mov.b64         %rd85, {r2,r3};
	}
	// end inline asm
$L__BB1_12:
	mov.b32 	%r39, 1022;
	sub.s32 	%r40, %r39, %r47;
	cvt.u64.u32 	%rd67, %r40;
	cvt.u64.u32 	%rd68, %r46;
	shl.b64 	%rd69, %rd68, 32;
	shl.b64 	%rd70, %rd67, 52;
	add.s64 	%rd71, %rd85, 1;
	shr.u64 	%rd72, %rd71, 10;
	add.s64 	%rd73, %rd72, 1;
	shr.u64 	%rd74, %rd73, 1;
	add.s64 	%rd75, %rd70, %rd74;
	or.b64  	%rd76, %rd75, %rd69;
	mov.b64 	%fd4, %rd76;
$L__BB1_13:
	st.param.f64 	[func_retval0+0], %fd4;
	ret;
$L__func_end1:

}
.func  (.param .b64 func_retval0) __internal_accurate_pow(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<52>;
	.reg .f32 	%f<3>;
	.reg .f64 	%fd<134>;
$L__func_begin2:

	ld.param.f64 	%fd12, [__internal_accurate_pow_param_0];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r49}, %fd12;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r48, %temp}, %fd12;
	}
	shr.u32 	%r50, %r49, 20;
	setp.gt.u32 	%p1, %r49, 1048575;
	@%p1 bra 	$L__BB2_2;
	mul.f64 	%fd13, %fd12, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r49}, %fd13;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r48, %temp}, %fd13;
	}
	shr.u32 	%r16, %r49, 20;
	add.s32 	%r50, %r16, -54;
$L__BB2_2:
	ld.param.f64 	%fd11, [__internal_accurate_pow_param_1];
	add.s32 	%r51, %r50, -1023;
	and.b32  	%r17, %r49, -2146435073;
	or.b32  	%r18, %r17, 1072693248;
	mov.b64 	%fd132, {%r48, %r18};
	setp.lt.u32 	%p2, %r18, 1073127583;
	@%p2 bra 	$L__BB2_4;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd132;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd132;
	}
	add.s32 	%r21, %r20, -1048576;
	mov.b64 	%fd132, {%r19, %r21};
	add.s32 	%r51, %r50, -1022;
$L__BB2_4:
	add.f64 	%fd14, %fd132, 0dBFF0000000000000;
	add.f64 	%fd15, %fd132, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd16, %fd15;
	neg.f64 	%fd17, %fd15;
	mov.f64 	%fd18, 0d3FF0000000000000;
	fma.rn.f64 	%fd19, %fd17, %fd16, %fd18;
	fma.rn.f64 	%fd20, %fd19, %fd19, %fd19;
	fma.rn.f64 	%fd21, %fd20, %fd16, %fd16;
	mul.f64 	%fd22, %fd14, %fd21;
	fma.rn.f64 	%fd23, %fd14, %fd21, %fd22;
	mul.f64 	%fd24, %fd23, %fd23;
	mov.f64 	%fd25, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd26, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd27, %fd26, %fd24, %fd25;
	mov.f64 	%fd28, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd29, %fd27, %fd24, %fd28;
	mov.f64 	%fd30, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd31, %fd29, %fd24, %fd30;
	mov.f64 	%fd32, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd33, %fd31, %fd24, %fd32;
	mov.f64 	%fd34, 0d3F6249249242B910;
	fma.rn.f64 	%fd35, %fd33, %fd24, %fd34;
	mov.f64 	%fd36, 0d3F89999999999DFB;
	fma.rn.f64 	%fd37, %fd35, %fd24, %fd36;
	sub.f64 	%fd38, %fd14, %fd23;
	add.f64 	%fd39, %fd38, %fd38;
	neg.f64 	%fd40, %fd23;
	fma.rn.f64 	%fd41, %fd40, %fd14, %fd39;
	mul.f64 	%fd42, %fd21, %fd41;
	fma.rn.f64 	%fd43, %fd24, %fd37, 0d3FB5555555555555;
	mov.f64 	%fd44, 0d3FB5555555555555;
	sub.f64 	%fd45, %fd44, %fd43;
	fma.rn.f64 	%fd46, %fd24, %fd37, %fd45;
	add.f64 	%fd47, %fd46, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd48, %fd43, %fd47;
	sub.f64 	%fd49, %fd43, %fd48;
	add.f64 	%fd50, %fd47, %fd49;
	mul.rn.f64 	%fd51, %fd23, %fd23;
	neg.f64 	%fd52, %fd51;
	fma.rn.f64 	%fd53, %fd23, %fd23, %fd52;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd42;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd42;
	}
	add.s32 	%r24, %r23, 1048576;
	mov.b64 	%fd54, {%r22, %r24};
	fma.rn.f64 	%fd55, %fd23, %fd54, %fd53;
	mul.rn.f64 	%fd56, %fd51, %fd23;
	neg.f64 	%fd57, %fd56;
	fma.rn.f64 	%fd58, %fd51, %fd23, %fd57;
	fma.rn.f64 	%fd59, %fd51, %fd42, %fd58;
	fma.rn.f64 	%fd60, %fd55, %fd23, %fd59;
	mul.rn.f64 	%fd61, %fd48, %fd56;
	neg.f64 	%fd62, %fd61;
	fma.rn.f64 	%fd63, %fd48, %fd56, %fd62;
	fma.rn.f64 	%fd64, %fd48, %fd60, %fd63;
	fma.rn.f64 	%fd65, %fd50, %fd56, %fd64;
	add.f64 	%fd66, %fd61, %fd65;
	sub.f64 	%fd67, %fd61, %fd66;
	add.f64 	%fd68, %fd65, %fd67;
	add.f64 	%fd69, %fd23, %fd66;
	sub.f64 	%fd70, %fd23, %fd69;
	add.f64 	%fd71, %fd66, %fd70;
	add.f64 	%fd72, %fd68, %fd71;
	fma.rn.f64 	%fd73, %fd21, %fd41, %fd72;
	add.f64 	%fd74, %fd69, %fd73;
	sub.f64 	%fd75, %fd69, %fd74;
	add.f64 	%fd76, %fd73, %fd75;
	xor.b32  	%r25, %r51, -2147483648;
	mov.b32 	%r26, 1127219200;
	mov.b64 	%fd77, {%r25, %r26};
	mov.b32 	%r27, -2147483648;
	mov.b64 	%fd78, {%r27, %r26};
	sub.f64 	%fd79, %fd77, %fd78;
	mov.f64 	%fd80, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd81, %fd79, %fd80, %fd74;
	neg.f64 	%fd82, %fd79;
	fma.rn.f64 	%fd83, %fd82, %fd80, %fd81;
	sub.f64 	%fd84, %fd83, %fd74;
	sub.f64 	%fd85, %fd76, %fd84;
	mov.f64 	%fd86, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd87, %fd79, %fd86, %fd85;
	add.f64 	%fd88, %fd81, %fd87;
	sub.f64 	%fd89, %fd81, %fd88;
	add.f64 	%fd90, %fd87, %fd89;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd11;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r29, %temp}, %fd11;
	}
	shl.b32 	%r30, %r28, 1;
	setp.gt.u32 	%p3, %r30, -33554433;
	and.b32  	%r31, %r28, -15728641;
	selp.b32 	%r32, %r31, %r28, %p3;
	mov.b64 	%fd91, {%r29, %r32};
	mul.rn.f64 	%fd4, %fd88, %fd91;
	neg.f64 	%fd92, %fd4;
	fma.rn.f64 	%fd93, %fd88, %fd91, %fd92;
	fma.rn.f64 	%fd5, %fd90, %fd91, %fd93;
	add.f64 	%fd6, %fd4, %fd5;
	mov.f64 	%fd94, 0d4338000000000000;
	mov.f64 	%fd95, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd96, %fd6, %fd95, %fd94;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd96;
	}
	mov.f64 	%fd97, 0dC338000000000000;
	add.rn.f64 	%fd98, %fd96, %fd97;
	mov.f64 	%fd99, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd100, %fd98, %fd99, %fd6;
	mov.f64 	%fd101, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd102, %fd98, %fd101, %fd100;
	mov.f64 	%fd103, 0d3E928AF3FCA213EA;
	mov.f64 	%fd104, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd105, %fd104, %fd102, %fd103;
	mov.f64 	%fd106, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd107, %fd105, %fd102, %fd106;
	mov.f64 	%fd108, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd109, %fd107, %fd102, %fd108;
	mov.f64 	%fd110, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd111, %fd109, %fd102, %fd110;
	mov.f64 	%fd112, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd113, %fd111, %fd102, %fd112;
	mov.f64 	%fd114, 0d3F81111111122322;
	fma.rn.f64 	%fd115, %fd113, %fd102, %fd114;
	mov.f64 	%fd116, 0d3FA55555555502A1;
	fma.rn.f64 	%fd117, %fd115, %fd102, %fd116;
	mov.f64 	%fd118, 0d3FC5555555555511;
	fma.rn.f64 	%fd119, %fd117, %fd102, %fd118;
	mov.f64 	%fd120, 0d3FE000000000000B;
	fma.rn.f64 	%fd121, %fd119, %fd102, %fd120;
	fma.rn.f64 	%fd122, %fd121, %fd102, %fd18;
	fma.rn.f64 	%fd123, %fd122, %fd102, %fd18;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd123;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd123;
	}
	shl.b32 	%r33, %r13, 20;
	add.s32 	%r34, %r15, %r33;
	mov.b64 	%fd133, {%r14, %r34};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd6;
	}
	mov.b32 	%f2, %r35;
	abs.ftz.f32 	%f1, %f2;
	setp.lt.f32 	%p4, %f1, 0f4086232B;
	@%p4 bra 	$L__BB2_7;
	setp.lt.f64 	%p5, %fd6, 0d0000000000000000;
	add.f64 	%fd124, %fd6, 0d7FF0000000000000;
	selp.f64 	%fd133, 0d0000000000000000, %fd124, %p5;
	setp.geu.f32 	%p6, %f1, 0f40874800;
	@%p6 bra 	$L__BB2_7;
	shr.u32 	%r36, %r13, 31;
	add.s32 	%r37, %r13, %r36;
	shr.s32 	%r38, %r37, 1;
	shl.b32 	%r39, %r38, 20;
	add.s32 	%r40, %r15, %r39;
	mov.b64 	%fd125, {%r14, %r40};
	sub.s32 	%r41, %r13, %r38;
	shl.b32 	%r42, %r41, 20;
	add.s32 	%r43, %r42, 1072693248;
	mov.b32 	%r44, 0;
	mov.b64 	%fd126, {%r44, %r43};
	mul.f64 	%fd133, %fd126, %fd125;
$L__BB2_7:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r45, %temp}, %fd133;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r46}, %fd133;
	}
	and.b32  	%r47, %r46, 2147483647;
	setp.eq.s32 	%p7, %r47, 2146435072;
	setp.eq.s32 	%p8, %r45, 0;
	sub.f64 	%fd127, %fd4, %fd6;
	add.f64 	%fd128, %fd5, %fd127;
	fma.rn.f64 	%fd129, %fd133, %fd128, %fd133;
	selp.f64 	%fd130, %fd133, %fd129, %p8;
	selp.f64 	%fd131, %fd130, %fd129, %p7;
	st.param.f64 	[func_retval0+0], %fd131;
	ret;
$L__func_end2:

}
	.file	1 "/opt/inductor_cache/rm/crmw27gsadfdktgbyarbagsv2x3lnpfw2o5jk6xicnmp4yve5nzp.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 0
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 116
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 99
.b8 114
.b8 109
.b8 119
.b8 50
.b8 55
.b8 103
.b8 115
.b8 97
.b8 100
.b8 102
.b8 100
.b8 107
.b8 116
.b8 103
.b8 98
.b8 121
.b8 97
.b8 114
.b8 98
.b8 97
.b8 103
.b8 115
.b8 118
.b8 50
.b8 120
.b8 51
.b8 108
.b8 110
.b8 112
.b8 102
.b8 119
.b8 50
.b8 111
.b8 53
.b8 106
.b8 107
.b8 54
.b8 120
.b8 105
.b8 99
.b8 110
.b8 109
.b8 112
.b8 52
.b8 121
.b8 118
.b8 101
.b8 53
.b8 110
.b8 122
.b8 112
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 47
.b8 111
.b8 112
.b8 116
.b8 47
.b8 105
.b8 110
.b8 100
.b8 117
.b8 99
.b8 116
.b8 111
.b8 114
.b8 95
.b8 99
.b8 97
.b8 99
.b8 104
.b8 101
.b8 47
.b8 114
.b8 109
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
	}
	.section	.debug_loc	{	}
