//
// Generated by LLVM NVPTX Back-End
//

.version 8.4
.target sm_90a
.address_size 64

	// .globl	triton_
.extern .shared .align 16 .b8 global_smem[];

.visible .entry triton_(
	.param .u64 triton__param_0,
	.param .u64 triton__param_1,
	.param .u64 triton__param_2,
	.param .u32 triton__param_3,
	.param .u32 triton__param_4
)
.maxntid 64, 1, 1
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<49>;
	.reg .f32 	%f<23>;
	.reg .b64 	%rd<10>;
	.loc	1 18 0
$L__func_begin0:
	.loc	1 18 0

	ld.param.u64 	%rd4, [triton__param_0];
	ld.param.u64 	%rd5, [triton__param_1];
$L__tmp0:
	.loc	1 21 28
	// begin inline asm
	mov.u32 %r1, %ctaid.x;
	// end inline asm
	ld.param.u32 	%r17, [triton__param_3];
	.loc	1 23 21
	setp.lt.s32 	%p1, %r1, %r17;
	ld.param.u64 	%rd6, [triton__param_2];
	.loc	1 24 34
	mov.u32 	%r18, %tid.x;
	and.b32  	%r19, %r18, 31;
	shl.b32 	%r20, %r18, 1;
	and.b32  	%r21, %r20, 126;
	.loc	1 29 18
	mul.hi.s32 	%r22, %r1, 715827883;
	shr.u32 	%r23, %r22, 31;
	shr.u32 	%r24, %r22, 2;
	add.s32 	%r25, %r24, %r23;
	mul.lo.s32 	%r26, %r25, 24;
	sub.s32 	%r27, %r1, %r26;
	.loc	1 30 40
	shl.b32 	%r28, %r1, 7;
	.loc	1 30 36
	or.b32  	%r29, %r28, %r21;
	.loc	1 30 30
	mul.wide.s32 	%rd7, %r29, 2;
	add.s64 	%rd1, %rd4, %rd7;
	mov.b32 	%r3, 0;
	.loc	1 30 46
	// begin inline asm
	mov.u32 %r2, 0x0;
	@%p1 ld.global.b32 { %r2 }, [ %rd1 + 0 ];
	@!%p1 mov.u32 %r2, %r3;
	// end inline asm
	cvt.u16.u32 	%rs1, %r2;
	{ .reg .b16 tmp; mov.b32 {tmp, %rs2}, %r2; }
	.loc	1 30 67
	// begin inline asm
	cvt.f32.bf16 %r4, %rs1;
	// end inline asm
	mov.b32 	%f1, %r4;
	// begin inline asm
	cvt.f32.bf16 %r5, %rs2;
	// end inline asm
	mov.b32 	%f2, %r5;
	.loc	1 31 40
	shl.b32 	%r30, %r27, 7;
	.loc	1 31 36
	or.b32  	%r31, %r30, %r21;
	.loc	1 31 30
	mul.wide.s32 	%rd8, %r31, 2;
	add.s64 	%rd2, %rd5, %rd8;
	.loc	1 31 46
	// begin inline asm
	mov.u32 %r6, 0x0;
	@%p1 ld.global.L1::evict_last.b32 { %r6 }, [ %rd2 + 0 ];
	@!%p1 mov.u32 %r6, %r3;
	// end inline asm
	cvt.u16.u32 	%rs3, %r6;
	{ .reg .b16 tmp; mov.b32 {tmp, %rs4}, %r6; }
	.loc	1 31 97
	// begin inline asm
	cvt.f32.bf16 %r8, %rs3;
	// end inline asm
	mov.b32 	%f3, %r8;
	// begin inline asm
	cvt.f32.bf16 %r9, %rs4;
	// end inline asm
	mov.b32 	%f4, %r9;
	.loc	1 32 18
	add.f32 	%f5, %f1, %f3;
	add.f32 	%f6, %f2, %f4;
	.loc	1 33 18
	mul.f32 	%f7, %f6, %f6;
$L__tmp1:
	.loc	2 256 15
	fma.rn.f32 	%f8, %f5, %f5, %f7;
	selp.f32 	%f9, %f8, 0f00000000, %p1;
	.loc	2 267 36
	mov.b32 	%r32, %f9;
	shfl.sync.bfly.b32	%r33, %r32, 16, 31, -1;
	mov.b32 	%f10, %r33;
	.loc	2 256 15
	add.f32 	%f11, %f9, %f10;
	.loc	2 267 36
	mov.b32 	%r34, %f11;
	shfl.sync.bfly.b32	%r35, %r34, 8, 31, -1;
	mov.b32 	%f12, %r35;
	.loc	2 256 15
	add.f32 	%f13, %f11, %f12;
	.loc	2 267 36
	mov.b32 	%r36, %f13;
	shfl.sync.bfly.b32	%r37, %r36, 4, 31, -1;
	mov.b32 	%f14, %r37;
	.loc	2 256 15
	add.f32 	%f15, %f13, %f14;
	.loc	2 267 36
	mov.b32 	%r38, %f15;
	shfl.sync.bfly.b32	%r39, %r38, 2, 31, -1;
	mov.b32 	%f16, %r39;
	.loc	2 256 15
	add.f32 	%f17, %f15, %f16;
	.loc	2 267 36
	mov.b32 	%r40, %f17;
	shfl.sync.bfly.b32	%r41, %r40, 1, 31, -1;
	mov.b32 	%f18, %r41;
	.loc	2 256 15
	add.f32 	%f19, %f17, %f18;
	.loc	2 267 36
	setp.eq.s32 	%p5, %r19, 0;
	shr.u32 	%r42, %r18, 3;
	and.b32  	%r43, %r42, 4;
	mov.u32 	%r44, global_smem;
	add.s32 	%r10, %r44, %r43;
	mov.b32 	%r11, %f19;
	// begin inline asm
	@%p5 st.shared.b32 [ %r10 + 0 ], %r11;
	// end inline asm
	bar.sync 	0;
	setp.lt.s32 	%p6, %r18, 2;
	shl.b32 	%r45, %r18, 2;
	add.s32 	%r13, %r44, %r45;
	// begin inline asm
	@%p6 ld.shared.b32 %r12, [ %r13 + 0 ];
	// end inline asm
	mov.b32 	%f20, %r12;
	shfl.sync.bfly.b32	%r46, %r12, 1, 31, -1;
	mov.b32 	%f21, %r46;
	.loc	2 256 15
	add.f32 	%f22, %f20, %f21;
	.loc	2 267 36
	and.b32  	%r47, %r18, 1;
	setp.eq.b32 	%p9, %r47, 1;
	not.pred 	%p10, %p9;
	and.pred  	%p7, %p6, %p10;
	mov.b32 	%r15, %f22;
	// begin inline asm
	@%p7 st.shared.b32 [ %r13 + 0 ], %r15;
	// end inline asm
	bar.sync 	0;
	ld.shared.u32 	%r16, [global_smem];
$L__tmp2:
	.loc	1 37 27
	bar.sync 	0;
	.loc	1 38 25
	mul.wide.s32 	%rd9, %r1, 4;
	add.s64 	%rd3, %rd6, %rd9;
	.loc	1 38 36
	and.b32  	%r48, %r18, 63;
	setp.eq.s32 	%p11, %r48, 0;
	and.pred  	%p8, %p11, %p1;
	// begin inline asm
	@%p8 st.global.b32 [ %rd3 + 0 ], { %r16 };
	// end inline asm
	.loc	1 38 4
	ret;
$L__tmp3:
$L__func_end0:

}
	.file	1 "/opt/inductor_cache/pz/cpznml3oyu7tbn6gsmmr2sp2oh4c6lj3k3hxmjx7apwuatfv7asi.py"
	.file	2 "/usr/local/lib/python3.10/site-packages/triton/language/standard.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 1
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 2
.b8 46
.b8 0
.b8 3
.b8 8
.b8 32
.b8 11
.b8 0
.b8 0
.b8 3
.b8 46
.b8 1
.b8 17
.b8 1
.b8 18
.b8 1
.b8 49
.b8 19
.b8 0
.b8 0
.b8 4
.b8 29
.b8 0
.b8 49
.b8 19
.b8 17
.b8 1
.b8 18
.b8 1
.b8 88
.b8 11
.b8 89
.b8 11
.b8 87
.b8 11
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 173
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 99
.b8 112
.b8 122
.b8 110
.b8 109
.b8 108
.b8 51
.b8 111
.b8 121
.b8 117
.b8 55
.b8 116
.b8 98
.b8 110
.b8 54
.b8 103
.b8 115
.b8 109
.b8 109
.b8 114
.b8 50
.b8 115
.b8 112
.b8 50
.b8 111
.b8 104
.b8 52
.b8 99
.b8 54
.b8 108
.b8 106
.b8 51
.b8 107
.b8 51
.b8 104
.b8 120
.b8 109
.b8 106
.b8 120
.b8 55
.b8 97
.b8 112
.b8 119
.b8 117
.b8 97
.b8 116
.b8 102
.b8 118
.b8 55
.b8 97
.b8 115
.b8 105
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 47
.b8 111
.b8 112
.b8 116
.b8 47
.b8 105
.b8 110
.b8 100
.b8 117
.b8 99
.b8 116
.b8 111
.b8 114
.b8 95
.b8 99
.b8 97
.b8 99
.b8 104
.b8 101
.b8 47
.b8 112
.b8 122
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
.b8 2
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 95
.b8 0
.b8 1
.b8 3
.b64 $L__func_begin0
.b64 $L__func_end0
.b32 120
.b8 4
.b32 120
.b64 $L__tmp1
.b64 $L__tmp2
.b8 1
.b8 37
.b8 24
.b8 0
.b8 0
	}
	.section	.debug_loc	{	}
